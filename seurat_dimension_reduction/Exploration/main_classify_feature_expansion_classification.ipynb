{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c70773-809f-470a-a9a4-b5907273e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython import display\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8b0a34-3d3c-4a77-8a53-a01326ec1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a475f5-7b98-40da-be68-1770720796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22db1e1e-0226-489b-8e26-805b8c8d23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = pd.read_csv('./integrate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff9203-6526-4815-a36c-c9df9dcca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6eac3f-02b5-4c75-bca7-7e266b51af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat_std = train_test_seurat.std()\n",
    "column_names = list(train_test_seurat.columns)\n",
    "columns_remove = []\n",
    "for i in range(train_test_seurat.shape[1]):\n",
    "    if train_test_seurat_std[i] == 0:\n",
    "        columns_remove.append(column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6a640d-17ca-4197-9120-b68b3a8d8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.drop(columns_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0866b5b-8359-4a8d-aa6d-e3ae91eb6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat[columns_remove[0]] = train_test_seurat.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc71a28-7056-4114-b6cf-fd9f9454edb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109707, 54)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_seurat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1f1207-e8b6-4503-8c77-d2edcd5e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seurat = train_test_seurat.iloc[:90000, :]\n",
    "test_seurat = train_test_seurat.iloc[90000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9614e-73b8-4b4d-8ab0-7a7ca7ab07d1",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e7a64-bad8-4cf8-8ad0-00cd3ccf1f8b",
   "metadata": {},
   "source": [
    "# 1. Load data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75a9ad-7160-472c-9860-cd8b46ba1ac2",
   "metadata": {},
   "source": [
    "## 1.1 Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5f81e6-1e1c-4e7a-b3b3-9adfaa705711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./MLR_Project_train.csv')\n",
    "test = pd.read_csv('./MLR_Project_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0ca5-9892-4287-8e9d-41a273c0e932",
   "metadata": {},
   "source": [
    "Show the data format and dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d204a102-98ba-4502-bc62-b2219ccbba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5874.524387</td>\n",
       "      <td>1072.671848</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41440.23732</td>\n",
       "      <td>70.405148</td>\n",
       "      <td>7.392780</td>\n",
       "      <td>70.377281</td>\n",
       "      <td>23229.69262</td>\n",
       "      <td>23229.72655</td>\n",
       "      <td>70.378864</td>\n",
       "      <td>7.389173</td>\n",
       "      <td>70.380160</td>\n",
       "      <td>23229.76782</td>\n",
       "      <td>23379.81637</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.425618</td>\n",
       "      <td>70.494241</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>4898.757333</td>\n",
       "      <td>15165.92759</td>\n",
       "      <td>297487.1654</td>\n",
       "      <td>297487.16540</td>\n",
       "      <td>15165.92759</td>\n",
       "      <td>4898.757333</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>64.192982</td>\n",
       "      <td>20.940618</td>\n",
       "      <td>76.070270</td>\n",
       "      <td>23376.73707</td>\n",
       "      <td>23343.13291</td>\n",
       "      <td>77.290965</td>\n",
       "      <td>347.308164</td>\n",
       "      <td>102.380501</td>\n",
       "      <td>24823.08137</td>\n",
       "      <td>24120.94894</td>\n",
       "      <td>332.757607</td>\n",
       "      <td>17.386711</td>\n",
       "      <td>129.622187</td>\n",
       "      <td>23936.99077</td>\n",
       "      <td>21670.19233</td>\n",
       "      <td>71.518948</td>\n",
       "      <td>11.399004</td>\n",
       "      <td>78.006816</td>\n",
       "      <td>26437.161240</td>\n",
       "      <td>23811.09670</td>\n",
       "      <td>141.997532</td>\n",
       "      <td>22.474794</td>\n",
       "      <td>0.013314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6124.154099</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41442.22458</td>\n",
       "      <td>70.456758</td>\n",
       "      <td>7.356050</td>\n",
       "      <td>70.379576</td>\n",
       "      <td>23229.76020</td>\n",
       "      <td>23230.10472</td>\n",
       "      <td>70.273618</td>\n",
       "      <td>7.389813</td>\n",
       "      <td>70.329906</td>\n",
       "      <td>23229.46908</td>\n",
       "      <td>23384.98219</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.441223</td>\n",
       "      <td>70.702624</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5100.350569</td>\n",
       "      <td>15788.07683</td>\n",
       "      <td>308790.4312</td>\n",
       "      <td>308790.43120</td>\n",
       "      <td>15788.07683</td>\n",
       "      <td>5100.350569</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>2.937218</td>\n",
       "      <td>-14.428210</td>\n",
       "      <td>75.204973</td>\n",
       "      <td>23317.46049</td>\n",
       "      <td>23309.42032</td>\n",
       "      <td>74.880368</td>\n",
       "      <td>689.670872</td>\n",
       "      <td>127.070357</td>\n",
       "      <td>36746.26762</td>\n",
       "      <td>31012.79786</td>\n",
       "      <td>-310.576721</td>\n",
       "      <td>4.532673</td>\n",
       "      <td>-47.045260</td>\n",
       "      <td>22053.23653</td>\n",
       "      <td>14626.73339</td>\n",
       "      <td>48.124991</td>\n",
       "      <td>-99.618253</td>\n",
       "      <td>-115.120518</td>\n",
       "      <td>7705.543821</td>\n",
       "      <td>22665.35143</td>\n",
       "      <td>-377.287072</td>\n",
       "      <td>-73.700375</td>\n",
       "      <td>-0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5905.732593</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41443.14358</td>\n",
       "      <td>41442.30403</td>\n",
       "      <td>70.422472</td>\n",
       "      <td>7.417794</td>\n",
       "      <td>70.376448</td>\n",
       "      <td>23229.48142</td>\n",
       "      <td>23229.61008</td>\n",
       "      <td>70.474265</td>\n",
       "      <td>7.388979</td>\n",
       "      <td>70.397301</td>\n",
       "      <td>23229.83396</td>\n",
       "      <td>23387.39168</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.440201</td>\n",
       "      <td>70.686178</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5308.548086</td>\n",
       "      <td>16430.60795</td>\n",
       "      <td>320463.9968</td>\n",
       "      <td>320463.99680</td>\n",
       "      <td>16430.60795</td>\n",
       "      <td>5308.548086</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>141.231442</td>\n",
       "      <td>25.855392</td>\n",
       "      <td>75.184756</td>\n",
       "      <td>23248.26780</td>\n",
       "      <td>23307.91084</td>\n",
       "      <td>74.635569</td>\n",
       "      <td>634.523047</td>\n",
       "      <td>71.705965</td>\n",
       "      <td>28917.00549</td>\n",
       "      <td>24632.17456</td>\n",
       "      <td>419.071308</td>\n",
       "      <td>7.403187</td>\n",
       "      <td>118.846496</td>\n",
       "      <td>23430.24573</td>\n",
       "      <td>31251.55292</td>\n",
       "      <td>71.535567</td>\n",
       "      <td>53.482719</td>\n",
       "      <td>106.179152</td>\n",
       "      <td>37586.677270</td>\n",
       "      <td>23251.62576</td>\n",
       "      <td>261.098973</td>\n",
       "      <td>22.565621</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6029.325221</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41442.25682</td>\n",
       "      <td>70.458533</td>\n",
       "      <td>7.366031</td>\n",
       "      <td>70.379221</td>\n",
       "      <td>23230.08433</td>\n",
       "      <td>23229.87971</td>\n",
       "      <td>70.288944</td>\n",
       "      <td>7.390120</td>\n",
       "      <td>70.370247</td>\n",
       "      <td>23229.81662</td>\n",
       "      <td>23390.02296</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.439191</td>\n",
       "      <td>70.692085</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5510.265781</td>\n",
       "      <td>17053.14128</td>\n",
       "      <td>331774.2409</td>\n",
       "      <td>75710.89648</td>\n",
       "      <td>17053.14128</td>\n",
       "      <td>5510.265781</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>-1.580478</td>\n",
       "      <td>-7.499746</td>\n",
       "      <td>75.184756</td>\n",
       "      <td>23248.26836</td>\n",
       "      <td>23307.91084</td>\n",
       "      <td>74.635569</td>\n",
       "      <td>583.001082</td>\n",
       "      <td>70.384022</td>\n",
       "      <td>25642.94219</td>\n",
       "      <td>23482.26269</td>\n",
       "      <td>-249.671869</td>\n",
       "      <td>7.388974</td>\n",
       "      <td>49.809681</td>\n",
       "      <td>23193.67720</td>\n",
       "      <td>15867.01579</td>\n",
       "      <td>70.369496</td>\n",
       "      <td>-12.169114</td>\n",
       "      <td>63.930236</td>\n",
       "      <td>10052.351290</td>\n",
       "      <td>23229.64352</td>\n",
       "      <td>-10.549985</td>\n",
       "      <td>4.656636</td>\n",
       "      <td>-0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6142.360146</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41443.14358</td>\n",
       "      <td>41442.46480</td>\n",
       "      <td>70.413623</td>\n",
       "      <td>7.411287</td>\n",
       "      <td>70.376788</td>\n",
       "      <td>23229.70975</td>\n",
       "      <td>23229.82255</td>\n",
       "      <td>70.467206</td>\n",
       "      <td>7.389910</td>\n",
       "      <td>70.386986</td>\n",
       "      <td>23229.87603</td>\n",
       "      <td>23392.66710</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.438244</td>\n",
       "      <td>70.680386</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>1152.667997</td>\n",
       "      <td>17699.01445</td>\n",
       "      <td>343508.5253</td>\n",
       "      <td>343508.52530</td>\n",
       "      <td>17699.01445</td>\n",
       "      <td>5719.546213</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>149.144800</td>\n",
       "      <td>26.789196</td>\n",
       "      <td>74.904988</td>\n",
       "      <td>23229.77108</td>\n",
       "      <td>23260.32245</td>\n",
       "      <td>70.506980</td>\n",
       "      <td>566.549008</td>\n",
       "      <td>120.694073</td>\n",
       "      <td>25765.82131</td>\n",
       "      <td>24618.79392</td>\n",
       "      <td>363.188022</td>\n",
       "      <td>7.389057</td>\n",
       "      <td>78.826922</td>\n",
       "      <td>23235.86490</td>\n",
       "      <td>31790.06425</td>\n",
       "      <td>120.694068</td>\n",
       "      <td>42.971377</td>\n",
       "      <td>145.572170</td>\n",
       "      <td>37109.895810</td>\n",
       "      <td>24143.94971</td>\n",
       "      <td>188.639704</td>\n",
       "      <td>31.863254</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            0            1            2            3          4  \\\n",
       "0           0  5874.524387  1072.671848  41440.76212  41440.23732  70.405148   \n",
       "1           1  6124.154099  1072.802927  41440.76212  41442.22458  70.456758   \n",
       "2           2  5905.732593  1072.802927  41443.14358  41442.30403  70.422472   \n",
       "3           3  6029.325221  1072.802927  41440.76212  41442.25682  70.458533   \n",
       "4           4  6142.360146  1072.802927  41443.14358  41442.46480  70.413623   \n",
       "\n",
       "          5          6            7            8          9        10  \\\n",
       "0  7.392780  70.377281  23229.69262  23229.72655  70.378864  7.389173   \n",
       "1  7.356050  70.379576  23229.76020  23230.10472  70.273618  7.389813   \n",
       "2  7.417794  70.376448  23229.48142  23229.61008  70.474265  7.388979   \n",
       "3  7.366031  70.379221  23230.08433  23229.87971  70.288944  7.390120   \n",
       "4  7.411287  70.376788  23229.70975  23229.82255  70.467206  7.389910   \n",
       "\n",
       "          11           12           13         14         15         16  \\\n",
       "0  70.380160  23229.76782  23379.81637  83.418623  11.615135  83.418623   \n",
       "1  70.329906  23229.46908  23384.98219  83.418623  11.615135  83.418623   \n",
       "2  70.397301  23229.83396  23387.39168  83.418623  11.615135  83.418623   \n",
       "3  70.370247  23229.81662  23390.02296  83.418623  11.615135  83.418623   \n",
       "4  70.386986  23229.87603  23392.66710  70.572881  11.615135  83.418623   \n",
       "\n",
       "            17          18         19        20         21           22  \\\n",
       "0  23466.72590  23466.7259  83.418623  7.425618  70.494241  23229.77107   \n",
       "1  23466.72590  23466.7259  83.418623  7.441223  70.702624  23229.77107   \n",
       "2  23466.72590  23466.7259  83.418623  7.440201  70.686178  23229.77107   \n",
       "3  23233.34325  23466.7259  83.418623  7.439191  70.692085  23229.77107   \n",
       "4  23466.72590  23466.7259  83.418623  7.438244  70.680386  23229.77107   \n",
       "\n",
       "            23         24        25         26           27           28  \\\n",
       "0  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "1  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "2  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "3  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "4  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "\n",
       "          29           30           31           32            33  \\\n",
       "0  70.376262  4898.757333  15165.92759  297487.1654  297487.16540   \n",
       "1  70.376262  5100.350569  15788.07683  308790.4312  308790.43120   \n",
       "2  70.376262  5308.548086  16430.60795  320463.9968  320463.99680   \n",
       "3  70.376262  5510.265781  17053.14128  331774.2409   75710.89648   \n",
       "4  70.376262  1152.667997  17699.01445  343508.5253  343508.52530   \n",
       "\n",
       "            34           35         36           37           38         39  \\\n",
       "0  15165.92759  4898.757333  70.376262  23229.77107  23229.77107  70.376262   \n",
       "1  15788.07683  5100.350569  70.376262  23229.77107  23229.77107  70.376262   \n",
       "2  16430.60795  5308.548086  70.376262  23229.77107  23229.77107  70.376262   \n",
       "3  17053.14128  5510.265781  70.376262  23229.77107  23229.77107  70.376262   \n",
       "4  17699.01445  5719.546213  70.376262  23229.77107  23229.77107  70.376262   \n",
       "\n",
       "         40         41           42           43          44         45  \\\n",
       "0  7.389056  70.376262  23229.77107  23229.77107   64.192982  20.940618   \n",
       "1  7.389056  70.376262  23229.77107  23229.77107    2.937218 -14.428210   \n",
       "2  7.389056  70.376262  23229.77107  23229.77107  141.231442  25.855392   \n",
       "3  7.389056  70.376262  23229.77107  23229.77107   -1.580478  -7.499746   \n",
       "4  7.389056  70.376262  23229.77107  23229.77107  149.144800  26.789196   \n",
       "\n",
       "          46           47           48         49          50          51  \\\n",
       "0  76.070270  23376.73707  23343.13291  77.290965  347.308164  102.380501   \n",
       "1  75.204973  23317.46049  23309.42032  74.880368  689.670872  127.070357   \n",
       "2  75.184756  23248.26780  23307.91084  74.635569  634.523047   71.705965   \n",
       "3  75.184756  23248.26836  23307.91084  74.635569  583.001082   70.384022   \n",
       "4  74.904988  23229.77108  23260.32245  70.506980  566.549008  120.694073   \n",
       "\n",
       "            52           53          54         55          56           57  \\\n",
       "0  24823.08137  24120.94894  332.757607  17.386711  129.622187  23936.99077   \n",
       "1  36746.26762  31012.79786 -310.576721   4.532673  -47.045260  22053.23653   \n",
       "2  28917.00549  24632.17456  419.071308   7.403187  118.846496  23430.24573   \n",
       "3  25642.94219  23482.26269 -249.671869   7.388974   49.809681  23193.67720   \n",
       "4  25765.82131  24618.79392  363.188022   7.389057   78.826922  23235.86490   \n",
       "\n",
       "            58          59         60          61            62           63  \\\n",
       "0  21670.19233   71.518948  11.399004   78.006816  26437.161240  23811.09670   \n",
       "1  14626.73339   48.124991 -99.618253 -115.120518   7705.543821  22665.35143   \n",
       "2  31251.55292   71.535567  53.482719  106.179152  37586.677270  23251.62576   \n",
       "3  15867.01579   70.369496 -12.169114   63.930236  10052.351290  23229.64352   \n",
       "4  31790.06425  120.694068  42.971377  145.572170  37109.895810  24143.94971   \n",
       "\n",
       "           64         65    TARGET  \n",
       "0  141.997532  22.474794  0.013314  \n",
       "1 -377.287072 -73.700375 -0.000448  \n",
       "2  261.098973  22.565621  0.000244  \n",
       "3  -10.549985   4.656636 -0.000628  \n",
       "4  188.639704  31.863254  0.003811  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b677e07a-0a62-4e5f-921b-89861aa36a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16335.49448</td>\n",
       "      <td>1061.530132</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41237.30921</td>\n",
       "      <td>70.432910</td>\n",
       "      <td>7.379175</td>\n",
       "      <td>70.376460</td>\n",
       "      <td>23229.86828</td>\n",
       "      <td>23229.78910</td>\n",
       "      <td>70.397639</td>\n",
       "      <td>7.389850</td>\n",
       "      <td>70.389234</td>\n",
       "      <td>23229.90881</td>\n",
       "      <td>25602.51370</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>7.537712</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.703960</td>\n",
       "      <td>7.467846</td>\n",
       "      <td>70.433077</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23278.59091</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>7.537712</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23238.10616</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.703960</td>\n",
       "      <td>7133.700113</td>\n",
       "      <td>22063.32144</td>\n",
       "      <td>422799.6647</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>11589.95489</td>\n",
       "      <td>3740.045136</td>\n",
       "      <td>1860.710635</td>\n",
       "      <td>4138543.879</td>\n",
       "      <td>422799.6647</td>\n",
       "      <td>22063.32144</td>\n",
       "      <td>137.249904</td>\n",
       "      <td>66395.12033</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>258.079242</td>\n",
       "      <td>30.226219</td>\n",
       "      <td>75.913405</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.83301</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>1473.581679</td>\n",
       "      <td>71.758838</td>\n",
       "      <td>23874.69807</td>\n",
       "      <td>23499.73762</td>\n",
       "      <td>-155.498907</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.371005</td>\n",
       "      <td>23229.77106</td>\n",
       "      <td>12139.388420</td>\n",
       "      <td>71.758829</td>\n",
       "      <td>14.703740</td>\n",
       "      <td>84.826620</td>\n",
       "      <td>8035.667142</td>\n",
       "      <td>23254.88967</td>\n",
       "      <td>92.945298</td>\n",
       "      <td>12.071364</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13166.20458</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41238.66844</td>\n",
       "      <td>70.394428</td>\n",
       "      <td>7.378583</td>\n",
       "      <td>70.391576</td>\n",
       "      <td>23229.87463</td>\n",
       "      <td>23230.21191</td>\n",
       "      <td>70.288403</td>\n",
       "      <td>7.392921</td>\n",
       "      <td>70.343310</td>\n",
       "      <td>23229.64193</td>\n",
       "      <td>25607.58558</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.469469</td>\n",
       "      <td>70.576552</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>3945.011279</td>\n",
       "      <td>12222.51346</td>\n",
       "      <td>244010.9410</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>22695.88002</td>\n",
       "      <td>7338.666257</td>\n",
       "      <td>139.167102</td>\n",
       "      <td>1239716.334</td>\n",
       "      <td>244010.9410</td>\n",
       "      <td>12222.51346</td>\n",
       "      <td>792.472120</td>\n",
       "      <td>227216.19080</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>-147.808334</td>\n",
       "      <td>-20.702534</td>\n",
       "      <td>76.035317</td>\n",
       "      <td>23313.93717</td>\n",
       "      <td>23332.97615</td>\n",
       "      <td>75.852147</td>\n",
       "      <td>1828.336140</td>\n",
       "      <td>140.129854</td>\n",
       "      <td>37438.79508</td>\n",
       "      <td>30329.40335</td>\n",
       "      <td>167.006374</td>\n",
       "      <td>16.564437</td>\n",
       "      <td>34.929416</td>\n",
       "      <td>23264.53239</td>\n",
       "      <td>39398.972520</td>\n",
       "      <td>84.750309</td>\n",
       "      <td>60.805885</td>\n",
       "      <td>155.880284</td>\n",
       "      <td>41154.557460</td>\n",
       "      <td>24005.38063</td>\n",
       "      <td>199.782364</td>\n",
       "      <td>35.714646</td>\n",
       "      <td>-0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12897.58082</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41240.71985</td>\n",
       "      <td>41238.89069</td>\n",
       "      <td>70.476942</td>\n",
       "      <td>7.400935</td>\n",
       "      <td>70.375313</td>\n",
       "      <td>23229.84429</td>\n",
       "      <td>23229.89405</td>\n",
       "      <td>70.469979</td>\n",
       "      <td>7.391477</td>\n",
       "      <td>70.407134</td>\n",
       "      <td>23230.09254</td>\n",
       "      <td>25610.09095</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.470161</td>\n",
       "      <td>70.605117</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>995.797054</td>\n",
       "      <td>23323.37355</td>\n",
       "      <td>445692.4097</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>12850.00699</td>\n",
       "      <td>4148.336212</td>\n",
       "      <td>3120.762743</td>\n",
       "      <td>4161436.624</td>\n",
       "      <td>445692.4097</td>\n",
       "      <td>23323.37355</td>\n",
       "      <td>233.004088</td>\n",
       "      <td>67655.17244</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>289.867668</td>\n",
       "      <td>32.185249</td>\n",
       "      <td>75.966899</td>\n",
       "      <td>23326.54724</td>\n",
       "      <td>23327.60253</td>\n",
       "      <td>75.649862</td>\n",
       "      <td>1924.171252</td>\n",
       "      <td>111.147939</td>\n",
       "      <td>37691.96604</td>\n",
       "      <td>29511.34868</td>\n",
       "      <td>53.482595</td>\n",
       "      <td>13.727445</td>\n",
       "      <td>165.790143</td>\n",
       "      <td>24499.54560</td>\n",
       "      <td>7180.863302</td>\n",
       "      <td>74.919427</td>\n",
       "      <td>-22.153780</td>\n",
       "      <td>54.137349</td>\n",
       "      <td>6873.937564</td>\n",
       "      <td>23667.70306</td>\n",
       "      <td>74.616186</td>\n",
       "      <td>24.773579</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12887.42863</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41239.00250</td>\n",
       "      <td>70.412815</td>\n",
       "      <td>7.378490</td>\n",
       "      <td>70.377339</td>\n",
       "      <td>23229.91979</td>\n",
       "      <td>23229.86689</td>\n",
       "      <td>70.283757</td>\n",
       "      <td>7.389189</td>\n",
       "      <td>70.359007</td>\n",
       "      <td>23229.67957</td>\n",
       "      <td>25612.79171</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.469667</td>\n",
       "      <td>70.602493</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>4329.804019</td>\n",
       "      <td>13410.04591</td>\n",
       "      <td>265586.1421</td>\n",
       "      <td>88824.3188</td>\n",
       "      <td>23883.41247</td>\n",
       "      <td>7723.458997</td>\n",
       "      <td>1326.699554</td>\n",
       "      <td>1261291.535</td>\n",
       "      <td>265586.1421</td>\n",
       "      <td>13410.04591</td>\n",
       "      <td>1177.264861</td>\n",
       "      <td>228403.72320</td>\n",
       "      <td>455867.2494</td>\n",
       "      <td>455867.2494</td>\n",
       "      <td>-141.955445</td>\n",
       "      <td>-15.581110</td>\n",
       "      <td>75.883198</td>\n",
       "      <td>23230.50078</td>\n",
       "      <td>23310.57289</td>\n",
       "      <td>73.052689</td>\n",
       "      <td>1808.877007</td>\n",
       "      <td>123.263599</td>\n",
       "      <td>31327.33150</td>\n",
       "      <td>25876.72169</td>\n",
       "      <td>86.016065</td>\n",
       "      <td>7.327061</td>\n",
       "      <td>26.252334</td>\n",
       "      <td>22958.21980</td>\n",
       "      <td>39599.779840</td>\n",
       "      <td>122.820383</td>\n",
       "      <td>46.190366</td>\n",
       "      <td>145.600464</td>\n",
       "      <td>39883.925250</td>\n",
       "      <td>24179.10475</td>\n",
       "      <td>145.999434</td>\n",
       "      <td>26.920634</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11994.73779</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41240.71985</td>\n",
       "      <td>41238.81077</td>\n",
       "      <td>70.481341</td>\n",
       "      <td>7.392913</td>\n",
       "      <td>70.385266</td>\n",
       "      <td>23229.57329</td>\n",
       "      <td>23229.79297</td>\n",
       "      <td>70.450114</td>\n",
       "      <td>7.390389</td>\n",
       "      <td>70.377065</td>\n",
       "      <td>23229.74335</td>\n",
       "      <td>25615.17798</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.469671</td>\n",
       "      <td>70.607659</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>1408.908041</td>\n",
       "      <td>24598.30068</td>\n",
       "      <td>468855.4055</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>14124.93413</td>\n",
       "      <td>4561.447200</td>\n",
       "      <td>4395.689873</td>\n",
       "      <td>4184599.619</td>\n",
       "      <td>468855.4055</td>\n",
       "      <td>24598.30068</td>\n",
       "      <td>175.464314</td>\n",
       "      <td>68930.09957</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>276.189378</td>\n",
       "      <td>21.701184</td>\n",
       "      <td>76.055710</td>\n",
       "      <td>23354.81165</td>\n",
       "      <td>23345.67063</td>\n",
       "      <td>77.102636</td>\n",
       "      <td>1906.984098</td>\n",
       "      <td>598.329736</td>\n",
       "      <td>40066.97972</td>\n",
       "      <td>36345.65544</td>\n",
       "      <td>-605.228200</td>\n",
       "      <td>-132.958332</td>\n",
       "      <td>-529.575904</td>\n",
       "      <td>12869.21582</td>\n",
       "      <td>6244.418503</td>\n",
       "      <td>-23.787704</td>\n",
       "      <td>-46.168214</td>\n",
       "      <td>-57.984418</td>\n",
       "      <td>-6030.026819</td>\n",
       "      <td>13649.75983</td>\n",
       "      <td>-694.862277</td>\n",
       "      <td>-218.983324</td>\n",
       "      <td>-0.001462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            0            1            2            3          4  \\\n",
       "0           0  16335.49448  1061.530132  41238.33840  41237.30921  70.432910   \n",
       "1           1  13166.20458  1061.661211  41238.33840  41238.66844  70.394428   \n",
       "2           2  12897.58082  1061.661211  41240.71985  41238.89069  70.476942   \n",
       "3           3  12887.42863  1061.661211  41238.33840  41239.00250  70.412815   \n",
       "4           4  11994.73779  1061.661211  41240.71985  41238.81077  70.481341   \n",
       "\n",
       "          5          6            7            8          9        10  \\\n",
       "0  7.379175  70.376460  23229.86828  23229.78910  70.397639  7.389850   \n",
       "1  7.378583  70.391576  23229.87463  23230.21191  70.288403  7.392921   \n",
       "2  7.400935  70.375313  23229.84429  23229.89405  70.469979  7.391477   \n",
       "3  7.378490  70.377339  23229.91979  23229.86689  70.283757  7.389189   \n",
       "4  7.392913  70.385266  23229.57329  23229.79297  70.450114  7.390389   \n",
       "\n",
       "          11           12           13         14        15         16  \\\n",
       "0  70.389234  23229.90881  25602.51370  70.835039  7.537712  70.835039   \n",
       "1  70.343310  23229.64193  25607.58558  70.572881  7.452766  70.572881   \n",
       "2  70.407134  23230.09254  25610.09095  70.572881  7.580185  70.966118   \n",
       "3  70.359007  23229.67957  25612.79171  70.572881  7.452766  70.572881   \n",
       "4  70.377065  23229.74335  25615.17798  70.572881  7.580185  70.966118   \n",
       "\n",
       "            17           18         19        20         21           22  \\\n",
       "0  23235.72471  23235.72471  70.703960  7.467846  70.433077  23230.96180   \n",
       "1  23240.48762  23240.48762  70.966118  7.469469  70.576552  23230.96180   \n",
       "2  23233.34325  23233.34325  70.572881  7.470161  70.605117  23233.34325   \n",
       "3  23233.34325  23240.48762  70.966118  7.469667  70.602493  23230.96180   \n",
       "4  23233.34325  23233.34325  70.572881  7.469671  70.607659  23233.34325   \n",
       "\n",
       "            23         24        25         26           27           28  \\\n",
       "0  23278.59091  70.835039  7.537712  70.441802  23238.10616  23235.72471   \n",
       "1  23235.72471  70.572881  7.452766  70.572881  23280.97236  23240.48762   \n",
       "2  23280.97236  70.966118  7.580185  70.441802  23235.72471  23233.34325   \n",
       "3  23235.72471  70.572881  7.452766  70.572881  23280.97236  23240.48762   \n",
       "4  23280.97236  70.966118  7.580185  70.441802  23235.72471  23233.34325   \n",
       "\n",
       "          29           30           31           32           33           34  \\\n",
       "0  70.703960  7133.700113  22063.32144  422799.6647  232518.5574  11589.95489   \n",
       "1  70.966118  3945.011279  12222.51346  244010.9410  434292.0482  22695.88002   \n",
       "2  70.572881   995.797054  23323.37355  445692.4097  255411.3025  12850.00699   \n",
       "3  70.966118  4329.804019  13410.04591  265586.1421   88824.3188  23883.41247   \n",
       "4  70.572881  1408.908041  24598.30068  468855.4055  278574.2983  14124.93413   \n",
       "\n",
       "            35           36           37           38           39  \\\n",
       "0  3740.045136  1860.710635  4138543.879  422799.6647  22063.32144   \n",
       "1  7338.666257   139.167102  1239716.334  244010.9410  12222.51346   \n",
       "2  4148.336212  3120.762743  4161436.624  445692.4097  23323.37355   \n",
       "3  7723.458997  1326.699554  1261291.535  265586.1421  13410.04591   \n",
       "4  4561.447200  4395.689873  4184599.619  468855.4055  24598.30068   \n",
       "\n",
       "            40            41           42           43          44         45  \\\n",
       "0   137.249904   66395.12033  232518.5574  232518.5574  258.079242  30.226219   \n",
       "1   792.472120  227216.19080  434292.0482  434292.0482 -147.808334 -20.702534   \n",
       "2   233.004088   67655.17244  255411.3025  255411.3025  289.867668  32.185249   \n",
       "3  1177.264861  228403.72320  455867.2494  455867.2494 -141.955445 -15.581110   \n",
       "4   175.464314   68930.09957  278574.2983  278574.2983  276.189378  21.701184   \n",
       "\n",
       "          46           47           48         49           50          51  \\\n",
       "0  75.913405  23229.77107  23229.83301  70.376262  1473.581679   71.758838   \n",
       "1  76.035317  23313.93717  23332.97615  75.852147  1828.336140  140.129854   \n",
       "2  75.966899  23326.54724  23327.60253  75.649862  1924.171252  111.147939   \n",
       "3  75.883198  23230.50078  23310.57289  73.052689  1808.877007  123.263599   \n",
       "4  76.055710  23354.81165  23345.67063  77.102636  1906.984098  598.329736   \n",
       "\n",
       "            52           53          54          55          56           57  \\\n",
       "0  23874.69807  23499.73762 -155.498907    7.389056   70.371005  23229.77106   \n",
       "1  37438.79508  30329.40335  167.006374   16.564437   34.929416  23264.53239   \n",
       "2  37691.96604  29511.34868   53.482595   13.727445  165.790143  24499.54560   \n",
       "3  31327.33150  25876.72169   86.016065    7.327061   26.252334  22958.21980   \n",
       "4  40066.97972  36345.65544 -605.228200 -132.958332 -529.575904  12869.21582   \n",
       "\n",
       "             58          59         60          61            62           63  \\\n",
       "0  12139.388420   71.758829  14.703740   84.826620   8035.667142  23254.88967   \n",
       "1  39398.972520   84.750309  60.805885  155.880284  41154.557460  24005.38063   \n",
       "2   7180.863302   74.919427 -22.153780   54.137349   6873.937564  23667.70306   \n",
       "3  39599.779840  122.820383  46.190366  145.600464  39883.925250  24179.10475   \n",
       "4   6244.418503  -23.787704 -46.168214  -57.984418  -6030.026819  13649.75983   \n",
       "\n",
       "           64          65    TARGET  \n",
       "0   92.945298   12.071364  0.010438  \n",
       "1  199.782364   35.714646 -0.000532  \n",
       "2   74.616186   24.773579  0.000726  \n",
       "3  145.999434   26.920634  0.001459  \n",
       "4 -694.862277 -218.983324 -0.001462  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b0f25-fdff-4ce6-a2a4-26bc5f7336a0",
   "metadata": {},
   "source": [
    "## 1.3 Show the maximum return of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0404bc17-40e7-423f-87c3-a050e7fbd448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum return of training set: 195.6927566509\n",
      "Maximum return of testing set: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "train_max = np.sum(train['TARGET'][train['TARGET']>0])\n",
    "test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Maximum return of training set:', train_max)\n",
    "print('Maximum return of testing set:', test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9149c4-b51c-435c-9e81-744f395e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=0.5).fit(pd.DataFrame(train_seurat.iloc[:, :]), train['TARGET'])\n",
    "pred = reg.predict(pd.DataFrame(train_seurat.iloc[:, :]))\n",
    "\n",
    "pred_test = reg.predict(pd.DataFrame(test_seurat.iloc[:, :]))\n",
    "\n",
    "train_res = np.sum(train['TARGET'][pred>0])\n",
    "test_res = np.sum(test['TARGET'][pred_test>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4db722a-05ac-4744-bd74-10c3aa0102b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train naive random selection percentage return: 6.326525261528151%\n",
      "Test naive random selection percentage return: 4.010962118285209%\n"
     ]
    }
   ],
   "source": [
    "print(f'Train naive random selection percentage return: {train_res/train_max*100}%')\n",
    "print(f'Test naive random selection percentage return: {test_res/test_max*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbf2b1-9cdc-476e-9975-8e12dcbdee2b",
   "metadata": {},
   "source": [
    "### 1.3.1 Remove the Unnamed columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb5b0f0-c09f-44ee-975c-8613c205ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5692e8-9d50-4da7-8910-71dd37a12bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 67)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80365c-a512-42fd-a21c-1858ea6fd192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c18b19d7-31bb-4904-8c99-fd0c206e925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame()\n",
    "\n",
    "for i in range(train.shape[1]-1):\n",
    "    for j in range(train.shape[1]-1):\n",
    "        train_[str(i)+'_'+str(j)+'_feat'] = train.iloc[:, i] * train.iloc[:, j]\n",
    "        \n",
    "train_target = pd.DataFrame(train['TARGET'])\n",
    "\n",
    "train = train.drop(['TARGET'], axis = 1)\n",
    "\n",
    "train = pd.concat([train, train_], axis = 1)\n",
    "\n",
    "train = (train-train.mean())/train.std()\n",
    "\n",
    "train['TARGET'] = train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d0774cd-7482-41db-977f-13a02a109d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.DataFrame()\n",
    "\n",
    "for i in range(test.shape[1]-1):\n",
    "    for j in range(test.shape[1]-1):\n",
    "        test_[str(i)+'_'+str(j)+'_feat'] = test.iloc[:, i] * test.iloc[:, j]\n",
    "        \n",
    "test_target = pd.DataFrame(test['TARGET'])\n",
    "\n",
    "test = test.drop(['TARGET'], axis = 1)\n",
    "\n",
    "test = pd.concat([test, test_], axis = 1)\n",
    "\n",
    "test = (test-test.mean())/test.std()\n",
    "\n",
    "test['TARGET'] = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91132f5e-e1fc-4108-b2c0-ed422db1125f",
   "metadata": {},
   "source": [
    "## 5.5 Autoencoder Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e90eb95-282e-415e-a6bb-81d93114b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (72000, 4422)\n",
      "Validation X shape: (18000, 4422)\n",
      "Test X shape: (19707, 4422)\n",
      "Train Y shape: (72000, 1)\n",
      "Val Y shape: (18000, 1)\n",
      "Test Y shape: (19707, 1)\n",
      "train_max: 156.45405680890002\n",
      "val_max: 39.238699842\n",
      "test_max: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "input_features = train.drop(['TARGET'], axis=1).to_numpy()\n",
    "output_features = pd.DataFrame((np.sign(train['TARGET'])+1)//2).to_numpy()\n",
    "# output_features = train['TARGET'].to_numpy()\n",
    "\n",
    "X_test = test.drop(['TARGET'], axis=1).to_numpy()\n",
    "Y_test = pd.DataFrame((np.sign(test['TARGET'])+1)//2).to_numpy()\n",
    "# Y_test = test['TARGET'].to_numpy()\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(input_features, output_features, test_size=0.2, random_state=42)\n",
    "\n",
    "####\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "test_data = test\n",
    "####\n",
    "\n",
    "auto_train_max = np.sum(train_data['TARGET'][train_data['TARGET']>0])\n",
    "auto_val_max = np.sum(val_data['TARGET'][val_data['TARGET']>0])\n",
    "auto_test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Train X shape:', X_train.shape)\n",
    "print('Validation X shape:', X_val.shape)\n",
    "print('Test X shape:', X_test.shape)\n",
    "\n",
    "print('Train Y shape:', Y_train.shape)\n",
    "print('Val Y shape:', Y_val.shape)\n",
    "print('Test Y shape:', Y_test.shape)\n",
    "\n",
    "print('train_max:', auto_train_max)\n",
    "print('val_max:', auto_val_max)\n",
    "print('test_max:', auto_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f7a19ea-d496-4ee3-b66d-7f33a40361be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_feature: 4422\n",
      "output_feature: 1\n"
     ]
    }
   ],
   "source": [
    "train_input = torch.from_numpy(X_train)\n",
    "train_output = torch.from_numpy(Y_train)\n",
    "val_input = torch.from_numpy(X_val)\n",
    "val_output = torch.from_numpy(Y_val)\n",
    "test_input = torch.from_numpy(X_test)\n",
    "test_output = torch.from_numpy(Y_test)\n",
    "\n",
    "# train_input = torch.unsqueeze(train_input, 1)\n",
    "# val_input = torch.unsqueeze(val_input, 1)\n",
    "# test_input = torch.unsqueeze(test_input, 1)\n",
    "\n",
    "train_input = train_input.float()\n",
    "train_output = train_output.float()\n",
    "val_input = val_input.float()\n",
    "val_output = val_output.float()\n",
    "test_input = test_input.float()\n",
    "test_output = test_output.float()\n",
    "\n",
    "input_feature = train_input.shape[1]\n",
    "output_feature = 1\n",
    "\n",
    "print('input_feature:', input_feature)\n",
    "print('output_feature:', output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5591cb-674f-4288-9119-9b6c5ee1b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.to(device)\n",
    "train_output = train_output.to(device)\n",
    "val_input = val_input.to(device)\n",
    "val_output = val_output.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_output = test_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "852276a1-d972-4d87-8724-e9c1c973513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9a0344a-7b96-48ff-852a-999ea40a6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_feature, input_feature*2)\n",
    "        self.linear2 = nn.Linear(input_feature*2, input_feature//16)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        self.linear = nn.Linear(input_feature//16, 2)\n",
    "        \n",
    "        # nn.init.constant_(self.linear1.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear2.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear3.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear4.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear.weight, 0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "#         x = self.batchnorm_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "#         x = self.batchnorm_2(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        output = self.linear(x)\n",
    "                \n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22bb31c5-cbce-4611-ab4a-3f33902bdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "train_ds = TensorDataset(train_input, train_output)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed2d7429-de64-4f06-8610-93c27155281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path):\n",
    "    best_loss = float('inf')\n",
    "    train_pred_output = []\n",
    "    val_pred_output = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    test_error = []\n",
    "    epochs = []\n",
    "    \n",
    "    train_returns = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "    \n",
    "    train_sum = []\n",
    "    val_sum = []\n",
    "    test_sum = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_dl:\n",
    "            model = model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            # y = torch.reshape(y, (y.shape[0], 1))\n",
    "            loss = loss_fn(pred, y.long().squeeze())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            \n",
    "            train_pred = model(train_input)\n",
    "            train_output_index = (torch.sign(train_output)+1)//2\n",
    "            train_pred_index = (torch.sign(train_pred)+1)//2\n",
    "            train_output = torch.reshape(train_output, (train_output_index.shape[0], 1))\n",
    "            # train_loss = loss_fn(train_output, train_pred)\n",
    "            train_loss = loss_fn(train_pred, train_output.long().squeeze())\n",
    "            train_loss = train_loss.cpu().detach().numpy()\n",
    "            \n",
    "            val_pred = model(val_input)\n",
    "            val_pred_index = (torch.sign(val_pred)+1)//2\n",
    "            val_output = torch.reshape(val_output, (val_output.shape[0], 1))\n",
    "            # val_loss = loss_fn(val_output, val_pred)\n",
    "            val_loss = loss_fn(val_pred, val_output.long().squeeze())\n",
    "            val_loss = val_loss.cpu().detach().numpy()\n",
    "        \n",
    "            test_pred = model(test_input)\n",
    "            test_pred_index = (torch.sign(test_pred)+1)//2\n",
    "            test_output = torch.reshape(test_output, (test_output.shape[0], 1))\n",
    "            # test_loss = loss_fn(test_output, test_pred)\n",
    "            test_loss = loss_fn(test_pred, test_output.long().squeeze())\n",
    "            test_loss = test_loss.cpu().detach().numpy()\n",
    "    \n",
    "            epochs.append(epoch)\n",
    "            train_error.append(math.log(train_loss+1))\n",
    "            val_error.append(math.log(val_loss+1))\n",
    "            test_error.append(math.log(test_loss+1))\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 2, figsize = (20, 7))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 4, figsize = (22, 5))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # train_conf = confusion_matrix(train_output.cpu().detach().numpy(), train_pred_index.cpu().detach().numpy())\n",
    "#             g1 = sns.heatmap(train_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[0], annot = True)\n",
    "#             g1.set_ylabel('True Target')\n",
    "#             g1.set_xlabel('Predict Target')\n",
    "#             g1.set_title('Train dataset')\n",
    "\n",
    "#             plt.grid(False)\n",
    "            # val_conf = confusion_matrix(val_output.cpu().detach().numpy(), val_pred_index.cpu().detach().numpy())\n",
    "#             g2 = sns.heatmap(val_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[1], annot = True)\n",
    "#             g2.set_ylabel('True Target')\n",
    "#             g2.set_xlabel('Predict Target')\n",
    "#             g2.set_title('Val dataset')\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # test_conf = confusion_matrix(test_output.cpu().detach().numpy(), test_pred_index.cpu().detach().numpy())\n",
    "#             g3 = sns.heatmap(test_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[2], annot = True)\n",
    "#             g3.set_ylabel('True Target')\n",
    "#             g3.set_xlabel('Predict Target')\n",
    "#             g3.set_title('Test dataset')\n",
    "            \n",
    "            \n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            train_pred_np = torch.argmax(softmax(train_pred), 1)\n",
    "            val_pred_np = torch.argmax(softmax(val_pred), 1)\n",
    "            test_pred_np = torch.argmax(softmax(test_pred), 1)\n",
    "            # print(train_pred_np)\n",
    "                            \n",
    "            # train_pred_np = train_pred_index.cpu().detach().numpy()\n",
    "            train_output_np = train_output.cpu().detach().numpy()\n",
    "            # val_pred_np = val_pred_index.cpu().detach().numpy()\n",
    "            val_output_np = val_output.cpu().detach().numpy()\n",
    "            # test_pred_np = test_pred_index.cpu().detach().numpy()\n",
    "            test_output_np = test_output.cpu().detach().numpy()\n",
    "            \n",
    "#             train_max_value = max(max(train_output_np), max(train_pred_np))\n",
    "#             train_min_value = min(min(train_output_np), min(train_pred_np))\n",
    "#             val_max_value = max(max(val_output_np), max(val_pred_np))\n",
    "#             val_min_value = min(min(val_output_np), min(val_pred_np))\n",
    "#             test_max_value = max(max(test_output_np), max(test_pred_np))\n",
    "#             test_min_value = min(min(test_output_np), min(test_pred_np))\n",
    "            \n",
    "#             ax[0].scatter(train_output_np, train_pred_np, s = 20, alpha=0.3, c='blue')\n",
    "#             ax[1].scatter(val_output_np, val_pred_np, s = 20, alpha=0.3, c='red')\n",
    "#             ax[2].scatter(test_output_np, test_pred_np, s = 20, alpha=0.3, c='green')\n",
    "            \n",
    "#             ax[0].plot(epochs, train_error, c='blue')\n",
    "#             ax[0].plot(epochs, val_error, c='red')\n",
    "#             ax[0].plot(epochs, test_error, c='green')\n",
    "#             ax[0].set_title('Errors vs Epochs', fontsize=15)\n",
    "#             ax[0].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[0].set_ylabel('Errors', fontsize=10)\n",
    "\n",
    "#             ax[0].legend(['train', 'valid', 'test'])\n",
    "            \n",
    "#             ax[0].set_xlim([train_min_value, train_max_value])\n",
    "#             ax[0].set_ylim([train_min_value, train_max_value])\n",
    "#             ax[0].set_title('Trainig data', fontsize=15)\n",
    "#             ax[0].set_xlabel('Target', fontsize=10)\n",
    "#             ax[0].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[0].plot([train_min_value, train_max_value], [train_min_value, train_max_value], 'k-')\n",
    "            \n",
    "#             ax[1].set_xlim([val_min_value, val_max_value])\n",
    "#             ax[1].set_ylim([val_min_value, val_max_value])\n",
    "#             ax[1].set_title('Validation data', fontsize=15)\n",
    "#             ax[1].set_xlabel('Target', fontsize=10)\n",
    "#             ax[1].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[1].plot([val_min_value, val_max_value], [val_min_value, val_max_value], 'k-')\n",
    "            \n",
    "#             ax[2].set_xlim([test_min_value, test_max_value])\n",
    "#             ax[2].set_ylim([test_min_value, test_max_value])\n",
    "#             ax[2].set_title('Testing data', fontsize=15)\n",
    "#             ax[2].set_xlabel('Target', fontsize=10)\n",
    "#             ax[2].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[2].plot([test_min_value, test_max_value], [test_min_value, test_max_value], 'k-')\n",
    "            \n",
    "#             ax[3].plot(epochs, train_error, c='blue')\n",
    "#             ax[3].plot(epochs, val_error, c='red')\n",
    "#             ax[3].plot(epochs, test_error, c='green')\n",
    "#             ax[3].set_title('Training and Validation error', fontsize=15)\n",
    "#             ax[3].set_xlabel('Epochs', fontsize=10)\n",
    "#             ax[3].set_ylabel('MSE error', fontsize=10)\n",
    "            \n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "#             print('Epoch ', epoch, 'Train_loss: ', train_loss*1000, ' Validation_loss: ', val_loss*100, ' Test_loss: ', test_loss*100)\n",
    "            # print(train_pred_np.shape, train_pred_np)\n",
    "            # print(train_pred, train_pred_np)\n",
    "            # train_pred_np = np.squeeze(train_pred_np)\n",
    "            # val_pred_np = np.squeeze(val_pred_np)\n",
    "            # test_pred_np = np.squeeze(test_pred_np)\n",
    "            train_pred_np = train_pred_np.cpu().detach().numpy()\n",
    "            val_pred_np = val_pred_np.cpu().detach().numpy()\n",
    "            test_pred_np = test_pred_np.cpu().detach().numpy()\n",
    "            \n",
    "            train_res = np.sum(train_data['TARGET'][train_pred_np>0])\n",
    "            train_output_check = np.squeeze(train_output_np)\n",
    "            train_check = np.sum(train_data['TARGET'][train_output_check>0])\n",
    "            \n",
    "            val_res = np.sum(val_data['TARGET'][val_pred_np>0])\n",
    "            val_output_check = np.squeeze(val_output_np)\n",
    "            val_check = np.sum(val_data['TARGET'][val_output_check>0])\n",
    "            \n",
    "            test_res = np.sum(test_data['TARGET'][test_pred_np>0])\n",
    "            test_output_check = np.squeeze(test_output_np)\n",
    "            test_check = np.sum(test_data['TARGET'][test_output_check>0])\n",
    "            \n",
    "#             train_returns.append(train_res)\n",
    "#             val_returns.append(val_res)\n",
    "#             test_returns.append(test_res)\n",
    "            \n",
    "#             ax[1].plot(epochs, train_returns, c='blu`e')\n",
    "#             ax[1].plot(epochs, val_returns, c='red')\n",
    "#             ax[1].plot(epochs, test_returns, c='green')\n",
    "#             ax[1].legend(['train', 'valid', 'test'])\n",
    "#             ax[1].set_title('Return vs Epochs', fontsize=15)\n",
    "#             ax[1].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[1].set_ylabel('Returns', fontsize=10)\n",
    "\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "            train_sum.append(train_res)\n",
    "            val_sum.append(val_res)\n",
    "            test_sum.append(test_res)\n",
    "            # print(f'Checks: {train_check/auto_train_max*100}%, {val_check/auto_val_max*100}%, {test_check/auto_test_max*100}%')\n",
    "#             print(f'Maximum sum train return {train_res}, Total train return: {auto_train_max}, Maximum train percentage return: {train_res/auto_train_max*100}%')\n",
    "#             print(f'Maximum sum train return {val_res}, Total train return: {auto_val_max}, Maximum train percentage return: {val_res/auto_val_max*100}%')\n",
    "#             print(f'Maximum sum test return {test_res}, Total test return: {auto_test_max}, Maximum test percentage return: {test_res/auto_test_max*100}%')\n",
    "#             print('Epoch:', epoch, 'Train loss:', train_loss, 'Val loss:', val_loss, 'Test loss:', test_loss)\n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss}, Train return: {train_res/auto_train_max*100}%, Val loss: {val_loss}, Val return: {val_res/auto_val_max*100}%, Test loss: {test_loss}, Test return: {test_res/auto_test_max*100}%')\n",
    "            # print(np.squeeze(train_output.cpu().detach().numpy()))\n",
    "            # print(train_pred_np)\n",
    "            # print(confusion_matrix(np.squeeze(train_output.cpu().detach().numpy()), train_pred_np))\n",
    "            # print(confusion_matrix(np.squeeze(val_output.cpu().detach().numpy()), val_pred_np))\n",
    "            # print(confusion_matrix(np.squeeze(test_output.cpu().detach().numpy()), test_pred_np))\n",
    "            # print(train_conf)\n",
    "            # print(val_conf)\n",
    "            # print(test_conf)\n",
    "            # print(train_output, train_pred)\n",
    "            # if val_loss < best_loss:\n",
    "            #     torch.save(model.state_dict(), model_path)\n",
    "            #     best_loss = val_loss\n",
    "                \n",
    "#             train_pred_output.append([train_pred.cpu().detach().numpy(), train_output.cpu().detach().numpy()])\n",
    "#             val_pred_output.append([val_pred.cpu().detach().numpy(), val_output.cpu().detach().numpy()])\n",
    "    return train_sum, val_sum, test_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "966e1596-f5a6-4719-8056-f538126e72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.7000365853309631, Train return: -0.5902353016182413%, Val loss: 0.7003042697906494, Val return: -1.4791476688500207%, Test loss: 0.6998363137245178, Test return: -2.6139642153796396%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[[  378 35538]\n",
      " [  407 35677]]\n",
      "[[ 101 8926]\n",
      " [ 108 8865]]\n",
      "[[ 123 9719]\n",
      " [ 130 9735]]\n",
      "Epoch: 50, Train loss: 0.6925455331802368, Train return: 0.20113072835457071%, Val loss: 0.6932920217514038, Val return: -1.0422268516712285%, Test loss: 0.6932787299156189, Test return: -2.5678618035590324%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 0 1 1]\n",
      "[[20146 15770]\n",
      " [19256 16828]]\n",
      "[[4968 4059]\n",
      " [4851 4122]]\n",
      "[[5494 4348]\n",
      " [5440 4425]]\n",
      "Epoch: 100, Train loss: 0.6915990114212036, Train return: 0.4979354936456324%, Val loss: 0.6931835412979126, Val return: 0.39543169020579694%, Test loss: 0.6930530667304993, Test return: -2.3937074444626334%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 0 1 0]\n",
      "[[19858 16058]\n",
      " [18168 17916]]\n",
      "[[4879 4148]\n",
      " [4581 4392]]\n",
      "[[5320 4522]\n",
      " [5140 4725]]\n",
      "Epoch: 150, Train loss: 0.690964937210083, Train return: 0.05870114522641726%, Val loss: 0.6930840611457825, Val return: -0.38933251003510644%, Test loss: 0.6928263902664185, Test return: -2.6072042393660166%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 0]\n",
      "[[19726 16190]\n",
      " [17820 18264]]\n",
      "[[4838 4189]\n",
      " [4484 4489]]\n",
      "[[5289 4553]\n",
      " [5022 4843]]\n",
      "Epoch: 200, Train loss: 0.6904484033584595, Train return: -0.014705225527061598%, Val loss: 0.6930205821990967, Val return: -0.47514703787519047%, Test loss: 0.6926521062850952, Test return: -2.7948981662121724%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 0]\n",
      "[[19795 16121]\n",
      " [17672 18412]]\n",
      "[[4847 4180]\n",
      " [4490 4483]]\n",
      "[[5291 4551]\n",
      " [5005 4860]]\n",
      "Epoch: 250, Train loss: 0.6900016069412231, Train return: -0.13968651779156244%, Val loss: 0.692975640296936, Val return: -1.7197840186276778%, Test loss: 0.6925206184387207, Test return: -2.4976907369559562%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[19952 15964]\n",
      " [17627 18457]]\n",
      "[[4857 4170]\n",
      " [4501 4472]]\n",
      "[[5329 4513]\n",
      " [5005 4860]]\n",
      "Epoch: 300, Train loss: 0.6895984411239624, Train return: -0.1518883893117867%, Val loss: 0.6929472088813782, Val return: -2.239733733632305%, Test loss: 0.6924163103103638, Test return: -2.4313580773683015%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20016 15900]\n",
      " [17519 18565]]\n",
      "[[4868 4159]\n",
      " [4472 4501]]\n",
      "[[5340 4502]\n",
      " [4993 4872]]\n",
      "Epoch: 350, Train loss: 0.6892297267913818, Train return: -0.28341201816299744%, Val loss: 0.6929296851158142, Val return: -3.1385007300415944%, Test loss: 0.6923527121543884, Test return: -2.5552299226578996%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20127 15789]\n",
      " [17537 18547]]\n",
      "[[4853 4174]\n",
      " [4482 4491]]\n",
      "[[5356 4486]\n",
      " [5000 4865]]\n",
      "Epoch: 400, Train loss: 0.68888920545578, Train return: -0.15097704509414073%, Val loss: 0.6929209232330322, Val return: -3.680431079049716%, Test loss: 0.6923062801361084, Test return: -2.8534646211559016%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20181 15735]\n",
      " [17459 18625]]\n",
      "[[4858 4169]\n",
      " [4480 4493]]\n",
      "[[5358 4484]\n",
      " [5000 4865]]\n",
      "Epoch: 450, Train loss: 0.6885675191879272, Train return: -0.19192386571781395%, Val loss: 0.6929099559783936, Val return: -4.067599422067518%, Test loss: 0.6922757625579834, Test return: -2.965082867320206%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20200 15716]\n",
      " [17418 18666]]\n",
      "[[4864 4163]\n",
      " [4467 4506]]\n",
      "[[5361 4481]\n",
      " [5002 4863]]\n",
      "Epoch: 500, Train loss: 0.6882641911506653, Train return: -0.16885859611980183%, Val loss: 0.69290691614151, Val return: -4.2289264085754725%, Test loss: 0.6922606229782104, Test return: -2.716177443646274%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20304 15612]\n",
      " [17471 18613]]\n",
      "[[4885 4142]\n",
      " [4476 4497]]\n",
      "[[5405 4437]\n",
      " [5018 4847]]\n",
      "Epoch: 550, Train loss: 0.6879736185073853, Train return: -0.10588371326307586%, Val loss: 0.6929085850715637, Val return: -4.59073927845053%, Test loss: 0.6922471523284912, Test return: -2.8269481306353477%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20336 15580]\n",
      " [17400 18684]]\n",
      "[[4888 4139]\n",
      " [4469 4504]]\n",
      "[[5384 4458]\n",
      " [4998 4867]]\n",
      "Epoch: 600, Train loss: 0.6876936554908752, Train return: -0.18075809324996567%, Val loss: 0.6929252743721008, Val return: -4.745697017225854%, Test loss: 0.6922506093978882, Test return: -2.856302156723617%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20343 15573]\n",
      " [17368 18716]]\n",
      "[[4898 4129]\n",
      " [4484 4489]]\n",
      "[[5383 4459]\n",
      " [4998 4867]]\n",
      "Epoch: 650, Train loss: 0.6874222755432129, Train return: -0.042337543845790214%, Val loss: 0.6929486989974976, Val return: -4.702302070225662%, Test loss: 0.6922512054443359, Test return: -2.3921338691126466%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20363 15553]\n",
      " [17303 18781]]\n",
      "[[4888 4139]\n",
      " [4477 4496]]\n",
      "[[5391 4451]\n",
      " [4972 4893]]\n",
      "Epoch: 700, Train loss: 0.6871620416641235, Train return: 0.16316639741200573%, Val loss: 0.6929793357849121, Val return: -4.665352527915697%, Test loss: 0.6922627091407776, Test return: -2.785033929123641%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20407 15509]\n",
      " [17276 18808]]\n",
      "[[4890 4137]\n",
      " [4474 4499]]\n",
      "[[5381 4461]\n",
      " [4985 4880]]\n",
      "Epoch: 750, Train loss: 0.6869044303894043, Train return: 0.17879840370115693%, Val loss: 0.6930091381072998, Val return: -5.654088514485596%, Test loss: 0.6922796368598938, Test return: -2.7997646301431884%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20376 15540]\n",
      " [17193 18891]]\n",
      "[[4873 4154]\n",
      " [4485 4488]]\n",
      "[[5378 4464]\n",
      " [4966 4899]]\n",
      "Epoch: 800, Train loss: 0.6866545677185059, Train return: 0.28256860647595916%, Val loss: 0.6930477619171143, Val return: -5.795179295329313%, Test loss: 0.6922994256019592, Test return: -2.5489431559797726%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20421 15495]\n",
      " [17165 18919]]\n",
      "[[4888 4139]\n",
      " [4488 4485]]\n",
      "[[5390 4452]\n",
      " [4961 4904]]\n",
      "Epoch: 850, Train loss: 0.6864063739776611, Train return: 0.42965708196438074%, Val loss: 0.693085253238678, Val return: -5.802105019196165%, Test loss: 0.6923204660415649, Test return: -2.945423250629654%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20431 15485]\n",
      " [17107 18977]]\n",
      "[[4877 4150]\n",
      " [4479 4494]]\n",
      "[[5373 4469]\n",
      " [4959 4906]]\n",
      "Epoch: 900, Train loss: 0.6861647963523865, Train return: 0.6146685331877622%, Val loss: 0.6931339502334595, Val return: -5.817897642358892%, Test loss: 0.6923593878746033, Test return: -2.88874739151707%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20511 15405]\n",
      " [17119 18965]]\n",
      "[[4883 4144]\n",
      " [4477 4496]]\n",
      "[[5380 4462]\n",
      " [4961 4904]]\n",
      "Epoch: 950, Train loss: 0.6859255433082581, Train return: 0.5405555038646362%, Val loss: 0.693177342414856, Val return: -5.5472215306944666%, Test loss: 0.6923901438713074, Test return: -3.073270675756526%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20514 15402]\n",
      " [17081 19003]]\n",
      "[[4880 4147]\n",
      " [4477 4496]]\n",
      "[[5363 4479]\n",
      " [4956 4909]]\n",
      "Epoch: 1000, Train loss: 0.6856869459152222, Train return: 0.719771811015101%, Val loss: 0.6932183504104614, Val return: -5.622322446674505%, Test loss: 0.6924225687980652, Test return: -3.135761811942369%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20518 15398]\n",
      " [17022 19062]]\n",
      "[[4873 4154]\n",
      " [4467 4506]]\n",
      "[[5353 4489]\n",
      " [4960 4905]]\n",
      "Epoch: 1050, Train loss: 0.6854516863822937, Train return: 0.9044277700183737%, Val loss: 0.6932733058929443, Val return: -5.813078884327626%, Test loss: 0.6924630403518677, Test return: -3.0035512832583358%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20562 15354]\n",
      " [17033 19051]]\n",
      "[[4867 4160]\n",
      " [4474 4499]]\n",
      "[[5365 4477]\n",
      " [4967 4898]]\n",
      "Epoch: 1100, Train loss: 0.6852149367332458, Train return: 1.1373590844457981%, Val loss: 0.6933332085609436, Val return: -5.8814089210208955%, Test loss: 0.6925057768821716, Test return: -3.0936484154441013%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20554 15362]\n",
      " [16962 19122]]\n",
      "[[4855 4172]\n",
      " [4472 4501]]\n",
      "[[5356 4486]\n",
      " [4954 4911]]\n",
      "Epoch: 1150, Train loss: 0.6849825978279114, Train return: 1.4816850814750775%, Val loss: 0.6933875679969788, Val return: -5.7687647886261475%, Test loss: 0.6925444006919861, Test return: -2.8715999721634318%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20606 15310]\n",
      " [16949 19135]]\n",
      "[[4861 4166]\n",
      " [4473 4500]]\n",
      "[[5368 4474]\n",
      " [4962 4903]]\n",
      "Epoch: 1200, Train loss: 0.684748113155365, Train return: 1.767939619794293%, Val loss: 0.6934457421302795, Val return: -5.872337626573493%, Test loss: 0.6925871968269348, Test return: -3.0503677195990266%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20621 15295]\n",
      " [16908 19176]]\n",
      "[[4859 4168]\n",
      " [4469 4504]]\n",
      "[[5360 4482]\n",
      " [4968 4897]]\n",
      "Epoch: 1250, Train loss: 0.6845149993896484, Train return: 1.9585623687871683%, Val loss: 0.693503201007843, Val return: -6.03754671673456%, Test loss: 0.6926236152648926, Test return: -3.3352777112509733%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20671 15245]\n",
      " [16882 19202]]\n",
      "[[4854 4173]\n",
      " [4482 4491]]\n",
      "[[5360 4482]\n",
      " [4971 4894]]\n",
      "Epoch: 1300, Train loss: 0.6842805743217468, Train return: 2.2921126497091993%, Val loss: 0.6935664415359497, Val return: -6.069978010460503%, Test loss: 0.6926682591438293, Test return: -3.4725907133111313%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20694 15222]\n",
      " [16841 19243]]\n",
      "[[4849 4178]\n",
      " [4486 4487]]\n",
      "[[5358 4484]\n",
      " [4968 4897]]\n",
      "Epoch: 1350, Train loss: 0.6840469241142273, Train return: 2.469163908493971%, Val loss: 0.6936282515525818, Val return: -6.23554530820889%, Test loss: 0.692711353302002, Test return: -3.5654799222076656%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20703 15213]\n",
      " [16815 19269]]\n",
      "[[4850 4177]\n",
      " [4498 4475]]\n",
      "[[5343 4499]\n",
      " [4963 4902]]\n",
      "Epoch: 1400, Train loss: 0.683815062046051, Train return: 2.6160457616636155%, Val loss: 0.6936944723129272, Val return: -6.505195335926545%, Test loss: 0.6927601099014282, Test return: -3.5840375457154927%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20746 15170]\n",
      " [16820 19264]]\n",
      "[[4854 4173]\n",
      " [4514 4459]]\n",
      "[[5346 4496]\n",
      " [4974 4891]]\n",
      "Epoch: 1450, Train loss: 0.6835787296295166, Train return: 2.7974125390982154%, Val loss: 0.6937529444694519, Val return: -6.724305671758756%, Test loss: 0.6928009390830994, Test return: -3.4505352355601335%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20695 15221]\n",
      " [16713 19371]]\n",
      "[[4833 4194]\n",
      " [4498 4475]]\n",
      "[[5336 4506]\n",
      " [4961 4904]]\n",
      "Epoch: 1500, Train loss: 0.6833482384681702, Train return: 3.147993707261825%, Val loss: 0.6938254237174988, Val return: -6.726913021146272%, Test loss: 0.692857027053833, Test return: -3.4213881686206213%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20804 15112]\n",
      " [16761 19323]]\n",
      "[[4847 4180]\n",
      " [4522 4451]]\n",
      "[[5355 4487]\n",
      " [4986 4879]]\n",
      "Epoch: 1550, Train loss: 0.6831098794937134, Train return: 3.2984448165529785%, Val loss: 0.6938883066177368, Val return: -6.837794353033397%, Test loss: 0.6929025053977966, Test return: -3.365765412234948%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20770 15146]\n",
      " [16709 19375]]\n",
      "[[4826 4201]\n",
      " [4507 4466]]\n",
      "[[5343 4499]\n",
      " [4966 4899]]\n",
      "Epoch: 1600, Train loss: 0.6828715801239014, Train return: 3.3963414483974823%, Val loss: 0.6939538717269897, Val return: -6.766239418458456%, Test loss: 0.6929542422294617, Test return: -3.2775683143854426%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20788 15128]\n",
      " [16690 19394]]\n",
      "[[4825 4202]\n",
      " [4506 4467]]\n",
      "[[5343 4499]\n",
      " [4969 4896]]\n",
      "Epoch: 1650, Train loss: 0.6826339364051819, Train return: 3.7408374467711982%, Val loss: 0.6940288543701172, Val return: -6.772054723270259%, Test loss: 0.6930044889450073, Test return: -3.4005077919039195%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20827 15089]\n",
      " [16674 19410]]\n",
      "[[4828 4199]\n",
      " [4520 4453]]\n",
      "[[5334 4508]\n",
      " [4971 4894]]\n",
      "Epoch: 1700, Train loss: 0.6823916435241699, Train return: 3.8808869101530483%, Val loss: 0.6940975189208984, Val return: -6.983082467138998%, Test loss: 0.6930521130561829, Test return: -3.1678966857436532%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20808 15108]\n",
      " [16611 19473]]\n",
      "[[4819 4208]\n",
      " [4519 4454]]\n",
      "[[5338 4504]\n",
      " [4964 4901]]\n",
      "Epoch: 1750, Train loss: 0.6821500658988953, Train return: 3.933616679315101%, Val loss: 0.6941710114479065, Val return: -7.024143697671298%, Test loss: 0.6931044459342957, Test return: -2.8538882849511786%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20807 15109]\n",
      " [16599 19485]]\n",
      "[[4821 4206]\n",
      " [4516 4457]]\n",
      "[[5342 4500]\n",
      " [4961 4904]]\n",
      "Epoch: 1800, Train loss: 0.6819068193435669, Train return: 4.112685306050808%, Val loss: 0.6942455768585205, Val return: -6.9776087613112106%, Test loss: 0.6931628584861755, Test return: -2.7230697249149594%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20852 15064]\n",
      " [16595 19489]]\n",
      "[[4831 4196]\n",
      " [4526 4447]]\n",
      "[[5349 4493]\n",
      " [4959 4906]]\n",
      "Epoch: 1850, Train loss: 0.6816604137420654, Train return: 4.26103228837514%, Val loss: 0.6943215727806091, Val return: -6.872935807912183%, Test loss: 0.6932058334350586, Test return: -2.7704048108641617%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20875 15041]\n",
      " [16563 19521]]\n",
      "[[4826 4201]\n",
      " [4524 4449]]\n",
      "[[5352 4490]\n",
      " [4962 4903]]\n",
      "Epoch: 1900, Train loss: 0.6814125180244446, Train return: 4.665077106958611%, Val loss: 0.6943973302841187, Val return: -6.7782908192924305%, Test loss: 0.6932610869407654, Test return: -2.89786793408575%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20914 15002]\n",
      " [16514 19570]]\n",
      "[[4827 4200]\n",
      " [4520 4453]]\n",
      "[[5348 4494]\n",
      " [4963 4902]]\n",
      "Epoch: 1950, Train loss: 0.6811619997024536, Train return: 4.744492440465599%, Val loss: 0.6944864392280579, Val return: -6.520421915869451%, Test loss: 0.6933193206787109, Test return: -2.6132483399691107%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20965 14951]\n",
      " [16522 19562]]\n",
      "[[4831 4196]\n",
      " [4511 4462]]\n",
      "[[5355 4487]\n",
      " [4959 4906]]\n",
      "Epoch: 2000, Train loss: 0.680908739566803, Train return: 4.877127921598227%, Val loss: 0.6945688724517822, Val return: -6.8525266887715235%, Test loss: 0.6933789253234863, Test return: -2.6414784906243876%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20979 14937]\n",
      " [16486 19598]]\n",
      "[[4826 4201]\n",
      " [4507 4466]]\n",
      "[[5349 4493]\n",
      " [4960 4905]]\n",
      "Epoch: 2050, Train loss: 0.6806514859199524, Train return: 5.102411847876033%, Val loss: 0.6946526765823364, Val return: -7.028747512291236%, Test loss: 0.6934376955032349, Test return: -2.9772421739567587%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20986 14930]\n",
      " [16416 19668]]\n",
      "[[4821 4206]\n",
      " [4509 4464]]\n",
      "[[5331 4511]\n",
      " [4959 4906]]\n",
      "Epoch: 2100, Train loss: 0.6803913116455078, Train return: 5.234011550817252%, Val loss: 0.6947270035743713, Val return: -7.273869127908705%, Test loss: 0.6934938430786133, Test return: -2.981750293122396%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[20971 14945]\n",
      " [16360 19724]]\n",
      "[[4808 4219]\n",
      " [4501 4472]]\n",
      "[[5331 4511]\n",
      " [4960 4905]]\n",
      "Epoch: 2150, Train loss: 0.6801299452781677, Train return: 5.401535888214359%, Val loss: 0.6948201656341553, Val return: -7.164473642908321%, Test loss: 0.6935675740242004, Test return: -2.7786915860543124%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21019 14897]\n",
      " [16336 19748]]\n",
      "[[4813 4214]\n",
      " [4505 4468]]\n",
      "[[5332 4510]\n",
      " [4965 4900]]\n",
      "Epoch: 2200, Train loss: 0.6798688769340515, Train return: 5.707415448489702%, Val loss: 0.6949060559272766, Val return: -7.189174418007976%, Test loss: 0.6936303377151489, Test return: -2.830669910821303%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21086 14830]\n",
      " [16337 19747]]\n",
      "[[4815 4212]\n",
      " [4518 4455]]\n",
      "[[5337 4505]\n",
      " [4977 4888]]\n",
      "Epoch: 2250, Train loss: 0.6796035170555115, Train return: 5.8861069491782905%, Val loss: 0.6949975490570068, Val return: -7.299930493451337%, Test loss: 0.6936992406845093, Test return: -2.8649519069431464%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21131 14785]\n",
      " [16313 19771]]\n",
      "[[4809 4218]\n",
      " [4529 4444]]\n",
      "[[5332 4510]\n",
      " [4975 4890]]\n",
      "Epoch: 2300, Train loss: 0.6793332099914551, Train return: 6.223855060526682%, Val loss: 0.695079505443573, Val return: -7.223331859650968%, Test loss: 0.6937571167945862, Test return: -2.6883109645613277%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21138 14778]\n",
      " [16253 19831]]\n",
      "[[4799 4228]\n",
      " [4529 4444]]\n",
      "[[5332 4510]\n",
      " [4964 4901]]\n",
      "Epoch: 2350, Train loss: 0.6790597438812256, Train return: 6.373529353911116%, Val loss: 0.6951609253883362, Val return: -7.080178762769108%, Test loss: 0.6938179731369019, Test return: -2.707507154224648%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21137 14779]\n",
      " [16208 19876]]\n",
      "[[4791 4236]\n",
      " [4523 4450]]\n",
      "[[5328 4514]\n",
      " [4966 4899]]\n",
      "Epoch: 2400, Train loss: 0.6787853837013245, Train return: 6.66266453789202%, Val loss: 0.6952481269836426, Val return: -7.284480819979966%, Test loss: 0.6938848495483398, Test return: -2.809653097493294%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21188 14728]\n",
      " [16185 19899]]\n",
      "[[4794 4233]\n",
      " [4524 4449]]\n",
      "[[5336 4506]\n",
      " [4987 4878]]\n",
      "Epoch: 2450, Train loss: 0.6785053014755249, Train return: 6.874046831036358%, Val loss: 0.6953376531600952, Val return: -7.18816979501693%, Test loss: 0.6939519643783569, Test return: -2.917336657457113%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21174 14742]\n",
      " [16134 19950]]\n",
      "[[4784 4243]\n",
      " [4521 4452]]\n",
      "[[5326 4516]\n",
      " [4978 4887]]\n",
      "Epoch: 2500, Train loss: 0.6782220005989075, Train return: 7.0611245431582645%, Val loss: 0.695425271987915, Val return: -7.4485625409830805%, Test loss: 0.6940110325813293, Test return: -2.763180264195249%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21208 14708]\n",
      " [16127 19957]]\n",
      "[[4785 4242]\n",
      " [4530 4443]]\n",
      "[[5325 4517]\n",
      " [4986 4879]]\n",
      "Epoch: 2550, Train loss: 0.6779370307922363, Train return: 7.270357944117527%, Val loss: 0.6955227255821228, Val return: -7.191407055693546%, Test loss: 0.6940821409225464, Test return: -2.7754737455612895%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21250 14666]\n",
      " [16119 19965]]\n",
      "[[4794 4233]\n",
      " [4539 4434]]\n",
      "[[5337 4505]\n",
      " [5000 4865]]\n",
      "Epoch: 2600, Train loss: 0.6776472330093384, Train return: 7.615134206876167%, Val loss: 0.6956167817115784, Val return: -7.102493401213442%, Test loss: 0.6941492557525635, Test return: -2.5696097439440546%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21266 14650]\n",
      " [16073 20011]]\n",
      "[[4792 4235]\n",
      " [4529 4444]]\n",
      "[[5337 4505]\n",
      " [4991 4874]]\n",
      "Epoch: 2650, Train loss: 0.677356481552124, Train return: 7.754216357533203%, Val loss: 0.6957029104232788, Val return: -7.133358763849737%, Test loss: 0.6942232251167297, Test return: -2.4662490157483523%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21288 14628]\n",
      " [16046 20038]]\n",
      "[[4788 4239]\n",
      " [4531 4442]]\n",
      "[[5342 4500]\n",
      " [4998 4867]]\n",
      "Epoch: 2700, Train loss: 0.6770590543746948, Train return: 8.004774913185624%, Val loss: 0.695803165435791, Val return: -6.84627709077293%, Test loss: 0.6942877769470215, Test return: -2.403215337062691%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21301 14615]\n",
      " [15999 20085]]\n",
      "[[4789 4238]\n",
      " [4529 4444]]\n",
      "[[5326 4516]\n",
      " [4990 4875]]\n",
      "Epoch: 2750, Train loss: 0.6767603754997253, Train return: 8.331154818197428%, Val loss: 0.6958966255187988, Val return: -6.596220349863853%, Test loss: 0.6943570375442505, Test return: -2.521875203018124%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21354 14562]\n",
      " [15988 20096]]\n",
      "[[4799 4228]\n",
      " [4519 4454]]\n",
      "[[5331 4511]\n",
      " [4990 4875]]\n",
      "Epoch: 2800, Train loss: 0.6764582991600037, Train return: 8.551888494296234%, Val loss: 0.6959887146949768, Val return: -6.3376675680221695%, Test loss: 0.694427490234375, Test return: -2.4874605103060268%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21412 14504]\n",
      " [15987 20097]]\n",
      "[[4810 4217]\n",
      " [4524 4449]]\n",
      "[[5335 4507]\n",
      " [4997 4868]]\n",
      "Epoch: 2850, Train loss: 0.6761488914489746, Train return: 8.791602153148874%, Val loss: 0.6960837244987488, Val return: -6.386380632106775%, Test loss: 0.6944929957389832, Test return: -2.32911140334247%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21408 14508]\n",
      " [15920 20164]]\n",
      "[[4797 4230]\n",
      " [4511 4462]]\n",
      "[[5325 4517]\n",
      " [4979 4886]]\n",
      "Epoch: 2900, Train loss: 0.6758365631103516, Train return: 9.155371369242236%, Val loss: 0.6961912512779236, Val return: -6.442651704004949%, Test loss: 0.6945721507072449, Test return: -2.2019298985241162%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21412 14504]\n",
      " [15863 20221]]\n",
      "[[4787 4240]\n",
      " [4503 4470]]\n",
      "[[5321 4521]\n",
      " [4979 4886]]\n",
      "Epoch: 2950, Train loss: 0.6755204200744629, Train return: 9.471996838791467%, Val loss: 0.6962905526161194, Val return: -6.348292639741639%, Test loss: 0.6946403980255127, Test return: -2.1984558907123617%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21425 14491]\n",
      " [15838 20246]]\n",
      "[[4784 4243]\n",
      " [4504 4469]]\n",
      "[[5319 4523]\n",
      " [4976 4889]]\n",
      "Epoch: 3000, Train loss: 0.6752036213874817, Train return: 9.603830744033012%, Val loss: 0.6963979601860046, Val return: -6.187098723391997%, Test loss: 0.6947280168533325, Test return: -2.21950510802593%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21485 14431]\n",
      " [15843 20241]]\n",
      "[[4794 4233]\n",
      " [4512 4461]]\n",
      "[[5325 4517]\n",
      " [4981 4884]]\n",
      "Epoch: 3050, Train loss: 0.6748798489570618, Train return: 9.826843921266363%, Val loss: 0.6964978575706482, Val return: -6.435970972965042%, Test loss: 0.6948051452636719, Test return: -2.0572084615549437%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21495 14421]\n",
      " [15793 20291]]\n",
      "[[4793 4234]\n",
      " [4506 4467]]\n",
      "[[5326 4516]\n",
      " [4976 4889]]\n",
      "Epoch: 3100, Train loss: 0.6745507121086121, Train return: 10.077176637456898%, Val loss: 0.6966005563735962, Val return: -6.452822515005491%, Test loss: 0.6948820948600769, Test return: -2.063763976889705%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21547 14369]\n",
      " [15778 20306]]\n",
      "[[4802 4225]\n",
      " [4511 4462]]\n",
      "[[5326 4516]\n",
      " [4971 4894]]\n",
      "Epoch: 3150, Train loss: 0.6742168664932251, Train return: 10.244715935156393%, Val loss: 0.6967010498046875, Val return: -6.524712445389488%, Test loss: 0.6949604153633118, Test return: -1.9411065502088116%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21550 14366]\n",
      " [15727 20357]]\n",
      "[[4786 4241]\n",
      " [4506 4467]]\n",
      "[[5324 4518]\n",
      " [4967 4898]]\n",
      "Epoch: 3200, Train loss: 0.6738784313201904, Train return: 10.505560336588866%, Val loss: 0.696792721748352, Val return: -6.463342504242192%, Test loss: 0.6950297951698303, Test return: -2.04498306000835%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21551 14365]\n",
      " [15665 20419]]\n",
      "[[4780 4247]\n",
      " [4500 4473]]\n",
      "[[5313 4529]\n",
      " [4968 4897]]\n",
      "Epoch: 3250, Train loss: 0.6735365390777588, Train return: 10.839366107849822%, Val loss: 0.6968976259231567, Val return: -6.50668842821135%, Test loss: 0.6951125264167786, Test return: -1.9624499104394968%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21606 14310]\n",
      " [15651 20433]]\n",
      "[[4781 4246]\n",
      " [4509 4464]]\n",
      "[[5312 4530]\n",
      " [4978 4887]]\n",
      "Epoch: 3300, Train loss: 0.6731884479522705, Train return: 11.074369893305567%, Val loss: 0.6970004439353943, Val return: -6.186939607518506%, Test loss: 0.6951979398727417, Test return: -1.6782012967484723%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21654 14262]\n",
      " [15625 20459]]\n",
      "[[4789 4238]\n",
      " [4514 4459]]\n",
      "[[5326 4516]\n",
      " [4974 4891]]\n",
      "Epoch: 3350, Train loss: 0.6728340983390808, Train return: 11.296546594306411%, Val loss: 0.6971036195755005, Val return: -6.094869072191212%, Test loss: 0.6952741742134094, Test return: -1.6681319574772042%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21663 14253]\n",
      " [15581 20503]]\n",
      "[[4788 4239]\n",
      " [4512 4461]]\n",
      "[[5323 4519]\n",
      " [4972 4893]]\n",
      "Epoch: 3400, Train loss: 0.6724779605865479, Train return: 11.520093033519041%, Val loss: 0.6972161531448364, Val return: -6.2375336233241745%, Test loss: 0.6953617334365845, Test return: -1.526054002411963%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21686 14230]\n",
      " [15542 20542]]\n",
      "[[4781 4246]\n",
      " [4519 4454]]\n",
      "[[5326 4516]\n",
      " [4973 4892]]\n",
      "Epoch: 3450, Train loss: 0.6721189022064209, Train return: 11.842399902567461%, Val loss: 0.6973234415054321, Val return: -6.1802984827858%, Test loss: 0.6954405307769775, Test return: -1.677559410497845%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21727 14189]\n",
      " [15511 20573]]\n",
      "[[4785 4242]\n",
      " [4520 4453]]\n",
      "[[5324 4518]\n",
      " [4983 4882]]\n",
      "Epoch: 3500, Train loss: 0.6717534065246582, Train return: 12.162862100369336%, Val loss: 0.6974309682846069, Val return: -6.070546026222741%, Test loss: 0.6955330967903137, Test return: -1.809593691806575%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21797 14119]\n",
      " [15528 20556]]\n",
      "[[4801 4226]\n",
      " [4532 4441]]\n",
      "[[5329 4513]\n",
      " [5005 4860]]\n",
      "Epoch: 3550, Train loss: 0.671381950378418, Train return: 12.369985292704202%, Val loss: 0.6975451111793518, Val return: -5.978529366278893%, Test loss: 0.6956265568733215, Test return: -1.691489155541908%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21828 14088]\n",
      " [15492 20592]]\n",
      "[[4802 4225]\n",
      " [4537 4436]]\n",
      "[[5323 4519]\n",
      " [5000 4865]]\n",
      "Epoch: 3600, Train loss: 0.6710030436515808, Train return: 12.587166414070094%, Val loss: 0.6976575255393982, Val return: -5.904572328668442%, Test loss: 0.6957097053527832, Test return: -1.694638022756083%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21853 14063]\n",
      " [15452 20632]]\n",
      "[[4793 4234]\n",
      " [4533 4440]]\n",
      "[[5315 4527]\n",
      " [4998 4867]]\n",
      "Epoch: 3650, Train loss: 0.6706151962280273, Train return: 12.813173985373858%, Val loss: 0.69776451587677, Val return: -5.8975857363220126%, Test loss: 0.6957955956459045, Test return: -1.7259222803214518%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21855 14061]\n",
      " [15373 20711]]\n",
      "[[4786 4241]\n",
      " [4529 4444]]\n",
      "[[5307 4535]\n",
      " [4980 4885]]\n",
      "Epoch: 3700, Train loss: 0.6702279448509216, Train return: 13.11988873971617%, Val loss: 0.6978935599327087, Val return: -5.76602866585877%, Test loss: 0.6958861947059631, Test return: -1.7381050334769925%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21884 14032]\n",
      " [15349 20735]]\n",
      "[[4786 4241]\n",
      " [4530 4443]]\n",
      "[[5309 4533]\n",
      " [4991 4874]]\n",
      "Epoch: 3750, Train loss: 0.6698331236839294, Train return: 13.412847910829163%, Val loss: 0.6980090737342834, Val return: -5.709722735517134%, Test loss: 0.6959856152534485, Test return: -1.4924705682443977%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21941 13975]\n",
      " [15322 20762]]\n",
      "[[4797 4230]\n",
      " [4538 4435]]\n",
      "[[5316 4526]\n",
      " [5002 4863]]\n",
      "Epoch: 3800, Train loss: 0.6694311499595642, Train return: 13.657174375605274%, Val loss: 0.6981214880943298, Val return: -5.663390101986447%, Test loss: 0.696079432964325, Test return: -1.3084062290824345%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 0 1 1]\n",
      "[[21970 13946]\n",
      " [15279 20805]]\n",
      "[[4799 4228]\n",
      " [4540 4433]]\n",
      "[[5327 4515]\n",
      " [4998 4867]]\n",
      "Epoch: 3850, Train loss: 0.6690239310264587, Train return: 13.911739789327005%, Val loss: 0.6982437372207642, Val return: -5.597966815528515%, Test loss: 0.6961803436279297, Test return: -1.235970091009421%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22005 13911]\n",
      " [15244 20840]]\n",
      "[[4794 4233]\n",
      " [4543 4430]]\n",
      "[[5317 4525]\n",
      " [4999 4866]]\n",
      "Epoch: 3900, Train loss: 0.6686118245124817, Train return: 14.180076278365947%, Val loss: 0.6983562707901001, Val return: -5.672913712643903%, Test loss: 0.6962816715240479, Test return: -1.393895544541803%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22060 13856]\n",
      " [15252 20832]]\n",
      "[[4807 4220]\n",
      " [4558 4415]]\n",
      "[[5328 4514]\n",
      " [5010 4855]]\n",
      "Epoch: 3950, Train loss: 0.6681938767433167, Train return: 14.465622515460764%, Val loss: 0.6985002756118774, Val return: -5.9228263024974455%, Test loss: 0.6963915824890137, Test return: -1.449679691859856%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22047 13869]\n",
      " [15187 20897]]\n",
      "[[4796 4231]\n",
      " [4568 4405]]\n",
      "[[5320 4522]\n",
      " [5004 4861]]\n",
      "Epoch: 4000, Train loss: 0.6677684783935547, Train return: 14.700401434264169%, Val loss: 0.698619544506073, Val return: -5.644815895834492%, Test loss: 0.6964861154556274, Test return: -1.3623565241758804%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22116 13800]\n",
      " [15189 20895]]\n",
      "[[4808 4219]\n",
      " [4573 4400]]\n",
      "[[5329 4513]\n",
      " [5015 4850]]\n",
      "Epoch: 4050, Train loss: 0.6673358678817749, Train return: 15.006583529360054%, Val loss: 0.6987484097480774, Val return: -5.6657719673483555%, Test loss: 0.6965905427932739, Test return: -1.4929923127962506%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22104 13812]\n",
      " [15109 20975]]\n",
      "[[4798 4229]\n",
      " [4558 4415]]\n",
      "[[5315 4527]\n",
      " [5013 4852]]\n",
      "Epoch: 4100, Train loss: 0.6668951511383057, Train return: 15.178563163182304%, Val loss: 0.6988693475723267, Val return: -5.741549363438767%, Test loss: 0.6966808438301086, Test return: -1.3664475800669125%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22123 13793]\n",
      " [15080 21004]]\n",
      "[[4790 4237]\n",
      " [4557 4416]]\n",
      "[[5323 4519]\n",
      " [5016 4849]]\n",
      "Epoch: 4150, Train loss: 0.6664511561393738, Train return: 15.50225342301275%, Val loss: 0.6989953517913818, Val return: -5.436895706509887%, Test loss: 0.6967931985855103, Test return: -1.2856096714311511%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22122 13794]\n",
      " [15021 21063]]\n",
      "[[4784 4243]\n",
      " [4551 4422]]\n",
      "[[5312 4530]\n",
      " [5015 4850]]\n",
      "Epoch: 4200, Train loss: 0.6660048961639404, Train return: 15.802065425058142%, Val loss: 0.6991465091705322, Val return: -5.331895991519584%, Test loss: 0.6969118714332581, Test return: -1.396378287738716%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22180 13736]\n",
      " [15003 21081]]\n",
      "[[4783 4244]\n",
      " [4548 4425]]\n",
      "[[5321 4521]\n",
      " [5016 4849]]\n",
      "Epoch: 4250, Train loss: 0.6655482053756714, Train return: 16.176836723392825%, Val loss: 0.6992803812026978, Val return: -5.480265782655441%, Test loss: 0.6970157027244568, Test return: -1.293023313421556%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22249 13667]\n",
      " [14997 21087]]\n",
      "[[4795 4232]\n",
      " [4571 4402]]\n",
      "[[5324 4518]\n",
      " [5015 4850]]\n",
      "Epoch: 4300, Train loss: 0.6650786995887756, Train return: 16.469328508095447%, Val loss: 0.6994125843048096, Val return: -5.4351038224698165%, Test loss: 0.6971212029457092, Test return: -1.3918291087528418%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22251 13665]\n",
      " [14912 21172]]\n",
      "[[4772 4255]\n",
      " [4546 4427]]\n",
      "[[5302 4540]\n",
      " [5004 4861]]\n",
      "Epoch: 4350, Train loss: 0.6646146178245544, Train return: 16.768724774676215%, Val loss: 0.6995596289634705, Val return: -5.3675698671993715%, Test loss: 0.6972492337226868, Test return: -1.3682853045445378%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22337 13579]\n",
      " [14914 21170]]\n",
      "[[4785 4242]\n",
      " [4564 4409]]\n",
      "[[5321 4521]\n",
      " [5022 4843]]\n",
      "Epoch: 4400, Train loss: 0.6641365885734558, Train return: 17.17228660418582%, Val loss: 0.6996845006942749, Val return: -5.439284661811094%, Test loss: 0.6973588466644287, Test return: -1.1350980689586483%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22403 13513]\n",
      " [14864 21220]]\n",
      "[[4779 4248]\n",
      " [4568 4405]]\n",
      "[[5332 4510]\n",
      " [5017 4848]]\n",
      "Epoch: 4450, Train loss: 0.6636476516723633, Train return: 17.57553655427859%, Val loss: 0.6998311877250671, Val return: -5.34781851704963%, Test loss: 0.6974726915359497, Test return: -1.1719038001947286%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22376 13540]\n",
      " [14774 21310]]\n",
      "[[4776 4251]\n",
      " [4558 4415]]\n",
      "[[5320 4522]\n",
      " [5008 4857]]\n",
      "Epoch: 4500, Train loss: 0.6631574034690857, Train return: 17.843785641301384%, Val loss: 0.6999649405479431, Val return: -5.537122386186732%, Test loss: 0.697586715221405, Test return: -1.2392199888257305%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22418 13498]\n",
      " [14727 21357]]\n",
      "[[4777 4250]\n",
      " [4557 4416]]\n",
      "[[5321 4521]\n",
      " [5008 4857]]\n",
      "Epoch: 4550, Train loss: 0.6626638770103455, Train return: 18.144269354915934%, Val loss: 0.7001170516014099, Val return: -5.552834828303382%, Test loss: 0.6977177262306213, Test return: -1.103268599236772%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22496 13420]\n",
      " [14739 21345]]\n",
      "[[4786 4241]\n",
      " [4573 4400]]\n",
      "[[5337 4505]\n",
      " [5015 4850]]\n",
      "Epoch: 4600, Train loss: 0.6621518731117249, Train return: 18.49226190244381%, Val loss: 0.7002481818199158, Val return: -5.472159262783964%, Test loss: 0.6978253722190857, Test return: -1.0234368509709881%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22484 13432]\n",
      " [14623 21461]]\n",
      "[[4774 4253]\n",
      " [4558 4415]]\n",
      "[[5318 4524]\n",
      " [4995 4870]]\n",
      "Epoch: 4650, Train loss: 0.6616393327713013, Train return: 18.80833427799366%, Val loss: 0.7004160284996033, Val return: -5.527303011397265%, Test loss: 0.6979790329933167, Test return: -0.9585597457498046%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22569 13347]\n",
      " [14629 21455]]\n",
      "[[4787 4240]\n",
      " [4579 4394]]\n",
      "[[5335 4507]\n",
      " [5015 4850]]\n",
      "Epoch: 4700, Train loss: 0.6611161231994629, Train return: 18.990165689592963%, Val loss: 0.7005631327629089, Val return: -5.494929550882135%, Test loss: 0.6981003880500793, Test return: -0.99625696005481%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22563 13353]\n",
      " [14568 21516]]\n",
      "[[4781 4246]\n",
      " [4579 4394]]\n",
      "[[5329 4513]\n",
      " [5011 4854]]\n",
      "Epoch: 4750, Train loss: 0.6605913043022156, Train return: 19.331437367611624%, Val loss: 0.7007133960723877, Val return: -5.703566196157453%, Test loss: 0.698236346244812, Test return: -1.1630814389796593%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22611 13305]\n",
      " [14533 21551]]\n",
      "[[4774 4253]\n",
      " [4590 4383]]\n",
      "[[5329 4513]\n",
      " [5020 4845]]\n",
      "Epoch: 4800, Train loss: 0.6600563526153564, Train return: 19.675998097448407%, Val loss: 0.7008708715438843, Val return: -5.585080741269277%, Test loss: 0.698371946811676, Test return: -1.0793070866761763%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22682 13234]\n",
      " [14511 21573]]\n",
      "[[4778 4249]\n",
      " [4597 4376]]\n",
      "[[5333 4509]\n",
      " [5026 4839]]\n",
      "Epoch: 4850, Train loss: 0.6595163345336914, Train return: 19.95897903513082%, Val loss: 0.7010436058044434, Val return: -5.486948170223219%, Test loss: 0.698517918586731, Test return: -1.1316846591373144%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22735 13181]\n",
      " [14486 21598]]\n",
      "[[4771 4256]\n",
      " [4589 4384]]\n",
      "[[5326 4516]\n",
      " [5035 4830]]\n",
      "Epoch: 4900, Train loss: 0.6589587926864624, Train return: 20.333754366598377%, Val loss: 0.701198935508728, Val return: -5.277187899542943%, Test loss: 0.6986328959465027, Test return: -1.1171364329050943%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[22775 13141]\n",
      " [14422 21662]]\n",
      "[[4772 4255]\n",
      " [4579 4394]]\n",
      "[[5321 4521]\n",
      " [5030 4835]]\n",
      "Epoch: 4950, Train loss: 0.6584010124206543, Train return: 20.587370779680374%, Val loss: 0.7013623118400574, Val return: -5.29835468140229%, Test loss: 0.6987846493721008, Test return: -1.0240747434580926%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[22788 13128]\n",
      " [14372 21712]]\n",
      "[[4776 4251]\n",
      " [4572 4401]]\n",
      "[[5318 4524]\n",
      " [5028 4837]]\n",
      "Epoch: 5000, Train loss: 0.6578305959701538, Train return: 20.99817791124939%, Val loss: 0.7015202045440674, Val return: -5.339290867016694%, Test loss: 0.6989060640335083, Test return: -1.2007729047668774%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[22848 13068]\n",
      " [14304 21780]]\n",
      "[[4764 4263]\n",
      " [4565 4408]]\n",
      "[[5307 4535]\n",
      " [5024 4841]]\n",
      "Epoch: 5050, Train loss: 0.6572552919387817, Train return: 21.275573473340888%, Val loss: 0.7016786932945251, Val return: -5.194715844835967%, Test loss: 0.6990528106689453, Test return: -1.1610704548549682%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[22878 13038]\n",
      " [14243 21841]]\n",
      "[[4765 4262]\n",
      " [4556 4417]]\n",
      "[[5309 4533]\n",
      " [5026 4839]]\n",
      "Epoch: 5100, Train loss: 0.656674325466156, Train return: 21.49863216553336%, Val loss: 0.7018582224845886, Val return: -5.268547773815914%, Test loss: 0.6992153525352478, Test return: -1.3698396701600164%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[22942 12974]\n",
      " [14219 21865]]\n",
      "[[4769 4258]\n",
      " [4570 4403]]\n",
      "[[5299 4543]\n",
      " [5038 4827]]\n",
      "Epoch: 5150, Train loss: 0.6560840010643005, Train return: 21.742858443709522%, Val loss: 0.702021598815918, Val return: -4.9813758862311275%, Test loss: 0.699360191822052, Test return: -1.1668511532624846%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[22995 12921]\n",
      " [14211 21873]]\n",
      "[[4781 4246]\n",
      " [4568 4405]]\n",
      "[[5325 4517]\n",
      " [5035 4830]]\n",
      "Epoch: 5200, Train loss: 0.6554765105247498, Train return: 21.990574513145415%, Val loss: 0.7021663188934326, Val return: -4.677898817726071%, Test loss: 0.6994855403900146, Test return: -1.0373995718146283%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[22949 12967]\n",
      " [14061 22023]]\n",
      "[[4757 4270]\n",
      " [4548 4425]]\n",
      "[[5305 4537]\n",
      " [5014 4851]]\n",
      "Epoch: 5250, Train loss: 0.6548682451248169, Train return: 22.242653792548%, Val loss: 0.7023322582244873, Val return: -4.424766161955239%, Test loss: 0.6996344923973083, Test return: -1.2581671963708212%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23007 12909]\n",
      " [14046 22038]]\n",
      "[[4764 4263]\n",
      " [4550 4423]]\n",
      "[[5304 4538]\n",
      " [5022 4843]]\n",
      "Epoch: 5300, Train loss: 0.6542549729347229, Train return: 22.584431628103363%, Val loss: 0.7025378942489624, Val return: -4.10272989034375%, Test loss: 0.6998019814491272, Test return: -1.0283323598376%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23052 12864]\n",
      " [14010 22074]]\n",
      "[[4773 4254]\n",
      " [4557 4416]]\n",
      "[[5307 4535]\n",
      " [5020 4845]]\n",
      "Epoch: 5350, Train loss: 0.6536239981651306, Train return: 22.86086863256421%, Val loss: 0.7027363777160645, Val return: -4.141653794707299%, Test loss: 0.69996577501297, Test return: -1.0254494433226005%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23049 12867]\n",
      " [13920 22164]]\n",
      "[[4760 4267]\n",
      " [4539 4434]]\n",
      "[[5297 4545]\n",
      " [5008 4857]]\n",
      "Epoch: 5400, Train loss: 0.6529920101165771, Train return: 23.13210068346483%, Val loss: 0.7029144763946533, Val return: -3.9998703329106093%, Test loss: 0.7001163363456726, Test return: -0.9868730774753242%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23120 12796]\n",
      " [13917 22167]]\n",
      "[[4767 4260]\n",
      " [4538 4435]]\n",
      "[[5298 4544]\n",
      " [5008 4857]]\n",
      "Epoch: 5450, Train loss: 0.6523508429527283, Train return: 23.547999381952902%, Val loss: 0.7030478715896606, Val return: -3.953873253311449%, Test loss: 0.7002665996551514, Test return: -1.1488939845059365%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23174 12742]\n",
      " [13871 22213]]\n",
      "[[4768 4259]\n",
      " [4538 4435]]\n",
      "[[5308 4534]\n",
      " [5018 4847]]\n",
      "Epoch: 5500, Train loss: 0.6517013311386108, Train return: 23.765874186833386%, Val loss: 0.7032741904258728, Val return: -3.989397067953952%, Test loss: 0.700474202632904, Test return: -1.189293343472232%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23266 12650]\n",
      " [13899 22185]]\n",
      "[[4777 4250]\n",
      " [4553 4420]]\n",
      "[[5325 4517]\n",
      " [5044 4821]]\n",
      "Epoch: 5550, Train loss: 0.6510429382324219, Train return: 24.006961306396363%, Val loss: 0.7034644484519958, Val return: -4.027217813951543%, Test loss: 0.7006386518478394, Test return: -1.1967183681354068%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23325 12591]\n",
      " [13875 22209]]\n",
      "[[4776 4251]\n",
      " [4552 4421]]\n",
      "[[5328 4514]\n",
      " [5055 4810]]\n",
      "Epoch: 5600, Train loss: 0.6503570675849915, Train return: 24.300271599756485%, Val loss: 0.7036473751068115, Val return: -3.9586179339647245%, Test loss: 0.7007846832275391, Test return: -1.2032630011346628%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23276 12640]\n",
      " [13743 22341]]\n",
      "[[4755 4272]\n",
      " [4526 4447]]\n",
      "[[5312 4530]\n",
      " [5033 4832]]\n",
      "Epoch: 5650, Train loss: 0.6496833562850952, Train return: 24.675428161351388%, Val loss: 0.7038275599479675, Val return: -3.8872659266027676%, Test loss: 0.7009725570678711, Test return: -1.143062778838476%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23365 12551]\n",
      " [13736 22348]]\n",
      "[[4765 4262]\n",
      " [4541 4432]]\n",
      "[[5310 4532]\n",
      " [5042 4823]]\n",
      "Epoch: 5700, Train loss: 0.6489918231964111, Train return: 25.00265422403209%, Val loss: 0.7040413618087769, Val return: -3.9522605138411104%, Test loss: 0.7011479139328003, Test return: -0.9989377957093833%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23381 12535]\n",
      " [13648 22436]]\n",
      "[[4750 4277]\n",
      " [4524 4449]]\n",
      "[[5306 4536]\n",
      " [5033 4832]]\n",
      "Epoch: 5750, Train loss: 0.6482889652252197, Train return: 25.310475748013573%, Val loss: 0.7041955590248108, Val return: -3.96155620919974%, Test loss: 0.7013105750083923, Test return: -0.9568697333411283%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23406 12510]\n",
      " [13599 22485]]\n",
      "[[4745 4282]\n",
      " [4524 4449]]\n",
      "[[5296 4546]\n",
      " [5025 4840]]\n",
      "Epoch: 5800, Train loss: 0.6475843191146851, Train return: 25.53709416956915%, Val loss: 0.7044024467468262, Val return: -3.761218755827093%, Test loss: 0.7015067338943481, Test return: -1.029305612310916%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23491 12425]\n",
      " [13584 22500]]\n",
      "[[4748 4279]\n",
      " [4534 4439]]\n",
      "[[5302 4540]\n",
      " [5031 4834]]\n",
      "Epoch: 5850, Train loss: 0.6468660235404968, Train return: 25.915389833530693%, Val loss: 0.7046239972114563, Val return: -4.021786237450329%, Test loss: 0.7016878724098206, Test return: -0.9264643360502669%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23539 12377]\n",
      " [13542 22542]]\n",
      "[[4733 4294]\n",
      " [4542 4431]]\n",
      "[[5309 4533]\n",
      " [5041 4824]]\n",
      "Epoch: 5900, Train loss: 0.6461418867111206, Train return: 26.313895149862336%, Val loss: 0.7048088908195496, Val return: -3.971150864514925%, Test loss: 0.7018637657165527, Test return: -0.9942434813914722%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23581 12335]\n",
      " [13483 22601]]\n",
      "[[4731 4296]\n",
      " [4536 4437]]\n",
      "[[5312 4530]\n",
      " [5041 4824]]\n",
      "Epoch: 5950, Train loss: 0.64540034532547, Train return: 26.648235767593288%, Val loss: 0.7049702405929565, Val return: -3.9404578011654907%, Test loss: 0.7020232081413269, Test return: -1.0082944099079465%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23556 12360]\n",
      " [13358 22726]]\n",
      "[[4710 4317]\n",
      " [4516 4457]]\n",
      "[[5298 4544]\n",
      " [5027 4838]]\n",
      "Epoch: 6000, Train loss: 0.6446447372436523, Train return: 26.80579291761407%, Val loss: 0.7051588296890259, Val return: -3.6911519668898807%, Test loss: 0.7022128701210022, Test return: -1.0421038589263754%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23589 12327]\n",
      " [13324 22760]]\n",
      "[[4713 4314]\n",
      " [4507 4466]]\n",
      "[[5293 4549]\n",
      " [5025 4840]]\n",
      "Epoch: 6050, Train loss: 0.6439064741134644, Train return: 27.284943387911976%, Val loss: 0.70539790391922, Val return: -3.848624906229889%, Test loss: 0.7024386525154114, Test return: -1.2792473541834497%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23764 12152]\n",
      " [13410 22674]]\n",
      "[[4742 4285]\n",
      " [4542 4431]]\n",
      "[[5325 4517]\n",
      " [5071 4794]]\n",
      "Epoch: 6100, Train loss: 0.6431370377540588, Train return: 27.46166742923084%, Val loss: 0.7056165337562561, Val return: -3.4710145073210956%, Test loss: 0.7026405334472656, Test return: -1.0812015440388543%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23826 12090]\n",
      " [13407 22677]]\n",
      "[[4758 4269]\n",
      " [4541 4432]]\n",
      "[[5328 4514]\n",
      " [5076 4789]]\n",
      "Epoch: 6150, Train loss: 0.6423479914665222, Train return: 27.78514705598042%, Val loss: 0.705792248249054, Val return: -3.3912930381440844%, Test loss: 0.7028003931045532, Test return: -0.8666849456404391%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23721 12195]\n",
      " [13203 22881]]\n",
      "[[4717 4310]\n",
      " [4497 4476]]\n",
      "[[5294 4548]\n",
      " [5040 4825]]\n",
      "Epoch: 6200, Train loss: 0.6415672302246094, Train return: 28.07055963083325%, Val loss: 0.7060397863388062, Val return: -3.1686278444658758%, Test loss: 0.7030614018440247, Test return: -0.9033933898692763%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23854 12062]\n",
      " [13243 22841]]\n",
      "[[4741 4286]\n",
      " [4520 4453]]\n",
      "[[5303 4539]\n",
      " [5063 4802]]\n",
      "Epoch: 6250, Train loss: 0.640774130821228, Train return: 28.405125281399506%, Val loss: 0.7062722444534302, Val return: -3.4791524171215165%, Test loss: 0.7032591700553894, Test return: -0.9185410380136818%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23937 11979]\n",
      " [13222 22862]]\n",
      "[[4745 4282]\n",
      " [4531 4442]]\n",
      "[[5309 4533]\n",
      " [5075 4790]]\n",
      "Epoch: 6300, Train loss: 0.6399529576301575, Train return: 28.59300346429574%, Val loss: 0.7064453363418579, Val return: -3.3132709550392034%, Test loss: 0.7034323811531067, Test return: -0.7900341058298727%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23865 12051]\n",
      " [13079 23005]]\n",
      "[[4712 4315]\n",
      " [4495 4478]]\n",
      "[[5275 4567]\n",
      " [5047 4818]]\n",
      "Epoch: 6350, Train loss: 0.63913494348526, Train return: 28.876258486274047%, Val loss: 0.7066938281059265, Val return: -3.2561647153059106%, Test loss: 0.7036917209625244, Test return: -0.8851353647416703%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[23922 11994]\n",
      " [13060 23024]]\n",
      "[[4718 4309]\n",
      " [4501 4472]]\n",
      "[[5275 4567]\n",
      " [5053 4812]]\n",
      "Epoch: 6400, Train loss: 0.6383153200149536, Train return: 29.20782030038131%, Val loss: 0.7068904638290405, Val return: -3.272012332645525%, Test loss: 0.7038888931274414, Test return: -1.0179057672509575%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24025 11891]\n",
      " [13051 23033]]\n",
      "[[4724 4303]\n",
      " [4509 4464]]\n",
      "[[5282 4560]\n",
      " [5075 4790]]\n",
      "Epoch: 6450, Train loss: 0.6374726295471191, Train return: 29.487699060338844%, Val loss: 0.7071029543876648, Val return: -3.133616987696114%, Test loss: 0.7040805816650391, Test return: -0.9499579653655478%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24063 11853]\n",
      " [12981 23103]]\n",
      "[[4729 4298]\n",
      " [4508 4465]]\n",
      "[[5276 4566]\n",
      " [5064 4801]]\n",
      "Epoch: 6500, Train loss: 0.6366303563117981, Train return: 29.89453151389258%, Val loss: 0.7073732018470764, Val return: -2.897921125263364%, Test loss: 0.704365611076355, Test return: -0.7558017324431928%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24119 11797]\n",
      " [12928 23156]]\n",
      "[[4731 4296]\n",
      " [4514 4459]]\n",
      "[[5288 4554]\n",
      " [5071 4794]]\n",
      "Epoch: 6550, Train loss: 0.6357660889625549, Train return: 30.085754162957805%, Val loss: 0.7076113820075989, Val return: -3.0788657087635625%, Test loss: 0.7045776844024658, Test return: -0.8174600572520377%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24138 11778]\n",
      " [12869 23215]]\n",
      "[[4715 4312]\n",
      " [4502 4471]]\n",
      "[[5280 4562]\n",
      " [5060 4805]]\n",
      "Epoch: 6600, Train loss: 0.6348956227302551, Train return: 30.474509865689704%, Val loss: 0.7078587412834167, Val return: -2.785608596618291%, Test loss: 0.7047987580299377, Test return: -0.7671201404656587%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24181 11735]\n",
      " [12798 23286]]\n",
      "[[4717 4310]\n",
      " [4494 4479]]\n",
      "[[5279 4563]\n",
      " [5064 4801]]\n",
      "Epoch: 6650, Train loss: 0.6340230107307434, Train return: 30.81109776039877%, Val loss: 0.7080848813056946, Val return: -2.657865982306825%, Test loss: 0.7050449252128601, Test return: -0.6859728322000448%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24307 11609]\n",
      " [12845 23239]]\n",
      "[[4743 4284]\n",
      " [4513 4460]]\n",
      "[[5305 4537]\n",
      " [5090 4775]]\n",
      "Epoch: 6700, Train loss: 0.6331338286399841, Train return: 31.13725579452587%, Val loss: 0.7083312273025513, Val return: -2.817307190226346%, Test loss: 0.7052701711654663, Test return: -0.762376017573052%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24324 11592]\n",
      " [12767 23317]]\n",
      "[[4726 4301]\n",
      " [4506 4467]]\n",
      "[[5293 4549]\n",
      " [5082 4783]]\n",
      "Epoch: 6750, Train loss: 0.6322312355041504, Train return: 31.50123038656573%, Val loss: 0.7086353898048401, Val return: -3.1436497793427582%, Test loss: 0.705556333065033, Test return: -0.530307439617254%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24309 11607]\n",
      " [12655 23429]]\n",
      "[[4698 4329]\n",
      " [4501 4472]]\n",
      "[[5287 4555]\n",
      " [5061 4804]]\n",
      "Epoch: 6800, Train loss: 0.6313189268112183, Train return: 31.801557601841406%, Val loss: 0.7088688015937805, Val return: -3.196965712552164%, Test loss: 0.7057780027389526, Test return: -0.621065490525809%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24339 11577]\n",
      " [12587 23497]]\n",
      "[[4698 4329]\n",
      " [4508 4465]]\n",
      "[[5279 4563]\n",
      " [5061 4804]]\n",
      "Epoch: 6850, Train loss: 0.6304136514663696, Train return: 32.15914063382589%, Val loss: 0.7091144919395447, Val return: -3.1732443022162453%, Test loss: 0.7060406804084778, Test return: -0.6887150753192574%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24496 11420]\n",
      " [12629 23455]]\n",
      "[[4719 4308]\n",
      " [4535 4438]]\n",
      "[[5303 4539]\n",
      " [5090 4775]]\n",
      "Epoch: 6900, Train loss: 0.6294618844985962, Train return: 32.50440252157599%, Val loss: 0.7093466520309448, Val return: -3.283973725400379%, Test loss: 0.706243634223938, Test return: -0.43781529480008974%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24395 11521]\n",
      " [12423 23661]]\n",
      "[[4674 4353]\n",
      " [4508 4465]]\n",
      "[[5264 4578]\n",
      " [5029 4836]]\n",
      "Epoch: 6950, Train loss: 0.6285347938537598, Train return: 32.716501869313134%, Val loss: 0.7096420526504517, Val return: -3.177556998627732%, Test loss: 0.7065104246139526, Test return: -0.443450012662975%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24537 11379]\n",
      " [12494 23590]]\n",
      "[[4706 4321]\n",
      " [4524 4449]]\n",
      "[[5297 4545]\n",
      " [5072 4793]]\n",
      "Epoch: 7000, Train loss: 0.6275851726531982, Train return: 33.038568791882916%, Val loss: 0.709896981716156, Val return: -3.260985973420011%, Test loss: 0.7067713737487793, Test return: -0.5243855978543047%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24591 11325]\n",
      " [12444 23640]]\n",
      "[[4708 4319]\n",
      " [4521 4452]]\n",
      "[[5283 4559]\n",
      " [5066 4799]]\n",
      "Epoch: 7050, Train loss: 0.626627504825592, Train return: 33.40714623484712%, Val loss: 0.7101389169692993, Val return: -3.1921048557763774%, Test loss: 0.7070255279541016, Test return: -0.4641454307752136%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24714 11202]\n",
      " [12464 23620]]\n",
      "[[4725 4302]\n",
      " [4547 4426]]\n",
      "[[5315 4527]\n",
      " [5106 4759]]\n",
      "Epoch: 7100, Train loss: 0.6256589293479919, Train return: 33.58244275869181%, Val loss: 0.7104086875915527, Val return: -3.268217793055795%, Test loss: 0.7073129415512085, Test return: -0.43291812231207144%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24680 11236]\n",
      " [12321 23763]]\n",
      "[[4708 4319]\n",
      " [4520 4453]]\n",
      "[[5286 4556]\n",
      " [5067 4798]]\n",
      "Epoch: 7150, Train loss: 0.6246631145477295, Train return: 34.149014721145114%, Val loss: 0.7106821537017822, Val return: -3.7062563689823707%, Test loss: 0.7075945734977722, Test return: -0.4357446222266476%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24710 11206]\n",
      " [12226 23858]]\n",
      "[[4681 4346]\n",
      " [4519 4454]]\n",
      "[[5279 4563]\n",
      " [5064 4801]]\n",
      "Epoch: 7200, Train loss: 0.6236631274223328, Train return: 34.477888884905845%, Val loss: 0.7109695672988892, Val return: -3.537132857583635%, Test loss: 0.7078375220298767, Test return: -0.5116143640903794%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24733 11183]\n",
      " [12141 23943]]\n",
      "[[4680 4347]\n",
      " [4499 4474]]\n",
      "[[5259 4583]\n",
      " [5049 4816]]\n",
      "Epoch: 7250, Train loss: 0.6226489543914795, Train return: 34.995528162223664%, Val loss: 0.7112502455711365, Val return: -3.5837895538394173%, Test loss: 0.7081124782562256, Test return: -0.586619667901258%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24710 11206]\n",
      " [12007 24077]]\n",
      "[[4651 4376]\n",
      " [4483 4490]]\n",
      "[[5237 4605]\n",
      " [5016 4849]]\n",
      "Epoch: 7300, Train loss: 0.6216577291488647, Train return: 35.346205407154514%, Val loss: 0.7115557789802551, Val return: -3.3367164770290865%, Test loss: 0.7084119319915771, Test return: -0.5967660197990647%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24876 11040]\n",
      " [12062 24022]]\n",
      "[[4683 4344]\n",
      " [4505 4468]]\n",
      "[[5271 4571]\n",
      " [5055 4810]]\n",
      "Epoch: 7350, Train loss: 0.6206297278404236, Train return: 35.54248857888147%, Val loss: 0.7118502259254456, Val return: -3.4330651510479266%, Test loss: 0.70872563123703, Test return: -0.5480525154073259%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[24916 11000]\n",
      " [12029 24055]]\n",
      "[[4689 4338]\n",
      " [4510 4463]]\n",
      "[[5278 4564]\n",
      " [5049 4816]]\n",
      "Epoch: 7400, Train loss: 0.6195976734161377, Train return: 35.83040417991328%, Val loss: 0.7121522426605225, Val return: -3.043810530953423%, Test loss: 0.7090318202972412, Test return: -0.487847266508548%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25013 10903]\n",
      " [12021 24063]]\n",
      "[[4698 4329]\n",
      " [4515 4458]]\n",
      "[[5288 4554]\n",
      " [5062 4803]]\n",
      "Epoch: 7450, Train loss: 0.6185450553894043, Train return: 36.1339176622003%, Val loss: 0.7124699354171753, Val return: -3.4076735069819444%, Test loss: 0.7093475461006165, Test return: -0.9108304297744767%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25010 10906]\n",
      " [11904 24180]]\n",
      "[[4675 4352]\n",
      " [4496 4477]]\n",
      "[[5251 4591]\n",
      " [5048 4817]]\n",
      "Epoch: 7500, Train loss: 0.6174809336662292, Train return: 36.509971493593525%, Val loss: 0.712721586227417, Val return: -2.854457320222236%, Test loss: 0.7096045017242432, Test return: -0.765907692828394%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25006 10910]\n",
      " [11783 24301]]\n",
      "[[4666 4361]\n",
      " [4478 4495]]\n",
      "[[5237 4605]\n",
      " [5030 4835]]\n",
      "Epoch: 7550, Train loss: 0.6164472699165344, Train return: 36.88923926210194%, Val loss: 0.7130270600318909, Val return: -2.9283431475221713%, Test loss: 0.709937572479248, Test return: -0.6166720236443609%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25207 10709]\n",
      " [11894 24190]]\n",
      "[[4706 4321]\n",
      " [4522 4451]]\n",
      "[[5278 4564]\n",
      " [5081 4784]]\n",
      "Epoch: 7600, Train loss: 0.6153613328933716, Train return: 37.1656541335477%, Val loss: 0.7133786082267761, Val return: -2.904505023329309%, Test loss: 0.7102205753326416, Test return: -0.03660812803681399%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25163 10753]\n",
      " [11762 24322]]\n",
      "[[4684 4343]\n",
      " [4504 4469]]\n",
      "[[5258 4584]\n",
      " [5048 4817]]\n",
      "Epoch: 7650, Train loss: 0.6142552495002747, Train return: 37.510283866389074%, Val loss: 0.7136099338531494, Val return: -2.9671614749930937%, Test loss: 0.7105291485786438, Test return: -0.07535050757541192%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25167 10749]\n",
      " [11656 24428]]\n",
      "[[4668 4359]\n",
      " [4483 4490]]\n",
      "[[5234 4608]\n",
      " [5033 4832]]\n",
      "Epoch: 7700, Train loss: 0.6131514310836792, Train return: 37.82671898612823%, Val loss: 0.7139267325401306, Val return: -2.8881687684946407%, Test loss: 0.7108312845230103, Test return: -0.10528134211845068%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25134 10782]\n",
      " [11488 24596]]\n",
      "[[4646 4381]\n",
      " [4456 4517]]\n",
      "[[5188 4654]\n",
      " [5002 4863]]\n",
      "Epoch: 7750, Train loss: 0.612063467502594, Train return: 38.1912834772214%, Val loss: 0.7142610549926758, Val return: -2.992183155220864%, Test loss: 0.711144745349884, Test return: -0.5267018270226327%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25311 10605]\n",
      " [11577 24507]]\n",
      "[[4684 4343]\n",
      " [4492 4481]]\n",
      "[[5238 4604]\n",
      " [5046 4819]]\n",
      "Epoch: 7800, Train loss: 0.6109482049942017, Train return: 38.50260016145089%, Val loss: 0.7146328091621399, Val return: -3.168767663063896%, Test loss: 0.7115269303321838, Test return: -0.10566076394848657%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25363 10553]\n",
      " [11513 24571]]\n",
      "[[4674 4353]\n",
      " [4482 4491]]\n",
      "[[5238 4604]\n",
      " [5034 4831]]\n",
      "Epoch: 7850, Train loss: 0.6098185777664185, Train return: 38.79854832057441%, Val loss: 0.7148677706718445, Val return: -3.0261011903585997%, Test loss: 0.7117924094200134, Test return: -0.3551895596073367%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25357 10559]\n",
      " [11405 24679]]\n",
      "[[4662 4365]\n",
      " [4455 4518]]\n",
      "[[5205 4637]\n",
      " [5028 4837]]\n",
      "Epoch: 7900, Train loss: 0.6086857914924622, Train return: 39.294573588201644%, Val loss: 0.7152119278907776, Val return: -2.987226836566497%, Test loss: 0.7121601700782776, Test return: -0.35905921447185213%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25464 10452]\n",
      " [11375 24709]]\n",
      "[[4679 4348]\n",
      " [4467 4506]]\n",
      "[[5225 4617]\n",
      " [5037 4828]]\n",
      "Epoch: 7950, Train loss: 0.6075725555419922, Train return: 39.609437823778286%, Val loss: 0.715591311454773, Val return: -3.1032662063298746%, Test loss: 0.7125185132026672, Test return: -0.8720665410942565%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25656 10260]\n",
      " [11485 24599]]\n",
      "[[4719 4308]\n",
      " [4523 4450]]\n",
      "[[5263 4579]\n",
      " [5085 4780]]\n",
      "Epoch: 8000, Train loss: 0.6063763499259949, Train return: 39.97540384452479%, Val loss: 0.7158689498901367, Val return: -2.976615514028376%, Test loss: 0.7127295136451721, Test return: -0.4616628180233717%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25554 10362]\n",
      " [11257 24827]]\n",
      "[[4674 4353]\n",
      " [4473 4500]]\n",
      "[[5218 4624]\n",
      " [5036 4829]]\n",
      "Epoch: 8050, Train loss: 0.6052045226097107, Train return: 40.40097900862122%, Val loss: 0.7162161469459534, Val return: -3.2711961333288047%, Test loss: 0.7131479382514954, Test return: -0.5366517411495899%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25517 10399]\n",
      " [11123 24961]]\n",
      "[[4646 4381]\n",
      " [4459 4514]]\n",
      "[[5189 4653]\n",
      " [5010 4855]]\n",
      "Epoch: 8100, Train loss: 0.6040497422218323, Train return: 40.67235574672562%, Val loss: 0.7165662050247192, Val return: -3.2173499812262203%, Test loss: 0.7134982943534851, Test return: -0.6133233753342581%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25692 10224]\n",
      " [11199 24885]]\n",
      "[[4685 4342]\n",
      " [4507 4466]]\n",
      "[[5221 4621]\n",
      " [5034 4831]]\n",
      "Epoch: 8150, Train loss: 0.602889358997345, Train return: 41.05992722321386%, Val loss: 0.7169673442840576, Val return: -3.037995812297637%, Test loss: 0.7139412760734558, Test return: -0.6224351441316196%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25848 10068]\n",
      " [11249 24835]]\n",
      "[[4713 4314]\n",
      " [4525 4448]]\n",
      "[[5259 4583]\n",
      " [5076 4789]]\n",
      "Epoch: 8200, Train loss: 0.601671576499939, Train return: 41.37564837936345%, Val loss: 0.7172225117683411, Val return: -2.904525923614061%, Test loss: 0.7142117619514465, Test return: -0.45714400950945905%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25810 10106]\n",
      " [11106 24978]]\n",
      "[[4701 4326]\n",
      " [4501 4472]]\n",
      "[[5240 4602]\n",
      " [5044 4821]]\n",
      "Epoch: 8250, Train loss: 0.6004590392112732, Train return: 41.74974250957504%, Val loss: 0.7176735401153564, Val return: -3.0578464088550263%, Test loss: 0.7145885229110718, Test return: -0.3729135372471109%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25756 10160]\n",
      " [10935 25149]]\n",
      "[[4659 4368]\n",
      " [4485 4488]]\n",
      "[[5214 4628]\n",
      " [5013 4852]]\n",
      "Epoch: 8300, Train loss: 0.5992593169212341, Train return: 42.04127313096194%, Val loss: 0.7180127501487732, Val return: -2.91967792412361%, Test loss: 0.7149078845977783, Test return: -0.36288248628501774%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25806 10110]\n",
      " [10874 25210]]\n",
      "[[4661 4366]\n",
      " [4469 4504]]\n",
      "[[5216 4626]\n",
      " [5001 4864]]\n",
      "Epoch: 8350, Train loss: 0.5980682373046875, Train return: 42.53418740735041%, Val loss: 0.7183949947357178, Val return: -3.5183906412777617%, Test loss: 0.7152754664421082, Test return: -0.7090964478127569%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[25995  9921]\n",
      " [10954 25130]]\n",
      "[[4700 4327]\n",
      " [4524 4449]]\n",
      "[[5246 4596]\n",
      " [5050 4815]]\n",
      "Epoch: 8400, Train loss: 0.5968577265739441, Train return: 42.80685807483018%, Val loss: 0.7187323570251465, Val return: -3.300122496958853%, Test loss: 0.715667724609375, Test return: -0.3690464877102087%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26127  9789]\n",
      " [11006 25078]]\n",
      "[[4721 4306]\n",
      " [4545 4428]]\n",
      "[[5272 4570]\n",
      " [5075 4790]]\n",
      "Epoch: 8450, Train loss: 0.5956013202667236, Train return: 43.315116770206345%, Val loss: 0.7191401720046997, Val return: -3.4530803019872387%, Test loss: 0.7160049080848694, Test return: -0.4509896717411517%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26075  9841]\n",
      " [10786 25298]]\n",
      "[[4689 4338]\n",
      " [4507 4466]]\n",
      "[[5230 4612]\n",
      " [5029 4836]]\n",
      "Epoch: 8500, Train loss: 0.5943328738212585, Train return: 43.804397240533184%, Val loss: 0.7195634841918945, Val return: -3.3522777622515494%, Test loss: 0.7164931297302246, Test return: -0.35160846532559986%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26077  9839]\n",
      " [10638 25446]]\n",
      "[[4665 4362]\n",
      " [4490 4483]]\n",
      "[[5207 4635]\n",
      " [4996 4869]]\n",
      "Epoch: 8550, Train loss: 0.5930839776992798, Train return: 44.09098021264982%, Val loss: 0.71983402967453, Val return: -3.5703020223429345%, Test loss: 0.7168639898300171, Test return: -0.492633308729336%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26158  9758]\n",
      " [10670 25414]]\n",
      "[[4678 4349]\n",
      " [4505 4468]]\n",
      "[[5211 4631]\n",
      " [5025 4840]]\n",
      "Epoch: 8600, Train loss: 0.591810405254364, Train return: 44.44018110915793%, Val loss: 0.720217227935791, Val return: -3.2738125706830856%, Test loss: 0.7171456217765808, Test return: -0.6635111073939511%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26151  9765]\n",
      " [10512 25572]]\n",
      "[[4655 4372]\n",
      " [4492 4481]]\n",
      "[[5188 4654]\n",
      " [5003 4862]]\n",
      "Epoch: 8650, Train loss: 0.5905316472053528, Train return: 44.783629525875135%, Val loss: 0.7206377983093262, Val return: -3.4022231276151156%, Test loss: 0.7175009846687317, Test return: -0.3078656579829163%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26134  9782]\n",
      " [10390 25694]]\n",
      "[[4623 4404]\n",
      " [4472 4501]]\n",
      "[[5174 4668]\n",
      " [4968 4897]]\n",
      "Epoch: 8700, Train loss: 0.5892912745475769, Train return: 45.20311017175039%, Val loss: 0.7211260795593262, Val return: -3.2801916913218396%, Test loss: 0.7180178761482239, Test return: -0.46577278487608786%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26343  9573]\n",
      " [10485 25599]]\n",
      "[[4672 4355]\n",
      " [4510 4463]]\n",
      "[[5217 4625]\n",
      " [5028 4837]]\n",
      "Epoch: 8750, Train loss: 0.5879724621772766, Train return: 45.577920673926286%, Val loss: 0.7214200496673584, Val return: -3.179897259654977%, Test loss: 0.7183350324630737, Test return: -0.21979577624369814%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26242  9674]\n",
      " [10278 25806]]\n",
      "[[4630 4397]\n",
      " [4456 4517]]\n",
      "[[5177 4665]\n",
      " [4982 4883]]\n",
      "Epoch: 8800, Train loss: 0.5867082476615906, Train return: 45.78018353783305%, Val loss: 0.7218031883239746, Val return: -2.9454901172920462%, Test loss: 0.7188019752502441, Test return: -0.4113755617340688%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26529  9387]\n",
      " [10487 25597]]\n",
      "[[4709 4318]\n",
      " [4532 4441]]\n",
      "[[5238 4604]\n",
      " [5060 4805]]\n",
      "Epoch: 8850, Train loss: 0.5853870511054993, Train return: 46.27456004412253%, Val loss: 0.722173810005188, Val return: -3.0247530646507386%, Test loss: 0.7191352844238281, Test return: -0.3518395782558064%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26488  9428]\n",
      " [10321 25763]]\n",
      "[[4674 4353]\n",
      " [4505 4468]]\n",
      "[[5220 4622]\n",
      " [5028 4837]]\n",
      "Epoch: 8900, Train loss: 0.5840594172477722, Train return: 46.4539178279496%, Val loss: 0.7225993275642395, Val return: -2.9210384024323957%, Test loss: 0.7196425795555115, Test return: -0.10898976329943993%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26496  9420]\n",
      " [10252 25832]]\n",
      "[[4666 4361]\n",
      " [4493 4480]]\n",
      "[[5216 4626]\n",
      " [5017 4848]]\n",
      "Epoch: 8950, Train loss: 0.582761824131012, Train return: 46.68230925626295%, Val loss: 0.723053514957428, Val return: -2.8860582857229518%, Test loss: 0.720015823841095, Test return: -0.11603376505333199%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26499  9417]\n",
      " [10164 25920]]\n",
      "[[4661 4366]\n",
      " [4491 4482]]\n",
      "[[5201 4641]\n",
      " [5008 4857]]\n",
      "Epoch: 9000, Train loss: 0.581467866897583, Train return: 47.11965586775783%, Val loss: 0.723527193069458, Val return: -3.092019572221788%, Test loss: 0.7204956412315369, Test return: -0.055582975641938336%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26710  9206]\n",
      " [10244 25840]]\n",
      "[[4693 4334]\n",
      " [4536 4437]]\n",
      "[[5232 4610]\n",
      " [5033 4832]]\n",
      "Epoch: 9050, Train loss: 0.5800977945327759, Train return: 47.55260136870278%, Val loss: 0.7238516807556152, Val return: -2.9147899792943397%, Test loss: 0.7208061218261719, Test return: -0.18036090312707448%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26544  9372]\n",
      " [ 9921 26163]]\n",
      "[[4625 4402]\n",
      " [4462 4511]]\n",
      "[[5162 4680]\n",
      " [4971 4894]]\n",
      "Epoch: 9100, Train loss: 0.5787755250930786, Train return: 47.95114455813353%, Val loss: 0.7242270708084106, Val return: -3.040056308703624%, Test loss: 0.7212607264518738, Test return: -0.04723152828649849%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26765  9151]\n",
      " [10064 26020]]\n",
      "[[4669 4358]\n",
      " [4509 4464]]\n",
      "[[5215 4627]\n",
      " [5022 4843]]\n",
      "Epoch: 9150, Train loss: 0.5774127244949341, Train return: 48.36363305230423%, Val loss: 0.7247213125228882, Val return: -2.8476885867761865%, Test loss: 0.7216476798057556, Test return: 0.07217041109604662%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26761  9155]\n",
      " [ 9932 26152]]\n",
      "[[4660 4367]\n",
      " [4483 4490]]\n",
      "[[5195 4647]\n",
      " [4993 4872]]\n",
      "Epoch: 9200, Train loss: 0.5760690569877625, Train return: 48.84178528450473%, Val loss: 0.7251288294792175, Val return: -2.9537206856161866%, Test loss: 0.7221584320068359, Test return: 0.06638868163638253%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26811  9105]\n",
      " [ 9839 26245]]\n",
      "[[4643 4384]\n",
      " [4477 4496]]\n",
      "[[5188 4654]\n",
      " [4993 4872]]\n",
      "Epoch: 9250, Train loss: 0.5747628808021545, Train return: 49.32514458845919%, Val loss: 0.7256072759628296, Val return: -3.166709618319162%, Test loss: 0.7226200103759766, Test return: -0.2733054153736194%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27124  8792]\n",
      " [10056 26028]]\n",
      "[[4705 4322]\n",
      " [4556 4417]]\n",
      "[[5255 4587]\n",
      " [5082 4783]]\n",
      "Epoch: 9300, Train loss: 0.573371171951294, Train return: 49.65414408102505%, Val loss: 0.7260324954986572, Val return: -2.9815690344248886%, Test loss: 0.7230701446533203, Test return: -0.17597857053668337%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27046  8870]\n",
      " [ 9859 26225]]\n",
      "[[4680 4347]\n",
      " [4516 4457]]\n",
      "[[5222 4620]\n",
      " [5052 4813]]\n",
      "Epoch: 9350, Train loss: 0.5719621181488037, Train return: 50.00662055127319%, Val loss: 0.7264209389686584, Val return: -2.8979588405802814%, Test loss: 0.7234041690826416, Test return: -0.23109305931208424%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26908  9008]\n",
      " [ 9611 26473]]\n",
      "[[4627 4400]\n",
      " [4467 4506]]\n",
      "[[5161 4681]\n",
      " [4982 4883]]\n",
      "Epoch: 9400, Train loss: 0.5705535411834717, Train return: 50.33660816331101%, Val loss: 0.726760983467102, Val return: -2.8343667564886177%, Test loss: 0.7238045930862427, Test return: -0.31511948867716155%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26963  8953]\n",
      " [ 9521 26563]]\n",
      "[[4621 4406]\n",
      " [4454 4519]]\n",
      "[[5160 4682]\n",
      " [4985 4880]]\n",
      "Epoch: 9450, Train loss: 0.5692270994186401, Train return: 50.63621317232177%, Val loss: 0.7273032069206238, Val return: -2.7785623616229094%, Test loss: 0.724395215511322, Test return: -0.17617129866409903%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27197  8719]\n",
      " [ 9657 26427]]\n",
      "[[4677 4350]\n",
      " [4510 4463]]\n",
      "[[5211 4631]\n",
      " [5025 4840]]\n",
      "Epoch: 9500, Train loss: 0.5678315758705139, Train return: 51.100889032972674%, Val loss: 0.7276986837387085, Val return: -2.468298595264144%, Test loss: 0.7248287796974182, Test return: -0.3980485819272954%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27219  8697]\n",
      " [ 9533 26551]]\n",
      "[[4663 4364]\n",
      " [4497 4476]]\n",
      "[[5203 4639]\n",
      " [5026 4839]]\n",
      "Epoch: 9550, Train loss: 0.5663657784461975, Train return: 51.35948336619228%, Val loss: 0.7280523180961609, Val return: -2.343008641218882%, Test loss: 0.7252241373062134, Test return: -0.5432630959100025%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[26999  8917]\n",
      " [ 9247 26837]]\n",
      "[[4575 4452]\n",
      " [4409 4564]]\n",
      "[[5113 4729]\n",
      " [4950 4915]]\n",
      "Epoch: 9600, Train loss: 0.565032958984375, Train return: 51.766041787222505%, Val loss: 0.7286944389343262, Val return: -2.168402524614923%, Test loss: 0.7258603572845459, Test return: -0.344063223913086%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27377  8539]\n",
      " [ 9514 26570]]\n",
      "[[4673 4354]\n",
      " [4498 4475]]\n",
      "[[5211 4631]\n",
      " [5030 4835]]\n",
      "Epoch: 9650, Train loss: 0.5635985136032104, Train return: 52.19676617733731%, Val loss: 0.7290171384811401, Val return: -2.460256947572691%, Test loss: 0.7261993885040283, Test return: -0.14785986464634698%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27342  8574]\n",
      " [ 9349 26735]]\n",
      "[[4637 4390]\n",
      " [4476 4497]]\n",
      "[[5193 4649]\n",
      " [5002 4863]]\n",
      "Epoch: 9700, Train loss: 0.5621969103813171, Train return: 52.507381532996355%, Val loss: 0.7295920252799988, Val return: -2.347000042581074%, Test loss: 0.7267372012138367, Test return: -0.258705590074066%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27432  8484]\n",
      " [ 9340 26744]]\n",
      "[[4653 4374]\n",
      " [4480 4493]]\n",
      "[[5204 4638]\n",
      " [5012 4853]]\n",
      "Epoch: 9750, Train loss: 0.5607665181159973, Train return: 52.89439826899548%, Val loss: 0.7300384044647217, Val return: -2.4359536932895502%, Test loss: 0.7272109389305115, Test return: -0.26622386688185495%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27407  8509]\n",
      " [ 9183 26901]]\n",
      "[[4628 4399]\n",
      " [4460 4513]]\n",
      "[[5176 4666]\n",
      " [4995 4870]]\n",
      "Epoch: 9800, Train loss: 0.559339165687561, Train return: 53.280860511862414%, Val loss: 0.7306109666824341, Val return: -2.328323623562328%, Test loss: 0.7277073264122009, Test return: -0.40978296320389707%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27506  8410]\n",
      " [ 9174 26910]]\n",
      "[[4648 4379]\n",
      " [4473 4500]]\n",
      "[[5182 4660]\n",
      " [5002 4863]]\n",
      "Epoch: 9850, Train loss: 0.5579166412353516, Train return: 53.65082413364758%, Val loss: 0.7309297919273376, Val return: -2.4833890952650184%, Test loss: 0.7281156182289124, Test return: -0.24813341899952326%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27614  8302]\n",
      " [ 9158 26926]]\n",
      "[[4650 4377]\n",
      " [4477 4496]]\n",
      "[[5205 4637]\n",
      " [5018 4847]]\n",
      "Epoch: 9900, Train loss: 0.5564634799957275, Train return: 54.05032828377866%, Val loss: 0.7315758466720581, Val return: -2.584762352177632%, Test loss: 0.7287873029708862, Test return: -0.7316465429012974%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27663  8253]\n",
      " [ 9081 27003]]\n",
      "[[4652 4375]\n",
      " [4470 4503]]\n",
      "[[5192 4650]\n",
      " [5038 4827]]\n",
      "Epoch: 9950, Train loss: 0.5550764203071594, Train return: 54.45925020951782%, Val loss: 0.7319974303245544, Val return: -2.197830357459783%, Test loss: 0.7291526794433594, Test return: -0.47783766750667644%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27794  8122]\n",
      " [ 9112 26972]]\n",
      "[[4683 4344]\n",
      " [4501 4472]]\n",
      "[[5230 4612]\n",
      " [5050 4815]]\n",
      "Epoch: 10000, Train loss: 0.5535802245140076, Train return: 54.71012067910201%, Val loss: 0.732463002204895, Val return: -2.409875046338442%, Test loss: 0.7295854687690735, Test return: -0.46672085430272336%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27650  8266]\n",
      " [ 8866 27218]]\n",
      "[[4622 4405]\n",
      " [4457 4516]]\n",
      "[[5169 4673]\n",
      " [4993 4872]]\n",
      "Epoch: 10050, Train loss: 0.5521748065948486, Train return: 55.07020746783139%, Val loss: 0.7330545783042908, Val return: -1.973094473357907%, Test loss: 0.7301821112632751, Test return: -0.16029545430397335%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27818  8098]\n",
      " [ 8937 27147]]\n",
      "[[4669 4358]\n",
      " [4488 4485]]\n",
      "[[5205 4637]\n",
      " [5022 4843]]\n",
      "Epoch: 10100, Train loss: 0.5507123470306396, Train return: 55.35327987198191%, Val loss: 0.7334449887275696, Val return: -2.1432939072558588%, Test loss: 0.7306385040283203, Test return: -0.411481506362935%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27793  8123]\n",
      " [ 8787 27297]]\n",
      "[[4634 4393]\n",
      " [4469 4504]]\n",
      "[[5181 4661]\n",
      " [5011 4854]]\n",
      "Epoch: 10150, Train loss: 0.5492544770240784, Train return: 55.66562994865454%, Val loss: 0.733915388584137, Val return: -2.4095116423506093%, Test loss: 0.7311345338821411, Test return: -0.5547625652670236%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27895  8021]\n",
      " [ 8789 27295]]\n",
      "[[4644 4383]\n",
      " [4481 4492]]\n",
      "[[5174 4668]\n",
      " [5013 4852]]\n",
      "Epoch: 10200, Train loss: 0.5477733612060547, Train return: 55.950228313875485%, Val loss: 0.7345251441001892, Val return: -2.134572086671128%, Test loss: 0.7315661311149597, Test return: -0.47774338073607703%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27825  8091]\n",
      " [ 8599 27485]]\n",
      "[[4621 4406]\n",
      " [4444 4529]]\n",
      "[[5155 4687]\n",
      " [4984 4881]]\n",
      "Epoch: 10250, Train loss: 0.5463461875915527, Train return: 56.21089487178912%, Val loss: 0.7349620461463928, Val return: -2.0946646634816397%, Test loss: 0.7323631644248962, Test return: -0.6527380937937114%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27985  7931]\n",
      " [ 8677 27407]]\n",
      "[[4637 4390]\n",
      " [4455 4518]]\n",
      "[[5174 4668]\n",
      " [5026 4839]]\n",
      "Epoch: 10300, Train loss: 0.5448369979858398, Train return: 56.521923841139774%, Val loss: 0.7353002429008484, Val return: -2.187219929446713%, Test loss: 0.7325633764266968, Test return: -0.7658837624117943%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27970  7946]\n",
      " [ 8569 27515]]\n",
      "[[4634 4393]\n",
      " [4457 4516]]\n",
      "[[5166 4676]\n",
      " [5007 4858]]\n",
      "Epoch: 10350, Train loss: 0.5433924198150635, Train return: 56.85072201773696%, Val loss: 0.7360304594039917, Val return: -2.5013971995815543%, Test loss: 0.7333166599273682, Test return: -1.172505927502032%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28041  7875]\n",
      " [ 8528 27556]]\n",
      "[[4633 4394]\n",
      " [4484 4489]]\n",
      "[[5142 4700]\n",
      " [5020 4845]]\n",
      "Epoch: 10400, Train loss: 0.5418919920921326, Train return: 57.08301375846434%, Val loss: 0.7364132404327393, Val return: -1.9800575124264839%, Test loss: 0.7336040139198303, Test return: -0.9128552535852905%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[27875  8041]\n",
      " [ 8271 27813]]\n",
      "[[4583 4444]\n",
      " [4413 4560]]\n",
      "[[5089 4753]\n",
      " [4945 4920]]\n",
      "Epoch: 10450, Train loss: 0.5404629707336426, Train return: 57.31727957583952%, Val loss: 0.7370109558105469, Val return: -2.2867013882034533%, Test loss: 0.7342979907989502, Test return: -1.3841418162301686%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28181  7735]\n",
      " [ 8488 27596]]\n",
      "[[4647 4380]\n",
      " [4482 4491]]\n",
      "[[5151 4691]\n",
      " [5030 4835]]\n",
      "Epoch: 10500, Train loss: 0.5389317274093628, Train return: 57.617529437416316%, Val loss: 0.7373270392417908, Val return: -2.291192092551699%, Test loss: 0.7347713112831116, Test return: -1.0827198514198477%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28079  7837]\n",
      " [ 8285 27799]]\n",
      "[[4612 4415]\n",
      " [4455 4518]]\n",
      "[[5123 4719]\n",
      " [4992 4873]]\n",
      "Epoch: 10550, Train loss: 0.5375051498413086, Train return: 57.856845254236156%, Val loss: 0.7381477355957031, Val return: -1.9955073337111102%, Test loss: 0.7353689074516296, Test return: -1.2320720152013556%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28209  7707]\n",
      " [ 8341 27743]]\n",
      "[[4650 4377]\n",
      " [4467 4506]]\n",
      "[[5138 4704]\n",
      " [5013 4852]]\n",
      "Epoch: 10600, Train loss: 0.5359752774238586, Train return: 58.213038547888296%, Val loss: 0.7385477423667908, Val return: -2.1976609609194604%, Test loss: 0.735816240310669, Test return: -1.4325383516040093%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28236  7680]\n",
      " [ 8236 27848]]\n",
      "[[4638 4389]\n",
      " [4470 4503]]\n",
      "[[5139 4703]\n",
      " [5021 4844]]\n",
      "Epoch: 10650, Train loss: 0.5345898866653442, Train return: 58.58917014268534%, Val loss: 0.7392972111701965, Val return: -2.5574212271067225%, Test loss: 0.7367724776268005, Test return: -1.4344771227660635%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28506  7410]\n",
      " [ 8426 27658]]\n",
      "[[4690 4337]\n",
      " [4542 4431]]\n",
      "[[5193 4649]\n",
      " [5086 4779]]\n",
      "Epoch: 10700, Train loss: 0.5330240726470947, Train return: 58.78376518432869%, Val loss: 0.7394607067108154, Val return: -2.149435583737759%, Test loss: 0.736900269985199, Test return: -1.5623246711903325%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28329  7587]\n",
      " [ 8156 27928]]\n",
      "[[4632 4395]\n",
      " [4467 4506]]\n",
      "[[5118 4724]\n",
      " [5015 4850]]\n",
      "Epoch: 10750, Train loss: 0.5315315127372742, Train return: 59.11404818084516%, Val loss: 0.7400040030479431, Val return: -1.9546661665355194%, Test loss: 0.737300455570221, Test return: -1.4797291420730125%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[28385  7531]\n",
      " [ 8099 27985]]\n",
      "[[4649 4378]\n",
      " [4470 4503]]\n",
      "[[5132 4710]\n",
      " [5015 4850]]\n",
      "Epoch: 10800, Train loss: 0.5300166010856628, Train return: 59.391806041436%, Val loss: 0.7405985593795776, Val return: -2.239850585108487%, Test loss: 0.7379276752471924, Test return: -1.4327110858969467%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28330  7586]\n",
      " [ 7952 28132]]\n",
      "[[4594 4433]\n",
      " [4417 4556]]\n",
      "[[5072 4770]\n",
      " [4972 4893]]\n",
      "Epoch: 10850, Train loss: 0.5285492539405823, Train return: 59.80073749137692%, Val loss: 0.741217851638794, Val return: -2.2019739580544693%, Test loss: 0.7385733127593994, Test return: -1.3519714006139054%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28513  7403]\n",
      " [ 8008 28076]]\n",
      "[[4637 4390]\n",
      " [4479 4494]]\n",
      "[[5116 4726]\n",
      " [5011 4854]]\n",
      "Epoch: 10900, Train loss: 0.5270568132400513, Train return: 60.061038043057366%, Val loss: 0.7418211698532104, Val return: -2.2514411347911043%, Test loss: 0.7392044067382812, Test return: -1.4605589931773972%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28480  7436]\n",
      " [ 7886 28198]]\n",
      "[[4603 4424]\n",
      " [4443 4530]]\n",
      "[[5081 4761]\n",
      " [4986 4879]]\n",
      "Epoch: 10950, Train loss: 0.5255873799324036, Train return: 60.45544613805083%, Val loss: 0.7423145174980164, Val return: -2.4513217432613423%, Test loss: 0.7396646738052368, Test return: -1.1348119493069777%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28649  7267]\n",
      " [ 7919 28165]]\n",
      "[[4667 4360]\n",
      " [4510 4463]]\n",
      "[[5145 4697]\n",
      " [5038 4827]]\n",
      "Epoch: 11000, Train loss: 0.5240696668624878, Train return: 60.60793247612734%, Val loss: 0.7429274320602417, Val return: -2.2502558330306663%, Test loss: 0.7403854727745056, Test return: -1.1527647365386227%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28695  7221]\n",
      " [ 7919 28165]]\n",
      "[[4652 4375]\n",
      " [4492 4481]]\n",
      "[[5130 4712]\n",
      " [5025 4840]]\n",
      "Epoch: 11050, Train loss: 0.5225546360015869, Train return: 60.88488995031367%, Val loss: 0.7434871196746826, Val return: -2.0932854766019036%, Test loss: 0.7408114075660706, Test return: -1.516480322251898%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28669  7247]\n",
      " [ 7799 28285]]\n",
      "[[4642 4385]\n",
      " [4473 4500]]\n",
      "[[5093 4749]\n",
      " [5015 4850]]\n",
      "Epoch: 11100, Train loss: 0.5210258960723877, Train return: 61.4207954615553%, Val loss: 0.7438285946846008, Val return: -2.2360682655974533%, Test loss: 0.7412447333335876, Test return: -1.498315886996559%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28738  7178]\n",
      " [ 7724 28360]]\n",
      "[[4639 4388]\n",
      " [4456 4517]]\n",
      "[[5086 4756]\n",
      " [5000 4865]]\n",
      "Epoch: 11150, Train loss: 0.5195122361183167, Train return: 61.57901970901113%, Val loss: 0.7445745468139648, Val return: -2.21664140122454%, Test loss: 0.7420825362205505, Test return: -1.2475528004050065%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28744  7172]\n",
      " [ 7645 28439]]\n",
      "[[4615 4412]\n",
      " [4466 4507]]\n",
      "[[5091 4751]\n",
      " [5003 4862]]\n",
      "Epoch: 11200, Train loss: 0.5180099606513977, Train return: 61.91969517174524%, Val loss: 0.745124101638794, Val return: -2.2338828848293515%, Test loss: 0.7424612641334534, Test return: -1.2032413651218306%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28904  7012]\n",
      " [ 7731 28353]]\n",
      "[[4650 4377]\n",
      " [4483 4490]]\n",
      "[[5115 4727]\n",
      " [5025 4840]]\n",
      "Epoch: 11250, Train loss: 0.5165095329284668, Train return: 62.26262780829255%, Val loss: 0.7456318736076355, Val return: -2.024573243249204%, Test loss: 0.7432088851928711, Test return: -1.1454638884368529%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[28998  6918]\n",
      " [ 7689 28395]]\n",
      "[[4668 4359]\n",
      " [4502 4471]]\n",
      "[[5159 4683]\n",
      " [5045 4820]]\n",
      "Epoch: 11300, Train loss: 0.5150336623191833, Train return: 62.52162137756443%, Val loss: 0.7463178634643555, Val return: -2.0424789078820567%, Test loss: 0.7437437772750854, Test return: -1.1561777085648501%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29119  6797]\n",
      " [ 7729 28355]]\n",
      "[[4702 4325]\n",
      " [4521 4452]]\n",
      "[[5157 4685]\n",
      " [5061 4804]]\n",
      "Epoch: 11350, Train loss: 0.5134586095809937, Train return: 62.82796555602535%, Val loss: 0.7469304203987122, Val return: -1.965722343262751%, Test loss: 0.7443329691886902, Test return: -1.0931512061450996%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29121  6795]\n",
      " [ 7622 28462]]\n",
      "[[4679 4348]\n",
      " [4504 4469]]\n",
      "[[5137 4705]\n",
      " [5034 4831]]\n",
      "Epoch: 11400, Train loss: 0.5119732618331909, Train return: 63.09766057743049%, Val loss: 0.7473795413970947, Val return: -2.0135930986028514%, Test loss: 0.7448651790618896, Test return: -1.2346454177237005%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29159  6757]\n",
      " [ 7570 28514]]\n",
      "[[4685 4342]\n",
      " [4514 4459]]\n",
      "[[5137 4705]\n",
      " [5052 4813]]\n",
      "Epoch: 11450, Train loss: 0.5104526877403259, Train return: 63.381451841176585%, Val loss: 0.7480854988098145, Val return: -1.945114193572367%, Test loss: 0.7455593943595886, Test return: -1.043144122641719%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29135  6781]\n",
      " [ 7470 28614]]\n",
      "[[4665 4362]\n",
      " [4493 4480]]\n",
      "[[5135 4707]\n",
      " [5033 4832]]\n",
      "Epoch: 11500, Train loss: 0.5089309811592102, Train return: 63.73954030204935%, Val loss: 0.7484461665153503, Val return: -1.8058639043935323%, Test loss: 0.7458876371383667, Test return: -1.2349098584764882%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29089  6827]\n",
      " [ 7326 28758]]\n",
      "[[4638 4389]\n",
      " [4441 4532]]\n",
      "[[5104 4738]\n",
      " [5007 4858]]\n",
      "Epoch: 11550, Train loss: 0.5074477791786194, Train return: 64.01168517427858%, Val loss: 0.7489783763885498, Val return: -1.8278036450950967%, Test loss: 0.7466048002243042, Test return: -1.0510524480856602%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29354  6562]\n",
      " [ 7484 28600]]\n",
      "[[4699 4328]\n",
      " [4506 4467]]\n",
      "[[5167 4675]\n",
      " [5054 4811]]\n",
      "Epoch: 11600, Train loss: 0.5059144496917725, Train return: 64.22686179217298%, Val loss: 0.7497527003288269, Val return: -2.1805341498195507%, Test loss: 0.7472760081291199, Test return: -1.2092195019747243%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29290  6626]\n",
      " [ 7349 28735]]\n",
      "[[4648 4379]\n",
      " [4483 4490]]\n",
      "[[5130 4712]\n",
      " [5018 4847]]\n",
      "Epoch: 11650, Train loss: 0.5044219493865967, Train return: 64.54591401119578%, Val loss: 0.7503292560577393, Val return: -1.9649908944605339%, Test loss: 0.7477560639381409, Test return: -0.9376825765523799%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29380  6536]\n",
      " [ 7331 28753]]\n",
      "[[4682 4345]\n",
      " [4509 4464]]\n",
      "[[5158 4684]\n",
      " [5035 4830]]\n",
      "Epoch: 11700, Train loss: 0.5029820799827576, Train return: 64.83376633358719%, Val loss: 0.7509446144104004, Val return: -1.7411898119740963%, Test loss: 0.7483800649642944, Test return: -0.8144692537274592%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29604  6312]\n",
      " [ 7434 28650]]\n",
      "[[4734 4293]\n",
      " [4552 4421]]\n",
      "[[5205 4637]\n",
      " [5076 4789]]\n",
      "Epoch: 11750, Train loss: 0.501361072063446, Train return: 65.14420464430052%, Val loss: 0.7514703273773193, Val return: -1.6767369883539576%, Test loss: 0.74886554479599, Test return: -1.1569377583951137%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[[29364  6552]\n",
      " [ 7116 28968]]\n",
      "[[4640 4387]\n",
      " [4447 4526]]\n",
      "[[5094 4748]\n",
      " [4992 4873]]\n",
      "Epoch: 11800, Train loss: 0.4998961091041565, Train return: 65.28311209638751%, Val loss: 0.7522820234298706, Val return: -2.0178619123167225%, Test loss: 0.7498101592063904, Test return: -1.0101535470336054%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29547  6369]\n",
      " [ 7198 28886]]\n",
      "[[4675 4352]\n",
      " [4503 4470]]\n",
      "[[5155 4687]\n",
      " [5024 4841]]\n",
      "Epoch: 11850, Train loss: 0.4983676075935364, Train return: 65.57734339175258%, Val loss: 0.7530167698860168, Val return: -1.836995771782585%, Test loss: 0.7502642869949341, Test return: -1.0392159068742162%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29584  6332]\n",
      " [ 7138 28946]]\n",
      "[[4680 4347]\n",
      " [4512 4461]]\n",
      "[[5153 4689]\n",
      " [5034 4831]]\n",
      "Epoch: 11900, Train loss: 0.49684759974479675, Train return: 65.82105798872576%, Val loss: 0.7531516551971436, Val return: -2.0689950973630933%, Test loss: 0.750593364238739, Test return: -1.0285949993762604%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29481  6435]\n",
      " [ 6968 29116]]\n",
      "[[4633 4394]\n",
      " [4458 4515]]\n",
      "[[5122 4720]\n",
      " [4993 4872]]\n",
      "Epoch: 11950, Train loss: 0.4953477680683136, Train return: 66.05744462870388%, Val loss: 0.7540476322174072, Val return: -2.124374832388719%, Test loss: 0.7515324354171753, Test return: -1.313291180832763%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29566  6350]\n",
      " [ 6963 29121]]\n",
      "[[4619 4408]\n",
      " [4467 4506]]\n",
      "[[5126 4716]\n",
      " [5012 4853]]\n",
      "Epoch: 12000, Train loss: 0.49382904171943665, Train return: 66.52419765569118%, Val loss: 0.7546073794364929, Val return: -1.901479934361583%, Test loss: 0.751953125, Test return: -0.8066033961957714%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29735  6181]\n",
      " [ 7005 29079]]\n",
      "[[4693 4334]\n",
      " [4521 4452]]\n",
      "[[5160 4682]\n",
      " [5041 4824]]\n",
      "Epoch: 12050, Train loss: 0.492367684841156, Train return: 66.78903823090624%, Val loss: 0.7550762295722961, Val return: -2.095565034802358%, Test loss: 0.7525109052658081, Test return: -0.7810238554635294%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29772  6144]\n",
      " [ 6969 29115]]\n",
      "[[4667 4360]\n",
      " [4506 4467]]\n",
      "[[5160 4682]\n",
      " [5033 4832]]\n",
      "Epoch: 12100, Train loss: 0.4907734990119934, Train return: 66.81907494772811%, Val loss: 0.7560077905654907, Val return: -2.0697456268179066%, Test loss: 0.7532447576522827, Test return: -0.9597067367644495%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29686  6230]\n",
      " [ 6796 29288]]\n",
      "[[4644 4383]\n",
      " [4483 4490]]\n",
      "[[5121 4721]\n",
      " [4993 4872]]\n",
      "Epoch: 12150, Train loss: 0.48928213119506836, Train return: 67.2563227902916%, Val loss: 0.7564525604248047, Val return: -2.0539935605545288%, Test loss: 0.753748893737793, Test return: -0.911081014758871%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29781  6135]\n",
      " [ 6794 29290]]\n",
      "[[4637 4390]\n",
      " [4458 4515]]\n",
      "[[5128 4714]\n",
      " [5004 4861]]\n",
      "Epoch: 12200, Train loss: 0.48777902126312256, Train return: 67.44134955783244%, Val loss: 0.7571834921836853, Val return: -1.971278039574755%, Test loss: 0.7543972134590149, Test return: -1.284507079988036%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29677  6239]\n",
      " [ 6634 29450]]\n",
      "[[4603 4424]\n",
      " [4427 4546]]\n",
      "[[5092 4750]\n",
      " [4978 4887]]\n",
      "Epoch: 12250, Train loss: 0.4862166941165924, Train return: 67.82423897420067%, Val loss: 0.7575666904449463, Val return: -2.1107572124840948%, Test loss: 0.7551360726356506, Test return: -0.9983996061437936%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29776  6140]\n",
      " [ 6593 29491]]\n",
      "[[4620 4407]\n",
      " [4453 4520]]\n",
      "[[5098 4744]\n",
      " [4990 4875]]\n",
      "Epoch: 12300, Train loss: 0.48477935791015625, Train return: 67.85344691033991%, Val loss: 0.7583104372024536, Val return: -1.9164008237477623%, Test loss: 0.7558225989341736, Test return: -0.8019631218762767%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29840  6076]\n",
      " [ 6631 29453]]\n",
      "[[4630 4397]\n",
      " [4463 4510]]\n",
      "[[5103 4739]\n",
      " [4976 4889]]\n",
      "Epoch: 12350, Train loss: 0.48330578207969666, Train return: 68.1779834026216%, Val loss: 0.759193480014801, Val return: -1.82241460313266%, Test loss: 0.7564621567726135, Test return: -0.84848680194884%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[29988  5928]\n",
      " [ 6658 29426]]\n",
      "[[4678 4349]\n",
      " [4515 4458]]\n",
      "[[5134 4708]\n",
      " [5023 4842]]\n",
      "Epoch: 12400, Train loss: 0.4818069040775299, Train return: 68.54050238779101%, Val loss: 0.7599621415138245, Val return: -2.201543474881784%, Test loss: 0.7573196291923523, Test return: -0.9683686183757277%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30059  5857]\n",
      " [ 6637 29447]]\n",
      "[[4668 4359]\n",
      " [4530 4443]]\n",
      "[[5148 4694]\n",
      " [5032 4833]]\n",
      "Epoch: 12450, Train loss: 0.48034965991973877, Train return: 68.72773954608202%, Val loss: 0.7604321837425232, Val return: -1.5890842472119444%, Test loss: 0.7577649354934692, Test return: -1.2230365853621445%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30072  5844]\n",
      " [ 6586 29498]]\n",
      "[[4676 4351]\n",
      " [4505 4468]]\n",
      "[[5132 4710]\n",
      " [5026 4839]]\n",
      "Epoch: 12500, Train loss: 0.4788939356803894, Train return: 69.24049644881921%, Val loss: 0.7611035108566284, Val return: -1.2220528048351327%, Test loss: 0.7583101987838745, Test return: -0.9302998375357405%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30263  5653]\n",
      " [ 6648 29436]]\n",
      "[[4719 4308]\n",
      " [4541 4432]]\n",
      "[[5188 4654]\n",
      " [5077 4788]]\n",
      "Epoch: 12550, Train loss: 0.4773084819316864, Train return: 69.25741737349189%, Val loss: 0.7615365982055664, Val return: -1.885287756675819%, Test loss: 0.7590249180793762, Test return: -1.3013981032973343%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30200  5716]\n",
      " [ 6527 29557]]\n",
      "[[4672 4355]\n",
      " [4519 4454]]\n",
      "[[5143 4699]\n",
      " [5055 4810]]\n",
      "Epoch: 12600, Train loss: 0.47582629323005676, Train return: 69.46776689955244%, Val loss: 0.7624480724334717, Val return: -1.734928574955814%, Test loss: 0.7597513794898987, Test return: -1.0511177135079932%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30136  5780]\n",
      " [ 6383 29701]]\n",
      "[[4653 4374]\n",
      " [4487 4486]]\n",
      "[[5104 4738]\n",
      " [4997 4868]]\n",
      "Epoch: 12650, Train loss: 0.4742811322212219, Train return: 69.74587054280373%, Val loss: 0.7629528641700745, Val return: -2.128899640823119%, Test loss: 0.7602404952049255, Test return: -1.3416013818765413%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30119  5797]\n",
      " [ 6256 29828]]\n",
      "[[4619 4408]\n",
      " [4469 4504]]\n",
      "[[5074 4768]\n",
      " [4981 4884]]\n",
      "Epoch: 12700, Train loss: 0.47287696599960327, Train return: 69.9387540788111%, Val loss: 0.7637746334075928, Val return: -1.843414338172759%, Test loss: 0.7608449459075928, Test return: -1.4730710111391134%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30279  5637]\n",
      " [ 6379 29705]]\n",
      "[[4675 4352]\n",
      " [4510 4463]]\n",
      "[[5126 4716]\n",
      " [5049 4816]]\n",
      "Epoch: 12750, Train loss: 0.4713253676891327, Train return: 70.16719045003065%, Val loss: 0.7641284465789795, Val return: -1.7867086443307238%, Test loss: 0.7615127563476562, Test return: -1.1819023989244786%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30217  5699]\n",
      " [ 6224 29860]]\n",
      "[[4635 4392]\n",
      " [4468 4505]]\n",
      "[[5077 4765]\n",
      " [4987 4878]]\n",
      "Epoch: 12800, Train loss: 0.4698387682437897, Train return: 70.33626015355587%, Val loss: 0.7648020386695862, Val return: -1.628368053918253%, Test loss: 0.7622630596160889, Test return: -1.1696998663647158%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30293  5623]\n",
      " [ 6200 29884]]\n",
      "[[4621 4406]\n",
      " [4461 4512]]\n",
      "[[5075 4767]\n",
      " [4985 4880]]\n",
      "Epoch: 12850, Train loss: 0.468367338180542, Train return: 70.93332945623365%, Val loss: 0.7654806971549988, Val return: -1.8894433148532448%, Test loss: 0.7629296779632568, Test return: -1.5300803343170597%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30465  5451]\n",
      " [ 6212 29872]]\n",
      "[[4666 4361]\n",
      " [4492 4481]]\n",
      "[[5099 4743]\n",
      " [5029 4836]]\n",
      "Epoch: 12900, Train loss: 0.466874361038208, Train return: 71.02213079883462%, Val loss: 0.7660940885543823, Val return: -1.5705660750266814%, Test loss: 0.7633199691772461, Test return: -1.4684392729307374%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30401  5515]\n",
      " [ 6121 29963]]\n",
      "[[4660 4367]\n",
      " [4481 4492]]\n",
      "[[5092 4750]\n",
      " [5016 4849]]\n",
      "Epoch: 12950, Train loss: 0.4653492271900177, Train return: 71.39858034064447%, Val loss: 0.7670172452926636, Val return: -1.78595589512856%, Test loss: 0.7642176747322083, Test return: -1.3765073646833703%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30530  5386]\n",
      " [ 6113 29971]]\n",
      "[[4662 4365]\n",
      " [4492 4481]]\n",
      "[[5112 4730]\n",
      " [5033 4832]]\n",
      "Epoch: 13000, Train loss: 0.46394801139831543, Train return: 71.61058731365965%, Val loss: 0.7676060795783997, Val return: -1.8343525088707753%, Test loss: 0.7648518085479736, Test return: -1.4636209428741125%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30563  5353]\n",
      " [ 6064 30020]]\n",
      "[[4659 4368]\n",
      " [4493 4480]]\n",
      "[[5111 4731]\n",
      " [5025 4840]]\n",
      "Epoch: 13050, Train loss: 0.46245530247688293, Train return: 71.84090259812692%, Val loss: 0.7687683701515198, Val return: -1.8546612602618455%, Test loss: 0.7657835483551025, Test return: -1.6239784804553903%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30585  5331]\n",
      " [ 6028 30056]]\n",
      "[[4661 4366]\n",
      " [4495 4478]]\n",
      "[[5103 4739]\n",
      " [5048 4817]]\n",
      "Epoch: 13100, Train loss: 0.4609041213989258, Train return: 72.10319312569133%, Val loss: 0.7690041065216064, Val return: -1.832282231304824%, Test loss: 0.766248881816864, Test return: -1.7088189942883918%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30461  5455]\n",
      " [ 5834 30250]]\n",
      "[[4606 4421]\n",
      " [4453 4520]]\n",
      "[[5030 4812]\n",
      " [4980 4885]]\n",
      "Epoch: 13150, Train loss: 0.4595843553543091, Train return: 72.16639099221952%, Val loss: 0.7697319388389587, Val return: -1.5480517281304471%, Test loss: 0.767007052898407, Test return: -1.9528442497936278%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30829  5087]\n",
      " [ 6137 29947]]\n",
      "[[4735 4292]\n",
      " [4558 4415]]\n",
      "[[5157 4685]\n",
      " [5107 4758]]\n",
      "Epoch: 13200, Train loss: 0.45794373750686646, Train return: 72.47905273457208%, Val loss: 0.7702446579933167, Val return: -1.3609627820246875%, Test loss: 0.76726895570755, Test return: -1.7452565309052703%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30520  5396]\n",
      " [ 5726 30358]]\n",
      "[[4602 4425]\n",
      " [4433 4540]]\n",
      "[[5019 4823]\n",
      " [4961 4904]]\n",
      "Epoch: 13250, Train loss: 0.45652973651885986, Train return: 72.59829111471066%, Val loss: 0.7712026238441467, Val return: -1.3745007662631816%, Test loss: 0.768453061580658, Test return: -1.8463549448467484%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30609  5307]\n",
      " [ 5774 30310]]\n",
      "[[4650 4377]\n",
      " [4462 4511]]\n",
      "[[5063 4779]\n",
      " [5008 4857]]\n",
      "Epoch: 13300, Train loss: 0.45516619086265564, Train return: 72.82803896997979%, Val loss: 0.771797776222229, Val return: -1.237083035254968%, Test loss: 0.7690156102180481, Test return: -1.3589502834728067%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30853  5063]\n",
      " [ 5930 30154]]\n",
      "[[4701 4326]\n",
      " [4508 4465]]\n",
      "[[5115 4727]\n",
      " [5055 4810]]\n",
      "Epoch: 13350, Train loss: 0.45354291796684265, Train return: 73.16385802172974%, Val loss: 0.7721452713012695, Val return: -1.6738251971768787%, Test loss: 0.7693620920181274, Test return: -1.7998343489960384%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30748  5168]\n",
      " [ 5694 30390]]\n",
      "[[4637 4390]\n",
      " [4478 4495]]\n",
      "[[5057 4785]\n",
      " [5016 4849]]\n",
      "Epoch: 13400, Train loss: 0.452158659696579, Train return: 73.38174907802649%, Val loss: 0.7730005383491516, Val return: -1.4581754474636446%, Test loss: 0.7703624963760376, Test return: -1.9585583425897088%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30825  5091]\n",
      " [ 5708 30376]]\n",
      "[[4675 4352]\n",
      " [4501 4472]]\n",
      "[[5072 4770]\n",
      " [5039 4826]]\n",
      "Epoch: 13450, Train loss: 0.45067209005355835, Train return: 73.72245115112841%, Val loss: 0.7735615968704224, Val return: -1.5915662891856113%, Test loss: 0.7712148427963257, Test return: -1.7419717971068858%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30911  5005]\n",
      " [ 5695 30389]]\n",
      "[[4678 4349]\n",
      " [4512 4461]]\n",
      "[[5083 4759]\n",
      " [5051 4814]]\n",
      "Epoch: 13500, Train loss: 0.44917577505111694, Train return: 73.72007273980697%, Val loss: 0.7743202447891235, Val return: -1.080736988502587%, Test loss: 0.7715321183204651, Test return: -1.5986546817552059%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30867  5049]\n",
      " [ 5608 30476]]\n",
      "[[4662 4365]\n",
      " [4479 4494]]\n",
      "[[5062 4780]\n",
      " [5012 4853]]\n",
      "Epoch: 13550, Train loss: 0.4477527439594269, Train return: 73.98751301814444%, Val loss: 0.7749819159507751, Val return: -1.4464051619582714%, Test loss: 0.7721827626228333, Test return: -1.8473733724143033%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30855  5061]\n",
      " [ 5521 30563]]\n",
      "[[4634 4393]\n",
      " [4444 4529]]\n",
      "[[5037 4805]\n",
      " [5004 4861]]\n",
      "Epoch: 13600, Train loss: 0.44642457365989685, Train return: 74.2839760236174%, Val loss: 0.7763290405273438, Val return: -1.5407289753084374%, Test loss: 0.7735613584518433, Test return: -1.9837138153256415%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30919  4997]\n",
      " [ 5507 30577]]\n",
      "[[4654 4373]\n",
      " [4480 4493]]\n",
      "[[5050 4792]\n",
      " [5027 4838]]\n",
      "Epoch: 13650, Train loss: 0.4448191523551941, Train return: 74.45577384751677%, Val loss: 0.7766587734222412, Val return: -1.1718806251266516%, Test loss: 0.7736935615539551, Test return: -2.0307984378009283%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31016  4900]\n",
      " [ 5508 30576]]\n",
      "[[4665 4362]\n",
      " [4468 4505]]\n",
      "[[5035 4807]\n",
      " [5021 4844]]\n",
      "Epoch: 13700, Train loss: 0.44332095980644226, Train return: 74.5640854154341%, Val loss: 0.7772271633148193, Val return: -1.535018788658466%, Test loss: 0.7745964527130127, Test return: -1.7185801686907007%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30982  4934]\n",
      " [ 5403 30681]]\n",
      "[[4623 4404]\n",
      " [4448 4525]]\n",
      "[[5024 4818]\n",
      " [4994 4871]]\n",
      "Epoch: 13750, Train loss: 0.4418688118457794, Train return: 74.7848322576984%, Val loss: 0.777833878993988, Val return: -1.559962487199476%, Test loss: 0.7749315500259399, Test return: -1.718607867719054%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31028  4888]\n",
      " [ 5374 30710]]\n",
      "[[4642 4385]\n",
      " [4466 4507]]\n",
      "[[5039 4803]\n",
      " [4989 4876]]\n",
      "Epoch: 13800, Train loss: 0.44048675894737244, Train return: 74.94967558823281%, Val loss: 0.7787460684776306, Val return: -1.3405870380979168%, Test loss: 0.775735080242157, Test return: -2.180842273177813%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[30996  4920]\n",
      " [ 5293 30791]]\n",
      "[[4633 4394]\n",
      " [4442 4531]]\n",
      "[[5024 4818]\n",
      " [5003 4862]]\n",
      "Epoch: 13850, Train loss: 0.4391079246997833, Train return: 75.29348664105646%, Val loss: 0.779445230960846, Val return: -1.1345508943787397%, Test loss: 0.7765291929244995, Test return: -1.8628361347549158%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31286  4630]\n",
      " [ 5488 30596]]\n",
      "[[4709 4318]\n",
      " [4531 4442]]\n",
      "[[5108 4734]\n",
      " [5067 4798]]\n",
      "Epoch: 13900, Train loss: 0.4376473128795624, Train return: 75.45548381215544%, Val loss: 0.7800183892250061, Val return: -1.7551143074900049%, Test loss: 0.7771241664886475, Test return: -1.7736928455304364%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31115  4801]\n",
      " [ 5238 30846]]\n",
      "[[4621 4406]\n",
      " [4454 4519]]\n",
      "[[5015 4827]\n",
      " [4989 4876]]\n",
      "Epoch: 13950, Train loss: 0.43627071380615234, Train return: 75.61988545264096%, Val loss: 0.7814481854438782, Val return: -1.7392480962621386%, Test loss: 0.7782073616981506, Test return: -2.058631642313475%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31284  4632]\n",
      " [ 5345 30739]]\n",
      "[[4676 4351]\n",
      " [4503 4470]]\n",
      "[[5067 4775]\n",
      " [5052 4813]]\n",
      "Epoch: 14000, Train loss: 0.4347372353076935, Train return: 75.66402787911977%, Val loss: 0.7816638350486755, Val return: -1.579604949949351%, Test loss: 0.7785940170288086, Test return: -2.0721138017228724%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31064  4852]\n",
      " [ 5071 31013]]\n",
      "[[4596 4431]\n",
      " [4414 4559]]\n",
      "[[4984 4858]\n",
      " [4952 4913]]\n",
      "Epoch: 14050, Train loss: 0.4332382082939148, Train return: 75.97184822959382%, Val loss: 0.7819463014602661, Val return: -1.5079070289853973%, Test loss: 0.7789928317070007, Test return: -2.0817134511738904%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31203  4713]\n",
      " [ 5125 30959]]\n",
      "[[4619 4408]\n",
      " [4448 4525]]\n",
      "[[5020 4822]\n",
      " [4993 4872]]\n",
      "Epoch: 14100, Train loss: 0.4318816363811493, Train return: 76.16032833871633%, Val loss: 0.7830621004104614, Val return: -1.822108726535108%, Test loss: 0.7800067663192749, Test return: -1.7882599451990449%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31331  4585]\n",
      " [ 5205 30879]]\n",
      "[[4647 4380]\n",
      " [4500 4473]]\n",
      "[[5062 4780]\n",
      " [5013 4852]]\n",
      "Epoch: 14150, Train loss: 0.4304383099079132, Train return: 76.44644756766465%, Val loss: 0.7839891314506531, Val return: -1.4528790997039889%, Test loss: 0.7806312441825867, Test return: -2.025210038660316%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31323  4593]\n",
      " [ 5121 30963]]\n",
      "[[4670 4357]\n",
      " [4475 4498]]\n",
      "[[5040 4802]\n",
      " [5016 4849]]\n",
      "Epoch: 14200, Train loss: 0.42901232838630676, Train return: 76.66849644775495%, Val loss: 0.7844638228416443, Val return: -1.693997947629551%, Test loss: 0.7813450694084167, Test return: -2.089392768677974%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31369  4547]\n",
      " [ 5066 31018]]\n",
      "[[4647 4380]\n",
      " [4476 4497]]\n",
      "[[5022 4820]\n",
      " [5008 4857]]\n",
      "Epoch: 14250, Train loss: 0.4276587963104248, Train return: 76.64609627883966%, Val loss: 0.7854556441307068, Val return: -1.2633314584735567%, Test loss: 0.7825925350189209, Test return: -1.9477925392068365%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31336  4580]\n",
      " [ 5053 31031]]\n",
      "[[4644 4383]\n",
      " [4471 4502]]\n",
      "[[5049 4793]\n",
      " [5025 4840]]\n",
      "Epoch: 14300, Train loss: 0.4262193441390991, Train return: 76.90015137150212%, Val loss: 0.7860188484191895, Val return: -1.1646890973457547%, Test loss: 0.7829403877258301, Test return: -1.935444349534747%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31416  4500]\n",
      " [ 4991 31093]]\n",
      "[[4644 4383]\n",
      " [4450 4523]]\n",
      "[[5024 4818]\n",
      " [4992 4873]]\n",
      "Epoch: 14350, Train loss: 0.42488357424736023, Train return: 77.04752873051916%, Val loss: 0.7867909669876099, Val return: -1.4051914926340638%, Test loss: 0.7839708924293518, Test return: -1.5960561033338552%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[[31523  4393]\n",
      " [ 5067 31017]]\n",
      "[[4700 4327]\n",
      " [4504 4469]]\n",
      "[[5091 4751]\n",
      " [5043 4822]]\n",
      "Epoch: 14400, Train loss: 0.4233972132205963, Train return: 77.22499848404505%, Val loss: 0.7872915863990784, Val return: -1.3029762149579431%, Test loss: 0.7844478487968445, Test return: -1.8122989939533884%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31488  4428]\n",
      " [ 4975 31109]]\n",
      "[[4647 4380]\n",
      " [4458 4515]]\n",
      "[[5044 4798]\n",
      " [5011 4854]]\n",
      "Epoch: 14450, Train loss: 0.4219478964805603, Train return: 77.336982786385%, Val loss: 0.7884238958358765, Val return: -0.8225538239025123%, Test loss: 0.7849605083465576, Test return: -1.8374803394866586%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31402  4514]\n",
      " [ 4813 31271]]\n",
      "[[4616 4411]\n",
      " [4424 4549]]\n",
      "[[4995 4847]\n",
      " [4963 4902]]\n",
      "Epoch: 14500, Train loss: 0.4206840991973877, Train return: 77.70469167405079%, Val loss: 0.7890104055404663, Val return: -1.0735992520044924%, Test loss: 0.7856294512748718, Test return: -1.9677870956002979%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31760  4156]\n",
      " [ 5065 31019]]\n",
      "[[4718 4309]\n",
      " [4544 4429]]\n",
      "[[5120 4722]\n",
      " [5108 4757]]\n",
      "Epoch: 14550, Train loss: 0.4194095730781555, Train return: 77.74375767799255%, Val loss: 0.7896732687950134, Val return: -0.7233468441688639%, Test loss: 0.7866994738578796, Test return: -1.6257631995605841%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31873  4043]\n",
      " [ 5137 30947]]\n",
      "[[4772 4255]\n",
      " [4570 4403]]\n",
      "[[5150 4692]\n",
      " [5108 4757]]\n",
      "Epoch: 14600, Train loss: 0.4179702699184418, Train return: 77.9161216712953%, Val loss: 0.7906708717346191, Val return: -1.2769435481235885%, Test loss: 0.7872535586357117, Test return: -1.7702691787951776%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[31828  4088]\n",
      " [ 5028 31056]]\n",
      "[[4715 4312]\n",
      " [4544 4429]]\n",
      "[[5118 4724]\n",
      " [5085 4780]]\n",
      "Epoch: 14650, Train loss: 0.41649940609931946, Train return: 78.1063506548515%, Val loss: 0.7912122011184692, Val return: -1.2873352787783223%, Test loss: 0.7882198691368103, Test return: -2.0136815300858393%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31709  4207]\n",
      " [ 4878 31206]]\n",
      "[[4680 4347]\n",
      " [4509 4464]]\n",
      "[[5069 4773]\n",
      " [5035 4830]]\n",
      "Epoch: 14700, Train loss: 0.415061354637146, Train return: 78.48168761253568%, Val loss: 0.7923665642738342, Val return: -1.69965785993282%, Test loss: 0.7891216278076172, Test return: -2.1879427040405615%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31576  4340]\n",
      " [ 4653 31431]]\n",
      "[[4598 4429]\n",
      " [4440 4533]]\n",
      "[[4992 4850]\n",
      " [4972 4893]]\n",
      "Epoch: 14750, Train loss: 0.4137743413448334, Train return: 78.55037688796384%, Val loss: 0.7932430505752563, Val return: -1.2488180316196322%, Test loss: 0.7900545001029968, Test return: -2.2109904381463363%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31904  4012]\n",
      " [ 4895 31189]]\n",
      "[[4709 4318]\n",
      " [4548 4425]]\n",
      "[[5092 4750]\n",
      " [5092 4773]]\n",
      "Epoch: 14800, Train loss: 0.41234543919563293, Train return: 78.67280811787688%, Val loss: 0.7941606044769287, Val return: -1.7011457914961767%, Test loss: 0.7906627058982849, Test return: -1.630626141832026%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[31743  4173]\n",
      " [ 4693 31391]]\n",
      "[[4642 4385]\n",
      " [4480 4493]]\n",
      "[[5046 4796]\n",
      " [4997 4868]]\n",
      "Epoch: 14850, Train loss: 0.41098761558532715, Train return: 78.93163121218284%, Val loss: 0.7944903373718262, Val return: -1.0917226532095141%, Test loss: 0.7911258935928345, Test return: -1.8258031824977796%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[31964  3952]\n",
      " [ 4819 31265]]\n",
      "[[4723 4304]\n",
      " [4534 4439]]\n",
      "[[5118 4724]\n",
      " [5075 4790]]\n",
      "Epoch: 14900, Train loss: 0.40956759452819824, Train return: 79.12522971335176%, Val loss: 0.7950008511543274, Val return: -1.5155891056396635%, Test loss: 0.7921267747879028, Test return: -1.619456821805988%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31850  4066]\n",
      " [ 4679 31405]]\n",
      "[[4666 4361]\n",
      " [4500 4473]]\n",
      "[[5065 4777]\n",
      " [5017 4848]]\n",
      "Epoch: 14950, Train loss: 0.40824902057647705, Train return: 79.19836453537799%, Val loss: 0.7956878542900085, Val return: -1.2780643497856494%, Test loss: 0.7924219965934753, Test return: -1.9258728136772443%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 1 1 0]\n",
      "[[31845  4071]\n",
      " [ 4617 31467]]\n",
      "[[4640 4387]\n",
      " [4465 4508]]\n",
      "[[5036 4806]\n",
      " [5006 4859]]\n",
      "Epoch: 15000, Train loss: 0.40690553188323975, Train return: 79.47405975274576%, Val loss: 0.7962572574615479, Val return: -1.1842525539100912%, Test loss: 0.7935389280319214, Test return: -1.5853286400808033%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32036  3880]\n",
      " [ 4715 31369]]\n",
      "[[4706 4321]\n",
      " [4513 4460]]\n",
      "[[5102 4740]\n",
      " [5047 4818]]\n",
      "Epoch: 15050, Train loss: 0.4055997431278229, Train return: 79.74940385617556%, Val loss: 0.7978848814964294, Val return: -1.3363316040322535%, Test loss: 0.7946536540985107, Test return: -1.7860697509876775%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32068  3848]\n",
      " [ 4702 31382]]\n",
      "[[4693 4334]\n",
      " [4511 4462]]\n",
      "[[5089 4753]\n",
      " [5059 4806]]\n",
      "Epoch: 15100, Train loss: 0.4040969908237457, Train return: 80.0768404771548%, Val loss: 0.7980251908302307, Val return: -1.3987262427399159%, Test loss: 0.7950229048728943, Test return: -1.5235735554057108%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32037  3879]\n",
      " [ 4557 31527]]\n",
      "[[4663 4364]\n",
      " [4506 4467]]\n",
      "[[5074 4768]\n",
      " [5017 4848]]\n",
      "Epoch: 15150, Train loss: 0.4028025269508362, Train return: 80.02364915460466%, Val loss: 0.798976480960846, Val return: -1.3241914923079061%, Test loss: 0.7959789633750916, Test return: -2.1874880225513302%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32102  3814]\n",
      " [ 4635 31449]]\n",
      "[[4691 4336]\n",
      " [4524 4449]]\n",
      "[[5080 4762]\n",
      " [5060 4805]]\n",
      "Epoch: 15200, Train loss: 0.4013907313346863, Train return: 80.30991897472639%, Val loss: 0.7999653816223145, Val return: -1.3958486270071147%, Test loss: 0.796596109867096, Test return: -1.8016525781895314%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32044  3872]\n",
      " [ 4491 31593]]\n",
      "[[4653 4374]\n",
      " [4505 4468]]\n",
      "[[5044 4798]\n",
      " [5024 4841]]\n",
      "Epoch: 15250, Train loss: 0.400095134973526, Train return: 80.23637205856011%, Val loss: 0.8002347946166992, Val return: -1.22759903090471%, Test loss: 0.7968494892120361, Test return: -1.9244428730048835%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32042  3874]\n",
      " [ 4435 31649]]\n",
      "[[4644 4383]\n",
      " [4465 4508]]\n",
      "[[5030 4812]\n",
      " [4997 4868]]\n",
      "Epoch: 15300, Train loss: 0.39874860644340515, Train return: 80.71394269510337%, Val loss: 0.8013374209403992, Val return: -1.7237938125462722%, Test loss: 0.7979056239128113, Test return: -1.8508037490296638%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32175  3741]\n",
      " [ 4468 31616]]\n",
      "[[4672 4355]\n",
      " [4528 4445]]\n",
      "[[5081 4761]\n",
      " [5042 4823]]\n",
      "Epoch: 15350, Train loss: 0.39731940627098083, Train return: 80.8369425552061%, Val loss: 0.8020794987678528, Val return: -1.4218196353255206%, Test loss: 0.7986032366752625, Test return: -1.5258500899597647%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32113  3803]\n",
      " [ 4371 31713]]\n",
      "[[4647 4380]\n",
      " [4489 4484]]\n",
      "[[5048 4794]\n",
      " [5006 4859]]\n",
      "Epoch: 15400, Train loss: 0.39617326855659485, Train return: 80.86261593358903%, Val loss: 0.8031058311462402, Val return: -1.3635685844700212%, Test loss: 0.7998167276382446, Test return: -1.472222158592054%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32264  3652]\n",
      " [ 4487 31597]]\n",
      "[[4711 4316]\n",
      " [4562 4411]]\n",
      "[[5131 4711]\n",
      " [5068 4797]]\n",
      "Epoch: 15450, Train loss: 0.39474359154701233, Train return: 81.02271727011488%, Val loss: 0.8036937117576599, Val return: -1.2784317727649548%, Test loss: 0.8003754019737244, Test return: -1.6315737538074482%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32277  3639]\n",
      " [ 4445 31639]]\n",
      "[[4698 4329]\n",
      " [4523 4450]]\n",
      "[[5087 4755]\n",
      " [5046 4819]]\n",
      "Epoch: 15500, Train loss: 0.3933674097061157, Train return: 81.30470281142873%, Val loss: 0.8044859766960144, Val return: -1.5965748241470492%, Test loss: 0.8012453317642212, Test return: -1.6900148278065006%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32161  3755]\n",
      " [ 4241 31843]]\n",
      "[[4636 4391]\n",
      " [4485 4488]]\n",
      "[[5024 4818]\n",
      " [4991 4874]]\n",
      "Epoch: 15550, Train loss: 0.3919828236103058, Train return: 81.49678781397172%, Val loss: 0.8048863410949707, Val return: -1.2886329593896846%, Test loss: 0.8017550110816956, Test return: -1.9243289372751347%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32206  3710]\n",
      " [ 4213 31871]]\n",
      "[[4641 4386]\n",
      " [4480 4493]]\n",
      "[[5038 4804]\n",
      " [5005 4860]]\n",
      "Epoch: 15600, Train loss: 0.3907172977924347, Train return: 81.60076248699582%, Val loss: 0.8057776689529419, Val return: -1.5349284900498412%, Test loss: 0.8023579716682434, Test return: -1.511395190922748%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32347  3569]\n",
      " [ 4269 31815]]\n",
      "[[4678 4349]\n",
      " [4509 4464]]\n",
      "[[5061 4781]\n",
      " [5026 4839]]\n",
      "Epoch: 15650, Train loss: 0.38937219977378845, Train return: 81.75560436578195%, Val loss: 0.8066964745521545, Val return: -1.5400622967460649%, Test loss: 0.8034607172012329, Test return: -1.6225831541871478%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32368  3548]\n",
      " [ 4254 31830]]\n",
      "[[4665 4362]\n",
      " [4510 4463]]\n",
      "[[5056 4786]\n",
      " [5021 4844]]\n",
      "Epoch: 15700, Train loss: 0.3880460262298584, Train return: 81.88528960574207%, Val loss: 0.8071041107177734, Val return: -1.5826881713732799%, Test loss: 0.8039478659629822, Test return: -1.4565200295432872%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32241  3675]\n",
      " [ 4074 32010]]\n",
      "[[4605 4422]\n",
      " [4465 4508]]\n",
      "[[5009 4833]\n",
      " [4961 4904]]\n",
      "Epoch: 15750, Train loss: 0.3868328928947449, Train return: 82.00267081320052%, Val loss: 0.808113694190979, Val return: -1.1313735770745978%, Test loss: 0.8049416542053223, Test return: -1.8641340721615167%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32417  3499]\n",
      " [ 4240 31844]]\n",
      "[[4682 4345]\n",
      " [4515 4458]]\n",
      "[[5082 4760]\n",
      " [5036 4829]]\n",
      "Epoch: 15800, Train loss: 0.3853514492511749, Train return: 82.26645639244191%, Val loss: 0.8090178966522217, Val return: -1.4022115060271976%, Test loss: 0.8054986000061035, Test return: -1.815524775513568%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32275  3641]\n",
      " [ 3971 32113]]\n",
      "[[4613 4414]\n",
      " [4445 4528]]\n",
      "[[4989 4853]\n",
      " [4952 4913]]\n",
      "Epoch: 15850, Train loss: 0.38409677147865295, Train return: 82.35691484068326%, Val loss: 0.809885561466217, Val return: -1.346086840610971%, Test loss: 0.806749701499939, Test return: -2.0167533668042874%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32375  3541]\n",
      " [ 4047 32037]]\n",
      "[[4624 4403]\n",
      " [4474 4499]]\n",
      "[[5010 4832]\n",
      " [4978 4887]]\n",
      "Epoch: 15900, Train loss: 0.3828028738498688, Train return: 82.63145941866416%, Val loss: 0.8103538155555725, Val return: -1.6945447725775327%, Test loss: 0.8074921369552612, Test return: -1.8942750483556985%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32465  3451]\n",
      " [ 4052 32032]]\n",
      "[[4642 4385]\n",
      " [4504 4469]]\n",
      "[[5038 4804]\n",
      " [5023 4842]]\n",
      "Epoch: 15950, Train loss: 0.3815947473049164, Train return: 82.66873302676962%, Val loss: 0.8115228414535522, Val return: -2.043792756205462%, Test loss: 0.8083949685096741, Test return: -1.5432308076453036%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32466  3450]\n",
      " [ 4013 32071]]\n",
      "[[4654 4373]\n",
      " [4502 4471]]\n",
      "[[5054 4788]\n",
      " [5000 4865]]\n",
      "Epoch: 16000, Train loss: 0.3803471624851227, Train return: 82.6456168949558%, Val loss: 0.8122557997703552, Val return: -1.849758312387052%, Test loss: 0.8090237975120544, Test return: -1.5584802619146545%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32678  3238]\n",
      " [ 4214 31870]]\n",
      "[[4716 4311]\n",
      " [4563 4410]]\n",
      "[[5105 4737]\n",
      " [5049 4816]]\n",
      "Epoch: 16050, Train loss: 0.3790157735347748, Train return: 82.89393550932994%, Val loss: 0.8131342530250549, Val return: -1.5527742826683435%, Test loss: 0.8098094463348389, Test return: -1.5050592271535423%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32519  3397]\n",
      " [ 3991 32093]]\n",
      "[[4673 4354]\n",
      " [4499 4474]]\n",
      "[[5067 4775]\n",
      " [5010 4855]]\n",
      "Epoch: 16100, Train loss: 0.3777821660041809, Train return: 83.00823969079227%, Val loss: 0.8140151500701904, Val return: -1.9859305204753732%, Test loss: 0.8108401894569397, Test return: -1.9368450186902069%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32498  3418]\n",
      " [ 3919 32165]]\n",
      "[[4660 4367]\n",
      " [4513 4460]]\n",
      "[[5041 4801]\n",
      " [5008 4857]]\n",
      "Epoch: 16150, Train loss: 0.37636128067970276, Train return: 83.3652519494659%, Val loss: 0.8144466280937195, Val return: -1.639143314100228%, Test loss: 0.8114398717880249, Test return: -1.661404503028373%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32600  3316]\n",
      " [ 3930 32154]]\n",
      "[[4642 4385]\n",
      " [4496 4477]]\n",
      "[[5044 4798]\n",
      " [4993 4872]]\n",
      "Epoch: 16200, Train loss: 0.37514984607696533, Train return: 83.50953120792943%, Val loss: 0.8157263398170471, Val return: -2.3324469100282776%, Test loss: 0.8124542832374573, Test return: -1.7533447025789826%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32547  3369]\n",
      " [ 3831 32253]]\n",
      "[[4614 4413]\n",
      " [4487 4486]]\n",
      "[[5012 4830]\n",
      " [4962 4903]]\n",
      "Epoch: 16250, Train loss: 0.37385740876197815, Train return: 83.54116771580244%, Val loss: 0.8161532282829285, Val return: -1.6335516558423482%, Test loss: 0.8131858110427856, Test return: -1.7705879651094973%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32689  3227]\n",
      " [ 3908 32176]]\n",
      "[[4663 4364]\n",
      " [4501 4472]]\n",
      "[[5054 4788]\n",
      " [5016 4849]]\n",
      "Epoch: 16300, Train loss: 0.37258851528167725, Train return: 83.52761305289852%, Val loss: 0.8171428442001343, Val return: -1.4995197658669652%, Test loss: 0.8138493299484253, Test return: -1.8240937141171847%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32617  3299]\n",
      " [ 3786 32298]]\n",
      "[[4642 4385]\n",
      " [4499 4474]]\n",
      "[[5036 4806]\n",
      " [4997 4868]]\n",
      "Epoch: 16350, Train loss: 0.3713078796863556, Train return: 83.89262154270553%, Val loss: 0.8178719878196716, Val return: -1.9956987544266347%, Test loss: 0.8146634101867676, Test return: -1.7772123393603023%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32576  3340]\n",
      " [ 3664 32420]]\n",
      "[[4597 4430]\n",
      " [4458 4515]]\n",
      "[[4979 4863]\n",
      " [4938 4927]]\n",
      "Epoch: 16400, Train loss: 0.37006875872612, Train return: 83.96204316603402%, Val loss: 0.8185211420059204, Val return: -1.970641104607474%, Test loss: 0.8153428435325623, Test return: -1.4906654067881064%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32690  3226]\n",
      " [ 3731 32353]]\n",
      "[[4648 4379]\n",
      " [4497 4476]]\n",
      "[[5034 4808]\n",
      " [4981 4884]]\n",
      "Epoch: 16450, Train loss: 0.3687494099140167, Train return: 83.9792987217867%, Val loss: 0.8191456198692322, Val return: -1.79831682966393%, Test loss: 0.8164680600166321, Test return: -1.6821655564551359%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32649  3267]\n",
      " [ 3653 32431]]\n",
      "[[4598 4429]\n",
      " [4453 4520]]\n",
      "[[4978 4864]\n",
      " [4946 4919]]\n",
      "Epoch: 16500, Train loss: 0.36757075786590576, Train return: 84.16830633464852%, Val loss: 0.8199629187583923, Val return: -1.9979879791043555%, Test loss: 0.8174331784248352, Test return: -1.7395766558173333%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32819  3097]\n",
      " [ 3808 32276]]\n",
      "[[4664 4363]\n",
      " [4511 4462]]\n",
      "[[5056 4786]\n",
      " [5002 4863]]\n",
      "Epoch: 16550, Train loss: 0.3664197027683258, Train return: 84.29750950657197%, Val loss: 0.8203220367431641, Val return: -1.5709153628485302%, Test loss: 0.8178697824478149, Test return: -1.8423419522905116%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32885  3031]\n",
      " [ 3818 32266]]\n",
      "[[4711 4316]\n",
      " [4556 4417]]\n",
      "[[5078 4764]\n",
      " [5053 4812]]\n",
      "Epoch: 16600, Train loss: 0.3650791645050049, Train return: 84.399199652705%, Val loss: 0.82215416431427, Val return: -1.834514402614084%, Test loss: 0.818702757358551, Test return: -1.6145867658107949%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32732  3184]\n",
      " [ 3572 32512]]\n",
      "[[4640 4387]\n",
      " [4468 4505]]\n",
      "[[5009 4833]\n",
      " [4962 4903]]\n",
      "Epoch: 16650, Train loss: 0.3638297915458679, Train return: 84.5908857879941%, Val loss: 0.8221961259841919, Val return: -1.9975987969943088%, Test loss: 0.819807231426239, Test return: -1.402303937425658%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32880  3036]\n",
      " [ 3701 32383]]\n",
      "[[4638 4389]\n",
      " [4486 4487]]\n",
      "[[5029 4813]\n",
      " [4967 4898]]\n",
      "Epoch: 16700, Train loss: 0.362691730260849, Train return: 84.65520980237417%, Val loss: 0.8233046531677246, Val return: -2.2526217600459297%, Test loss: 0.820427417755127, Test return: -1.608039225494648%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32927  2989]\n",
      " [ 3689 32395]]\n",
      "[[4679 4348]\n",
      " [4524 4449]]\n",
      "[[5091 4751]\n",
      " [5029 4836]]\n",
      "Epoch: 16750, Train loss: 0.3613463342189789, Train return: 84.82316693455707%, Val loss: 0.8239601850509644, Val return: -2.1308491727981336%, Test loss: 0.8212823867797852, Test return: -1.4425451540779546%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32973  2943]\n",
      " [ 3675 32409]]\n",
      "[[4689 4338]\n",
      " [4537 4436]]\n",
      "[[5078 4764]\n",
      " [5038 4827]]\n",
      "Epoch: 16800, Train loss: 0.360151082277298, Train return: 85.07395137505213%, Val loss: 0.8253024816513062, Val return: -1.9769968733002234%, Test loss: 0.8226348757743835, Test return: -1.888167388480343%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33010  2906]\n",
      " [ 3611 32473]]\n",
      "[[4668 4359]\n",
      " [4529 4444]]\n",
      "[[5052 4790]\n",
      " [5019 4846]]\n",
      "Epoch: 16850, Train loss: 0.35904988646507263, Train return: 85.05399143880186%, Val loss: 0.8263304829597473, Val return: -1.6198912210634606%, Test loss: 0.8230202198028564, Test return: -1.6981255629039276%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32907  3009]\n",
      " [ 3550 32534]]\n",
      "[[4670 4357]\n",
      " [4512 4461]]\n",
      "[[5052 4790]\n",
      " [5031 4834]]\n",
      "Epoch: 16900, Train loss: 0.35761532187461853, Train return: 85.1791690008252%, Val loss: 0.8263851404190063, Val return: -1.584316793123168%, Test loss: 0.8234617710113525, Test return: -1.293089924393772%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32995  2921]\n",
      " [ 3556 32528]]\n",
      "[[4675 4352]\n",
      " [4501 4472]]\n",
      "[[5057 4785]\n",
      " [5009 4856]]\n",
      "Epoch: 16950, Train loss: 0.3564179539680481, Train return: 85.49365049464865%, Val loss: 0.8276474475860596, Val return: -2.1749158214629105%, Test loss: 0.8247166275978088, Test return: -1.5489721366577687%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[32931  2985]\n",
      " [ 3388 32696]]\n",
      "[[4596 4431]\n",
      " [4452 4521]]\n",
      "[[4988 4854]\n",
      " [4963 4902]]\n",
      "Epoch: 17000, Train loss: 0.3551539480686188, Train return: 85.49682618667053%, Val loss: 0.8282117247581482, Val return: -2.494445210828144%, Test loss: 0.825527548789978, Test return: -1.8068556407988725%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33020  2896]\n",
      " [ 3416 32668]]\n",
      "[[4654 4373]\n",
      " [4500 4473]]\n",
      "[[5017 4825]\n",
      " [4991 4874]]\n",
      "Epoch: 17050, Train loss: 0.35397788882255554, Train return: 85.61854827357851%, Val loss: 0.8287392854690552, Val return: -1.882934679219843%, Test loss: 0.8265494704246521, Test return: -1.4777574505058642%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33088  2828]\n",
      " [ 3481 32603]]\n",
      "[[4667 4360]\n",
      " [4500 4473]]\n",
      "[[5048 4794]\n",
      " [4995 4870]]\n",
      "Epoch: 17100, Train loss: 0.35276246070861816, Train return: 85.76009040582917%, Val loss: 0.8298335075378418, Val return: -1.8372316817397767%, Test loss: 0.8268682360649109, Test return: -1.9153425744393142%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33029  2887]\n",
      " [ 3354 32730]]\n",
      "[[4651 4376]\n",
      " [4483 4490]]\n",
      "[[5010 4832]\n",
      " [4987 4878]]\n",
      "Epoch: 17150, Train loss: 0.35156765580177307, Train return: 85.91320905349238%, Val loss: 0.8305399417877197, Val return: -1.9325473092977714%, Test loss: 0.8277066946029663, Test return: -1.7679362812482633%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33033  2883]\n",
      " [ 3301 32783]]\n",
      "[[4660 4367]\n",
      " [4506 4467]]\n",
      "[[5013 4829]\n",
      " [5001 4864]]\n",
      "Epoch: 17200, Train loss: 0.35039910674095154, Train return: 85.87303055490813%, Val loss: 0.8315684199333191, Val return: -2.579338599584989%, Test loss: 0.8287189602851868, Test return: -1.3063239754882343%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33118  2798]\n",
      " [ 3355 32729]]\n",
      "[[4640 4387]\n",
      " [4515 4458]]\n",
      "[[5035 4807]\n",
      " [4995 4870]]\n",
      "Epoch: 17250, Train loss: 0.34913840889930725, Train return: 86.07429653446952%, Val loss: 0.8328643441200256, Val return: -2.0597572530548067%, Test loss: 0.8300551176071167, Test return: -1.809615379640077%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33057  2859]\n",
      " [ 3256 32828]]\n",
      "[[4623 4404]\n",
      " [4470 4503]]\n",
      "[[4982 4860]\n",
      " [4960 4905]]\n",
      "Epoch: 17300, Train loss: 0.34795331954956055, Train return: 86.15872216758771%, Val loss: 0.8333193063735962, Val return: -2.2854513595277437%, Test loss: 0.8308096528053284, Test return: -1.523708498152899%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33208  2708]\n",
      " [ 3377 32707]]\n",
      "[[4688 4339]\n",
      " [4523 4450]]\n",
      "[[5050 4792]\n",
      " [5034 4831]]\n",
      "Epoch: 17350, Train loss: 0.34677746891975403, Train return: 86.30973717149304%, Val loss: 0.8342731595039368, Val return: -2.130436009261491%, Test loss: 0.8314960598945618, Test return: -1.5472076704902786%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33153  2763]\n",
      " [ 3237 32847]]\n",
      "[[4645 4382]\n",
      " [4482 4491]]\n",
      "[[5005 4837]\n",
      " [4973 4892]]\n",
      "Epoch: 17400, Train loss: 0.3455563485622406, Train return: 86.52311369736208%, Val loss: 0.8357630968093872, Val return: -2.107498987810117%, Test loss: 0.8320055603981018, Test return: -1.2060649952448097%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33188  2728]\n",
      " [ 3203 32881]]\n",
      "[[4620 4407]\n",
      " [4472 4501]]\n",
      "[[5017 4825]\n",
      " [4966 4899]]\n",
      "Epoch: 17450, Train loss: 0.34464791417121887, Train return: 86.37725221466576%, Val loss: 0.8363281488418579, Val return: -2.2212086371605313%, Test loss: 0.8334774374961853, Test return: -1.6194337616188454%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33305  2611]\n",
      " [ 3364 32720]]\n",
      "[[4723 4304]\n",
      " [4556 4417]]\n",
      "[[5104 4738]\n",
      " [5086 4779]]\n",
      "Epoch: 17500, Train loss: 0.34330976009368896, Train return: 86.53938542691596%, Val loss: 0.8363628387451172, Val return: -1.9383910707099314%, Test loss: 0.8335813283920288, Test return: -1.4578610299061043%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33221  2695]\n",
      " [ 3216 32868]]\n",
      "[[4661 4366]\n",
      " [4503 4470]]\n",
      "[[5057 4785]\n",
      " [5031 4834]]\n",
      "Epoch: 17550, Train loss: 0.34211909770965576, Train return: 86.7501545208761%, Val loss: 0.8374907374382019, Val return: -2.0670734027018622%, Test loss: 0.8348095417022705, Test return: -1.4820118986783355%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33268  2648]\n",
      " [ 3213 32871]]\n",
      "[[4654 4373]\n",
      " [4502 4471]]\n",
      "[[5050 4792]\n",
      " [5034 4831]]\n",
      "Epoch: 17600, Train loss: 0.34086737036705017, Train return: 86.84850058817426%, Val loss: 0.8377901911735535, Val return: -1.72617654185118%, Test loss: 0.835180401802063, Test return: -1.579786236230161%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33088  2828]\n",
      " [ 2966 33118]]\n",
      "[[4559 4468]\n",
      " [4401 4572]]\n",
      "[[4938 4904]\n",
      " [4917 4948]]\n",
      "Epoch: 17650, Train loss: 0.3399524390697479, Train return: 86.93894685072968%, Val loss: 0.8395463228225708, Val return: -1.916108568906338%, Test loss: 0.8361818790435791, Test return: -1.7179290011838138%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33287  2629]\n",
      " [ 3115 32969]]\n",
      "[[4712 4315]\n",
      " [4536 4437]]\n",
      "[[5088 4754]\n",
      " [5081 4784]]\n",
      "Epoch: 17700, Train loss: 0.33855754137039185, Train return: 87.13530574500574%, Val loss: 0.8404802083969116, Val return: -2.5665698992453376%, Test loss: 0.8373050689697266, Test return: -1.7095629868662987%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33307  2609]\n",
      " [ 3036 33048]]\n",
      "[[4654 4373]\n",
      " [4521 4452]]\n",
      "[[5017 4825]\n",
      " [5021 4844]]\n",
      "Epoch: 17750, Train loss: 0.33744364976882935, Train return: 87.23618572614473%, Val loss: 0.8404763340950012, Val return: -2.1528445677392325%, Test loss: 0.8376585841178894, Test return: -1.4928090928638071%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33415  2501]\n",
      " [ 3120 32964]]\n",
      "[[4688 4339]\n",
      " [4524 4449]]\n",
      "[[5058 4784]\n",
      " [5025 4840]]\n",
      "Epoch: 17800, Train loss: 0.336261123418808, Train return: 87.41121071788046%, Val loss: 0.8410868048667908, Val return: -2.3731032443722726%, Test loss: 0.8382801413536072, Test return: -1.4008589115847687%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33485  2431]\n",
      " [ 3141 32943]]\n",
      "[[4687 4340]\n",
      " [4544 4429]]\n",
      "[[5101 4741]\n",
      " [5073 4792]]\n",
      "Epoch: 17850, Train loss: 0.3350965082645416, Train return: 87.5840116694916%, Val loss: 0.8424448370933533, Val return: -2.2242917031256915%, Test loss: 0.8394498825073242, Test return: -1.0814280574400894%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33313  2603]\n",
      " [ 2926 33158]]\n",
      "[[4640 4387]\n",
      " [4479 4494]]\n",
      "[[5004 4838]\n",
      " [4966 4899]]\n",
      "Epoch: 17900, Train loss: 0.3339293599128723, Train return: 87.52503055843438%, Val loss: 0.8436579704284668, Val return: -2.1143502112472468%, Test loss: 0.8405399918556213, Test return: -1.3883201327270702%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33238  2678]\n",
      " [ 2833 33251]]\n",
      "[[4583 4444]\n",
      " [4429 4544]]\n",
      "[[4945 4897]\n",
      " [4935 4930]]\n",
      "Epoch: 17950, Train loss: 0.3327786326408386, Train return: 87.7794654482156%, Val loss: 0.8443395495414734, Val return: -2.1588538723530313%, Test loss: 0.8410575985908508, Test return: -1.5427240946543908%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33364  2552]\n",
      " [ 2912 33172]]\n",
      "[[4652 4375]\n",
      " [4488 4485]]\n",
      "[[5019 4823]\n",
      " [4999 4866]]\n",
      "Epoch: 18000, Train loss: 0.3315451741218567, Train return: 87.70880629673381%, Val loss: 0.8449720740318298, Val return: -2.1519927862037953%, Test loss: 0.8421347141265869, Test return: -1.2509178633512266%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33319  2597]\n",
      " [ 2858 33226]]\n",
      "[[4592 4435]\n",
      " [4428 4545]]\n",
      "[[4968 4874]\n",
      " [4921 4944]]\n",
      "Epoch: 18050, Train loss: 0.3304356634616852, Train return: 87.89247386653739%, Val loss: 0.8460062146186829, Val return: -2.387096450115861%, Test loss: 0.8432889580726624, Test return: -1.235484967571476%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33467  2449]\n",
      " [ 2939 33145]]\n",
      "[[4640 4387]\n",
      " [4508 4465]]\n",
      "[[5036 4806]\n",
      " [5010 4855]]\n",
      "Epoch: 18100, Train loss: 0.3294108510017395, Train return: 88.0355882827225%, Val loss: 0.8469545841217041, Val return: -2.6904007147301843%, Test loss: 0.8439050316810608, Test return: -1.5099958405133629%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33513  2403]\n",
      " [ 2923 33161]]\n",
      "[[4663 4364]\n",
      " [4517 4456]]\n",
      "[[5066 4776]\n",
      " [5047 4818]]\n",
      "Epoch: 18150, Train loss: 0.3281133472919464, Train return: 88.15296444524947%, Val loss: 0.8474835157394409, Val return: -2.355857749421502%, Test loss: 0.8443575501441956, Test return: -1.6204302833488033%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33386  2530]\n",
      " [ 2762 33322]]\n",
      "[[4622 4405]\n",
      " [4469 4504]]\n",
      "[[4988 4854]\n",
      " [4976 4889]]\n",
      "Epoch: 18200, Train loss: 0.3270803987979889, Train return: 88.30258851750084%, Val loss: 0.8486506342887878, Val return: -2.4686863680512467%, Test loss: 0.84554523229599, Test return: -1.4160066830265974%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33575  2341]\n",
      " [ 2922 33162]]\n",
      "[[4663 4364]\n",
      " [4524 4449]]\n",
      "[[5063 4779]\n",
      " [5024 4841]]\n",
      "Epoch: 18250, Train loss: 0.3260408937931061, Train return: 88.37096206701555%, Val loss: 0.8497269153594971, Val return: -2.209258738160614%, Test loss: 0.8472894430160522, Test return: -1.4307904465999777%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33696  2220]\n",
      " [ 3001 33083]]\n",
      "[[4704 4323]\n",
      " [4562 4411]]\n",
      "[[5093 4749]\n",
      " [5059 4806]]\n",
      "Epoch: 18300, Train loss: 0.32508528232574463, Train return: 88.3914664448849%, Val loss: 0.8506848812103271, Val return: -2.2969866907650833%, Test loss: 0.8472386002540588, Test return: -1.4309911801236272%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33635  2281]\n",
      " [ 2954 33130]]\n",
      "[[4749 4278]\n",
      " [4573 4400]]\n",
      "[[5130 4712]\n",
      " [5116 4749]]\n",
      "Epoch: 18350, Train loss: 0.323688805103302, Train return: 88.43500185546436%, Val loss: 0.8512845039367676, Val return: -2.6377181409363573%, Test loss: 0.8483969569206238, Test return: -1.259144567692001%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33546  2370]\n",
      " [ 2748 33336]]\n",
      "[[4600 4427]\n",
      " [4460 4513]]\n",
      "[[4977 4865]\n",
      " [4969 4896]]\n",
      "Epoch: 18400, Train loss: 0.32253915071487427, Train return: 88.64292068456659%, Val loss: 0.8513554334640503, Val return: -2.4277475167012184%, Test loss: 0.8493283987045288, Test return: -1.6835206198689254%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33579  2337]\n",
      " [ 2749 33335]]\n",
      "[[4620 4407]\n",
      " [4492 4481]]\n",
      "[[4993 4849]\n",
      " [4989 4876]]\n",
      "Epoch: 18450, Train loss: 0.3215826749801636, Train return: 88.57941005273851%, Val loss: 0.8524200916290283, Val return: -2.161405684732218%, Test loss: 0.8499646782875061, Test return: -1.1019778416699437%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33619  2297]\n",
      " [ 2847 33237]]\n",
      "[[4635 4392]\n",
      " [4485 4488]]\n",
      "[[5024 4818]\n",
      " [4965 4900]]\n",
      "Epoch: 18500, Train loss: 0.3204460144042969, Train return: 88.72838315497305%, Val loss: 0.8539335131645203, Val return: -2.080249402469486%, Test loss: 0.8507071733474731, Test return: -0.9852906590215952%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33611  2305]\n",
      " [ 2745 33339]]\n",
      "[[4640 4387]\n",
      " [4478 4495]]\n",
      "[[5015 4827]\n",
      " [4972 4893]]\n",
      "Epoch: 18550, Train loss: 0.3193270266056061, Train return: 88.87264631158503%, Val loss: 0.8546316027641296, Val return: -2.199789540621039%, Test loss: 0.8516387939453125, Test return: -1.5283122978858024%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33775  2141]\n",
      " [ 2869 33215]]\n",
      "[[4692 4335]\n",
      " [4571 4402]]\n",
      "[[5092 4750]\n",
      " [5065 4800]]\n",
      "Epoch: 18600, Train loss: 0.3181585967540741, Train return: 89.10248755529226%, Val loss: 0.855158805847168, Val return: -2.108998619557268%, Test loss: 0.8519737124443054, Test return: -1.4723343735189973%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33574  2342]\n",
      " [ 2646 33438]]\n",
      "[[4639 4388]\n",
      " [4498 4475]]\n",
      "[[5012 4830]\n",
      " [4988 4877]]\n",
      "Epoch: 18650, Train loss: 0.31707215309143066, Train return: 88.98854483904954%, Val loss: 0.8562058806419373, Val return: -2.602577037751178%, Test loss: 0.8532483577728271, Test return: -1.1910683596083558%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33664  2252]\n",
      " [ 2666 33418]]\n",
      "[[4628 4399]\n",
      " [4493 4480]]\n",
      "[[5035 4807]\n",
      " [5009 4856]]\n",
      "Epoch: 18700, Train loss: 0.3160029649734497, Train return: 89.20573920967236%, Val loss: 0.857099711894989, Val return: -2.0403219735202964%, Test loss: 0.8543026447296143, Test return: -1.53889569617118%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33721  2195]\n",
      " [ 2715 33369]]\n",
      "[[4662 4365]\n",
      " [4522 4451]]\n",
      "[[5042 4800]\n",
      " [5027 4838]]\n",
      "Epoch: 18750, Train loss: 0.3149402439594269, Train return: 89.36814061247547%, Val loss: 0.8581669926643372, Val return: -2.2020386518391817%, Test loss: 0.8554126620292664, Test return: -1.148085884429106%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33702  2214]\n",
      " [ 2644 33440]]\n",
      "[[4690 4337]\n",
      " [4530 4443]]\n",
      "[[5057 4785]\n",
      " [5040 4825]]\n",
      "Epoch: 18800, Train loss: 0.3139494061470032, Train return: 89.33349687865001%, Val loss: 0.8589498400688171, Val return: -2.3269889182787464%, Test loss: 0.8560238480567932, Test return: -1.1965300733535646%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33865  2051]\n",
      " [ 2773 33311]]\n",
      "[[4713 4314]\n",
      " [4579 4394]]\n",
      "[[5132 4710]\n",
      " [5096 4769]]\n",
      "Epoch: 18850, Train loss: 0.3126756548881531, Train return: 89.50038971436531%, Val loss: 0.8592840433120728, Val return: -2.2203446049643447%, Test loss: 0.8566314578056335, Test return: -1.4139701316676843%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33654  2262]\n",
      " [ 2516 33568]]\n",
      "[[4588 4439]\n",
      " [4439 4534]]\n",
      "[[4960 4882]\n",
      " [4943 4922]]\n",
      "Epoch: 18900, Train loss: 0.3115396499633789, Train return: 89.61945117930867%, Val loss: 0.8600106835365295, Val return: -1.8988522682968245%, Test loss: 0.8573554754257202, Test return: -1.4068455648926748%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33800  2116]\n",
      " [ 2631 33453]]\n",
      "[[4649 4378]\n",
      " [4518 4455]]\n",
      "[[5020 4822]\n",
      " [5000 4865]]\n",
      "Epoch: 18950, Train loss: 0.3104912340641022, Train return: 89.7277104641458%, Val loss: 0.8619491457939148, Val return: -2.1827586297424695%, Test loss: 0.8585134744644165, Test return: -1.1492987573530455%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33679  2237]\n",
      " [ 2451 33633]]\n",
      "[[4602 4425]\n",
      " [4442 4531]]\n",
      "[[4984 4858]\n",
      " [4949 4916]]\n",
      "Epoch: 19000, Train loss: 0.3096519410610199, Train return: 89.5743810184971%, Val loss: 0.8618564009666443, Val return: -2.3401668778462676%, Test loss: 0.8599092960357666, Test return: -1.1563318410330554%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33929  1987]\n",
      " [ 2766 33318]]\n",
      "[[4666 4361]\n",
      " [4540 4433]]\n",
      "[[5064 4778]\n",
      " [5048 4817]]\n",
      "Epoch: 19050, Train loss: 0.3085334897041321, Train return: 89.74938903464236%, Val loss: 0.8628765940666199, Val return: -2.1974419042220004%, Test loss: 0.8600816130638123, Test return: -0.6794502801564289%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33860  2056]\n",
      " [ 2634 33450]]\n",
      "[[4675 4352]\n",
      " [4542 4431]]\n",
      "[[5088 4754]\n",
      " [5022 4843]]\n",
      "Epoch: 19100, Train loss: 0.307363361120224, Train return: 89.939391101104%, Val loss: 0.8634840250015259, Val return: -2.7791676543593056%, Test loss: 0.8613756895065308, Test return: -0.9077081179606218%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33873  2043]\n",
      " [ 2548 33536]]\n",
      "[[4676 4351]\n",
      " [4530 4443]]\n",
      "[[5053 4789]\n",
      " [5030 4835]]\n",
      "Epoch: 19150, Train loss: 0.30631953477859497, Train return: 90.10496604449868%, Val loss: 0.8648672699928284, Val return: -2.1549109180598722%, Test loss: 0.862452507019043, Test return: -0.9545641345527817%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33825  2091]\n",
      " [ 2476 33608]]\n",
      "[[4634 4393]\n",
      " [4491 4482]]\n",
      "[[5022 4820]\n",
      " [4983 4882]]\n",
      "Epoch: 19200, Train loss: 0.3051036596298218, Train return: 90.14328957935801%, Val loss: 0.8650352358818054, Val return: -2.59428944664063%, Test loss: 0.8630222678184509, Test return: -1.003968901335492%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33843  2073]\n",
      " [ 2470 33614]]\n",
      "[[4601 4426]\n",
      " [4483 4490]]\n",
      "[[5011 4831]\n",
      " [4961 4904]]\n",
      "Epoch: 19250, Train loss: 0.3041238486766815, Train return: 90.09805288211693%, Val loss: 0.8661608695983887, Val return: -2.2950566726884154%, Test loss: 0.8638660907745361, Test return: -1.2579259194464931%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33907  2009]\n",
      " [ 2533 33551]]\n",
      "[[4647 4380]\n",
      " [4503 4470]]\n",
      "[[5041 4801]\n",
      " [5016 4849]]\n",
      "Epoch: 19300, Train loss: 0.30306845903396606, Train return: 90.40176901494972%, Val loss: 0.8675975203514099, Val return: -2.2923696086311316%, Test loss: 0.8650147318840027, Test return: -1.3157157866264453%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33868  2048]\n",
      " [ 2392 33692]]\n",
      "[[4620 4407]\n",
      " [4485 4488]]\n",
      "[[5002 4840]\n",
      " [4989 4876]]\n",
      "Epoch: 19350, Train loss: 0.30198004841804504, Train return: 90.28734938809488%, Val loss: 0.8680540919303894, Val return: -2.8640413482739873%, Test loss: 0.8655826449394226, Test return: -1.4426051248598768%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33635  2281]\n",
      " [ 2187 33897]]\n",
      "[[4495 4532]\n",
      " [4374 4599]]\n",
      "[[4863 4979]\n",
      " [4853 5012]]\n",
      "Epoch: 19400, Train loss: 0.3010755777359009, Train return: 90.27533018425731%, Val loss: 0.8688429594039917, Val return: -2.4404941648321183%, Test loss: 0.8668805360794067, Test return: -1.059551016039926%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33927  1989]\n",
      " [ 2434 33650]]\n",
      "[[4673 4354]\n",
      " [4528 4445]]\n",
      "[[5056 4786]\n",
      " [5038 4827]]\n",
      "Epoch: 19450, Train loss: 0.29994499683380127, Train return: 90.42442102329187%, Val loss: 0.8697603940963745, Val return: -2.5459314350948725%, Test loss: 0.86741042137146, Test return: -1.5957025024805116%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33866  2050]\n",
      " [ 2336 33748]]\n",
      "[[4600 4427]\n",
      " [4466 4507]]\n",
      "[[4979 4863]\n",
      " [4966 4899]]\n",
      "Epoch: 19500, Train loss: 0.2989445924758911, Train return: 90.4889838394056%, Val loss: 0.870571494102478, Val return: -1.977756299583968%, Test loss: 0.8689151406288147, Test return: -1.0111100757338498%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33947  1969]\n",
      " [ 2431 33653]]\n",
      "[[4643 4384]\n",
      " [4495 4478]]\n",
      "[[5038 4804]\n",
      " [5013 4852]]\n",
      "Epoch: 19550, Train loss: 0.29800888895988464, Train return: 90.43965009787517%, Val loss: 0.8714197278022766, Val return: -2.845151568465161%, Test loss: 0.8698128461837769, Test return: -1.0802026528545896%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[34102  1814]\n",
      " [ 2553 33531]]\n",
      "[[4673 4354]\n",
      " [4553 4420]]\n",
      "[[5066 4776]\n",
      " [5045 4820]]\n",
      "Epoch: 19600, Train loss: 0.29691028594970703, Train return: 90.58450973822112%, Val loss: 0.8723353147506714, Val return: -2.473207789523272%, Test loss: 0.8703075051307678, Test return: -1.2111116220476084%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[34031  1885]\n",
      " [ 2436 33648]]\n",
      "[[4665 4362]\n",
      " [4536 4437]]\n",
      "[[5047 4795]\n",
      " [5046 4819]]\n",
      "Epoch: 19650, Train loss: 0.29578590393066406, Train return: 90.68259969838587%, Val loss: 0.8736052513122559, Val return: -1.8862512213204743%, Test loss: 0.8712587356567383, Test return: -0.98058614318424%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33949  1967]\n",
      " [ 2307 33777]]\n",
      "[[4636 4391]\n",
      " [4484 4489]]\n",
      "[[5022 4820]\n",
      " [5007 4858]]\n",
      "Epoch: 19700, Train loss: 0.29479479789733887, Train return: 90.79881363793368%, Val loss: 0.8740362524986267, Val return: -2.119558650895424%, Test loss: 0.87212073802948, Test return: -1.2357495637861984%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33937  1979]\n",
      " [ 2266 33818]]\n",
      "[[4623 4404]\n",
      " [4473 4500]]\n",
      "[[5008 4834]\n",
      " [4997 4868]]\n",
      "Epoch: 19750, Train loss: 0.2939094305038452, Train return: 90.87782736216775%, Val loss: 0.874344527721405, Val return: -2.24604423069253%, Test loss: 0.8723727464675903, Test return: -0.8203164290882556%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[34127  1789]\n",
      " [ 2451 33633]]\n",
      "[[4655 4372]\n",
      " [4524 4449]]\n",
      "[[5057 4785]\n",
      " [5010 4855]]\n",
      "Epoch: 19800, Train loss: 0.29278337955474854, Train return: 90.9350004191178%, Val loss: 0.8764119744300842, Val return: -2.1709435033018183%, Test loss: 0.8738492727279663, Test return: -1.179794404764933%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33846  2070]\n",
      " [ 2114 33970]]\n",
      "[[4590 4437]\n",
      " [4438 4535]]\n",
      "[[4969 4873]\n",
      " [4932 4933]]\n",
      "Epoch: 19850, Train loss: 0.29194149374961853, Train return: 90.9607435444937%, Val loss: 0.8773682117462158, Val return: -1.9131955671896597%, Test loss: 0.8745205998420715, Test return: -1.3876946557518037%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33935  1981]\n",
      " [ 2194 33890]]\n",
      "[[4626 4401]\n",
      " [4466 4507]]\n",
      "[[5011 4831]\n",
      " [4986 4879]]\n",
      "Epoch: 19900, Train loss: 0.290630966424942, Train return: 90.98322030330537%, Val loss: 0.8783280849456787, Val return: -2.150463252854279%, Test loss: 0.8757131695747375, Test return: -1.2468682854194328%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[33975  1941]\n",
      " [ 2198 33886]]\n",
      "[[4610 4417]\n",
      " [4477 4496]]\n",
      "[[4991 4851]\n",
      " [4977 4888]]\n",
      "Epoch: 19950, Train loss: 0.28979888558387756, Train return: 91.11467198963088%, Val loss: 0.8791510462760925, Val return: -2.4487651142088005%, Test loss: 0.8767006993293762, Test return: -1.3284959442628694%\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[[34089  1827]\n",
      " [ 2286 33798]]\n",
      "[[4657 4370]\n",
      " [4526 4447]]\n",
      "[[5045 4797]\n",
      " [5045 4820]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "# loss_fn = F.mse_loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20de8fa4-22a4-42a9-83f7-c7a3bb94972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.7000365853309631, Train return: -0.5902353016182413%, Val loss: 0.7003042697906494, Val return: -1.4791476688500207%, Test loss: 0.6998363137245178, Test return: -2.6139642153796396%\n",
      "Epoch: 500, Train loss: 0.6882641911506653, Train return: -0.16885859611980183%, Val loss: 0.69290691614151, Val return: -4.2289264085754725%, Test loss: 0.6922606229782104, Test return: -2.716177443646274%\n",
      "Epoch: 1000, Train loss: 0.6856869459152222, Train return: 0.719771811015101%, Val loss: 0.6932183504104614, Val return: -5.622322446674505%, Test loss: 0.6924225687980652, Test return: -3.135761811942369%\n",
      "Epoch: 1500, Train loss: 0.6833482384681702, Train return: 3.147993707261825%, Val loss: 0.6938254237174988, Val return: -6.726913021146272%, Test loss: 0.692857027053833, Test return: -3.4213881686206213%\n",
      "Epoch: 2000, Train loss: 0.680908739566803, Train return: 4.877127921598227%, Val loss: 0.6945688724517822, Val return: -6.8525266887715235%, Test loss: 0.6933789253234863, Test return: -2.6414784906243876%\n",
      "Epoch: 2500, Train loss: 0.6782220005989075, Train return: 7.0611245431582645%, Val loss: 0.695425271987915, Val return: -7.4485625409830805%, Test loss: 0.6940110325813293, Test return: -2.763180264195249%\n",
      "Epoch: 3000, Train loss: 0.6752036213874817, Train return: 9.603830744033012%, Val loss: 0.6963979601860046, Val return: -6.187098723391997%, Test loss: 0.6947280168533325, Test return: -2.21950510802593%\n",
      "Epoch: 3500, Train loss: 0.6717534065246582, Train return: 12.162862100369336%, Val loss: 0.6974309682846069, Val return: -6.070546026222741%, Test loss: 0.6955330967903137, Test return: -1.809593691806575%\n",
      "Epoch: 4000, Train loss: 0.6677684783935547, Train return: 14.700401434264169%, Val loss: 0.698619544506073, Val return: -5.644815895834492%, Test loss: 0.6964861154556274, Test return: -1.3623565241758804%\n",
      "Epoch: 4500, Train loss: 0.6631574034690857, Train return: 17.843785641301384%, Val loss: 0.6999649405479431, Val return: -5.537122386186732%, Test loss: 0.697586715221405, Test return: -1.2392199888257305%\n",
      "Epoch: 5000, Train loss: 0.6578305959701538, Train return: 20.99817791124939%, Val loss: 0.7015202045440674, Val return: -5.339290867016694%, Test loss: 0.6989060640335083, Test return: -1.2007729047668774%\n",
      "Epoch: 5500, Train loss: 0.6517013311386108, Train return: 23.765874186833386%, Val loss: 0.7032741904258728, Val return: -3.989397067953952%, Test loss: 0.700474202632904, Test return: -1.189293343472232%\n",
      "Epoch: 6000, Train loss: 0.6446447372436523, Train return: 26.80579291761407%, Val loss: 0.7051588296890259, Val return: -3.6911519668898807%, Test loss: 0.7022128701210022, Test return: -1.0421038589263754%\n",
      "Epoch: 6500, Train loss: 0.6366303563117981, Train return: 29.89453151389258%, Val loss: 0.7073732018470764, Val return: -2.897921125263364%, Test loss: 0.704365611076355, Test return: -0.7558017324431928%\n",
      "Epoch: 7000, Train loss: 0.6275851726531982, Train return: 33.038568791882916%, Val loss: 0.709896981716156, Val return: -3.260985973420011%, Test loss: 0.7067713737487793, Test return: -0.5243855978543047%\n",
      "Epoch: 7500, Train loss: 0.6174809336662292, Train return: 36.509971493593525%, Val loss: 0.712721586227417, Val return: -2.854457320222236%, Test loss: 0.7096045017242432, Test return: -0.765907692828394%\n",
      "Epoch: 8000, Train loss: 0.6063763499259949, Train return: 39.97540384452479%, Val loss: 0.7158689498901367, Val return: -2.976615514028376%, Test loss: 0.7127295136451721, Test return: -0.4616628180233717%\n",
      "Epoch: 8500, Train loss: 0.5943328738212585, Train return: 43.804397240533184%, Val loss: 0.7195634841918945, Val return: -3.3522777622515494%, Test loss: 0.7164931297302246, Test return: -0.35160846532559986%\n",
      "Epoch: 9000, Train loss: 0.581467866897583, Train return: 47.11965586775783%, Val loss: 0.723527193069458, Val return: -3.092019572221788%, Test loss: 0.7204956412315369, Test return: -0.055582975641938336%\n",
      "Epoch: 9500, Train loss: 0.5678315758705139, Train return: 51.100889032972674%, Val loss: 0.7276986837387085, Val return: -2.468298595264144%, Test loss: 0.7248287796974182, Test return: -0.3980485819272954%\n",
      "Epoch: 10000, Train loss: 0.5535802245140076, Train return: 54.71012067910201%, Val loss: 0.732463002204895, Val return: -2.409875046338442%, Test loss: 0.7295854687690735, Test return: -0.46672085430272336%\n",
      "Epoch: 10500, Train loss: 0.5389317274093628, Train return: 57.617529437416316%, Val loss: 0.7373270392417908, Val return: -2.291192092551699%, Test loss: 0.7347713112831116, Test return: -1.0827198514198477%\n",
      "Epoch: 11000, Train loss: 0.5240696668624878, Train return: 60.60793247612734%, Val loss: 0.7429274320602417, Val return: -2.2502558330306663%, Test loss: 0.7403854727745056, Test return: -1.1527647365386227%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-8b2c459a1c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sum_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_path_classify_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-2d167e686c30>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "num_epochs = 30000\n",
    "learning_rate = 0.001\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_classify_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2e089-815d-4596-ab7f-c16d8ca96272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
