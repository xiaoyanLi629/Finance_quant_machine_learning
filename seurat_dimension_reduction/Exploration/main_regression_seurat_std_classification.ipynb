{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c70773-809f-470a-a9a4-b5907273e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython import display\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8b0a34-3d3c-4a77-8a53-a01326ec1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a475f5-7b98-40da-be68-1770720796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22db1e1e-0226-489b-8e26-805b8c8d23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = pd.read_csv('./integrate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff9203-6526-4815-a36c-c9df9dcca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bddaa6-f20c-4981-a9a1-38680562673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat.reset_index(inplace=True) \n",
    "train_test_seurat.drop(\"index\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6eac3f-02b5-4c75-bca7-7e266b51af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat_std = train_test_seurat.std()\n",
    "column_names = list(train_test_seurat.columns)\n",
    "columns_remove = []\n",
    "for i in range(train_test_seurat.shape[1]):\n",
    "    if train_test_seurat_std[i] == 0:\n",
    "        columns_remove.append(column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6a640d-17ca-4197-9120-b68b3a8d8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.drop(columns_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0866b5b-8359-4a8d-aa6d-e3ae91eb6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat[columns_remove[0]] = train_test_seurat.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc71a28-7056-4114-b6cf-fd9f9454edb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109707, 54)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_seurat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f242b92-78e5-4a99-9bef-cb40d10e9121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X39</th>\n",
       "      <th>X53</th>\n",
       "      <th>X45</th>\n",
       "      <th>X13</th>\n",
       "      <th>X36</th>\n",
       "      <th>X15</th>\n",
       "      <th>X51</th>\n",
       "      <th>X49</th>\n",
       "      <th>X33</th>\n",
       "      <th>X40</th>\n",
       "      <th>X31</th>\n",
       "      <th>X34</th>\n",
       "      <th>X32</th>\n",
       "      <th>X50</th>\n",
       "      <th>X30</th>\n",
       "      <th>X18</th>\n",
       "      <th>X46</th>\n",
       "      <th>X35</th>\n",
       "      <th>X37</th>\n",
       "      <th>X17</th>\n",
       "      <th>X42</th>\n",
       "      <th>X38</th>\n",
       "      <th>X43</th>\n",
       "      <th>X47</th>\n",
       "      <th>X0</th>\n",
       "      <th>X52</th>\n",
       "      <th>X48</th>\n",
       "      <th>X41</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X25</th>\n",
       "      <th>X23</th>\n",
       "      <th>X9</th>\n",
       "      <th>X27</th>\n",
       "      <th>X14</th>\n",
       "      <th>X19</th>\n",
       "      <th>X24</th>\n",
       "      <th>X16</th>\n",
       "      <th>X29</th>\n",
       "      <th>X26</th>\n",
       "      <th>X28</th>\n",
       "      <th>X21</th>\n",
       "      <th>X1</th>\n",
       "      <th>X11</th>\n",
       "      <th>X20</th>\n",
       "      <th>X22</th>\n",
       "      <th>X5</th>\n",
       "      <th>X4</th>\n",
       "      <th>X6</th>\n",
       "      <th>X12</th>\n",
       "      <th>X7</th>\n",
       "      <th>X10</th>\n",
       "      <th>X8</th>\n",
       "      <th>X62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.582156</td>\n",
       "      <td>1.230171</td>\n",
       "      <td>2.175619</td>\n",
       "      <td>2.246967</td>\n",
       "      <td>-0.755876</td>\n",
       "      <td>3.583055</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>3.040176</td>\n",
       "      <td>0.919099</td>\n",
       "      <td>-0.889421</td>\n",
       "      <td>0.484578</td>\n",
       "      <td>0.481170</td>\n",
       "      <td>0.777112</td>\n",
       "      <td>-1.798247</td>\n",
       "      <td>0.873301</td>\n",
       "      <td>2.270459</td>\n",
       "      <td>2.912388</td>\n",
       "      <td>0.728549</td>\n",
       "      <td>-1.295910</td>\n",
       "      <td>2.275724</td>\n",
       "      <td>-0.359744</td>\n",
       "      <td>-0.363988</td>\n",
       "      <td>-0.717875</td>\n",
       "      <td>2.265473</td>\n",
       "      <td>1.019072</td>\n",
       "      <td>0.721157</td>\n",
       "      <td>2.260484</td>\n",
       "      <td>-1.834228</td>\n",
       "      <td>2.254474</td>\n",
       "      <td>2.254450</td>\n",
       "      <td>3.042155</td>\n",
       "      <td>2.259637</td>\n",
       "      <td>2.946272</td>\n",
       "      <td>2.258885</td>\n",
       "      <td>3.401480</td>\n",
       "      <td>3.345012</td>\n",
       "      <td>2.937407</td>\n",
       "      <td>3.343820</td>\n",
       "      <td>2.935077</td>\n",
       "      <td>2.941428</td>\n",
       "      <td>2.259203</td>\n",
       "      <td>2.938547</td>\n",
       "      <td>2.469231</td>\n",
       "      <td>2.945955</td>\n",
       "      <td>3.044228</td>\n",
       "      <td>2.259368</td>\n",
       "      <td>3.073369</td>\n",
       "      <td>2.943574</td>\n",
       "      <td>2.945749</td>\n",
       "      <td>2.259507</td>\n",
       "      <td>2.259501</td>\n",
       "      <td>3.071520</td>\n",
       "      <td>2.259504</td>\n",
       "      <td>-1.582156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.584404</td>\n",
       "      <td>1.605045</td>\n",
       "      <td>-2.204271</td>\n",
       "      <td>2.227963</td>\n",
       "      <td>-0.760217</td>\n",
       "      <td>3.535377</td>\n",
       "      <td>0.300338</td>\n",
       "      <td>2.880602</td>\n",
       "      <td>0.943713</td>\n",
       "      <td>-0.890453</td>\n",
       "      <td>0.513255</td>\n",
       "      <td>0.509388</td>\n",
       "      <td>0.803151</td>\n",
       "      <td>-1.042563</td>\n",
       "      <td>0.900408</td>\n",
       "      <td>2.251683</td>\n",
       "      <td>2.831043</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>-1.302729</td>\n",
       "      <td>2.256918</td>\n",
       "      <td>-0.368216</td>\n",
       "      <td>-0.372547</td>\n",
       "      <td>-0.725350</td>\n",
       "      <td>2.242051</td>\n",
       "      <td>1.084322</td>\n",
       "      <td>1.273463</td>\n",
       "      <td>2.239054</td>\n",
       "      <td>-1.836103</td>\n",
       "      <td>2.235786</td>\n",
       "      <td>2.235849</td>\n",
       "      <td>2.995526</td>\n",
       "      <td>2.240825</td>\n",
       "      <td>2.901227</td>\n",
       "      <td>2.240080</td>\n",
       "      <td>3.359956</td>\n",
       "      <td>3.304083</td>\n",
       "      <td>2.898066</td>\n",
       "      <td>3.302884</td>\n",
       "      <td>2.895731</td>\n",
       "      <td>2.902085</td>\n",
       "      <td>2.240400</td>\n",
       "      <td>2.910457</td>\n",
       "      <td>2.446440</td>\n",
       "      <td>2.903884</td>\n",
       "      <td>3.007216</td>\n",
       "      <td>2.240566</td>\n",
       "      <td>3.004351</td>\n",
       "      <td>2.907020</td>\n",
       "      <td>2.906517</td>\n",
       "      <td>2.240683</td>\n",
       "      <td>2.240706</td>\n",
       "      <td>3.025257</td>\n",
       "      <td>2.240732</td>\n",
       "      <td>-1.584404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.594548</td>\n",
       "      <td>1.172960</td>\n",
       "      <td>2.547328</td>\n",
       "      <td>2.138920</td>\n",
       "      <td>-0.779797</td>\n",
       "      <td>3.321969</td>\n",
       "      <td>-0.241806</td>\n",
       "      <td>2.693446</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>-0.895068</td>\n",
       "      <td>0.505998</td>\n",
       "      <td>0.502247</td>\n",
       "      <td>0.793162</td>\n",
       "      <td>-1.196648</td>\n",
       "      <td>0.893682</td>\n",
       "      <td>2.165409</td>\n",
       "      <td>2.656538</td>\n",
       "      <td>0.750002</td>\n",
       "      <td>-1.334065</td>\n",
       "      <td>2.170502</td>\n",
       "      <td>-0.407147</td>\n",
       "      <td>-0.411875</td>\n",
       "      <td>-0.759699</td>\n",
       "      <td>2.150257</td>\n",
       "      <td>0.909735</td>\n",
       "      <td>0.858274</td>\n",
       "      <td>2.152557</td>\n",
       "      <td>-1.844565</td>\n",
       "      <td>2.150014</td>\n",
       "      <td>2.149977</td>\n",
       "      <td>2.786986</td>\n",
       "      <td>2.154385</td>\n",
       "      <td>2.734335</td>\n",
       "      <td>2.153668</td>\n",
       "      <td>3.172415</td>\n",
       "      <td>3.119236</td>\n",
       "      <td>2.720591</td>\n",
       "      <td>3.118004</td>\n",
       "      <td>2.718235</td>\n",
       "      <td>2.724605</td>\n",
       "      <td>2.154003</td>\n",
       "      <td>2.731983</td>\n",
       "      <td>2.340769</td>\n",
       "      <td>2.729945</td>\n",
       "      <td>2.798645</td>\n",
       "      <td>2.154173</td>\n",
       "      <td>2.832534</td>\n",
       "      <td>2.727687</td>\n",
       "      <td>2.728807</td>\n",
       "      <td>2.154322</td>\n",
       "      <td>2.154294</td>\n",
       "      <td>2.816114</td>\n",
       "      <td>2.154304</td>\n",
       "      <td>-1.594548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.540117</td>\n",
       "      <td>1.471016</td>\n",
       "      <td>-1.464351</td>\n",
       "      <td>2.589700</td>\n",
       "      <td>-0.674728</td>\n",
       "      <td>4.499712</td>\n",
       "      <td>-0.070470</td>\n",
       "      <td>3.630669</td>\n",
       "      <td>-0.163725</td>\n",
       "      <td>-0.869522</td>\n",
       "      <td>0.771819</td>\n",
       "      <td>0.763805</td>\n",
       "      <td>1.057894</td>\n",
       "      <td>-1.013944</td>\n",
       "      <td>1.144550</td>\n",
       "      <td>2.601021</td>\n",
       "      <td>3.586176</td>\n",
       "      <td>1.014063</td>\n",
       "      <td>-1.175844</td>\n",
       "      <td>2.588606</td>\n",
       "      <td>-0.210578</td>\n",
       "      <td>-0.213300</td>\n",
       "      <td>-0.586264</td>\n",
       "      <td>2.586420</td>\n",
       "      <td>1.452937</td>\n",
       "      <td>1.030784</td>\n",
       "      <td>2.588699</td>\n",
       "      <td>-1.799160</td>\n",
       "      <td>2.583419</td>\n",
       "      <td>2.583483</td>\n",
       "      <td>3.941371</td>\n",
       "      <td>2.590836</td>\n",
       "      <td>3.676733</td>\n",
       "      <td>2.589976</td>\n",
       "      <td>4.174776</td>\n",
       "      <td>4.107204</td>\n",
       "      <td>3.672950</td>\n",
       "      <td>4.106144</td>\n",
       "      <td>3.670702</td>\n",
       "      <td>3.676991</td>\n",
       "      <td>2.590239</td>\n",
       "      <td>3.685219</td>\n",
       "      <td>2.879247</td>\n",
       "      <td>3.681201</td>\n",
       "      <td>3.949020</td>\n",
       "      <td>2.590387</td>\n",
       "      <td>3.953846</td>\n",
       "      <td>3.682269</td>\n",
       "      <td>3.681686</td>\n",
       "      <td>2.590519</td>\n",
       "      <td>2.590540</td>\n",
       "      <td>3.971859</td>\n",
       "      <td>2.590523</td>\n",
       "      <td>-1.540117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.601161</td>\n",
       "      <td>1.122772</td>\n",
       "      <td>2.557003</td>\n",
       "      <td>2.079653</td>\n",
       "      <td>-0.792563</td>\n",
       "      <td>3.184293</td>\n",
       "      <td>0.169903</td>\n",
       "      <td>2.380626</td>\n",
       "      <td>0.968678</td>\n",
       "      <td>-0.898042</td>\n",
       "      <td>0.546909</td>\n",
       "      <td>0.542502</td>\n",
       "      <td>0.829559</td>\n",
       "      <td>-1.363513</td>\n",
       "      <td>-0.469419</td>\n",
       "      <td>2.107699</td>\n",
       "      <td>2.530198</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>-1.355026</td>\n",
       "      <td>2.112698</td>\n",
       "      <td>-0.433188</td>\n",
       "      <td>-0.438182</td>\n",
       "      <td>-0.782676</td>\n",
       "      <td>2.091026</td>\n",
       "      <td>0.925432</td>\n",
       "      <td>0.645674</td>\n",
       "      <td>2.091058</td>\n",
       "      <td>-1.850081</td>\n",
       "      <td>2.092565</td>\n",
       "      <td>2.092535</td>\n",
       "      <td>2.652600</td>\n",
       "      <td>2.096564</td>\n",
       "      <td>2.618092</td>\n",
       "      <td>2.095867</td>\n",
       "      <td>2.443792</td>\n",
       "      <td>2.998522</td>\n",
       "      <td>2.604873</td>\n",
       "      <td>2.997269</td>\n",
       "      <td>2.602504</td>\n",
       "      <td>2.608884</td>\n",
       "      <td>2.096211</td>\n",
       "      <td>2.615890</td>\n",
       "      <td>2.270373</td>\n",
       "      <td>2.613631</td>\n",
       "      <td>2.663541</td>\n",
       "      <td>2.096384</td>\n",
       "      <td>2.693932</td>\n",
       "      <td>2.611509</td>\n",
       "      <td>2.613064</td>\n",
       "      <td>2.096538</td>\n",
       "      <td>2.096525</td>\n",
       "      <td>2.682193</td>\n",
       "      <td>2.096533</td>\n",
       "      <td>-1.601161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109702</th>\n",
       "      <td>-1.811006</td>\n",
       "      <td>-2.112556</td>\n",
       "      <td>-0.131305</td>\n",
       "      <td>-1.445311</td>\n",
       "      <td>-1.197634</td>\n",
       "      <td>-0.818321</td>\n",
       "      <td>-1.049270</td>\n",
       "      <td>-1.122315</td>\n",
       "      <td>-0.508777</td>\n",
       "      <td>-0.357198</td>\n",
       "      <td>1.027011</td>\n",
       "      <td>-0.311548</td>\n",
       "      <td>0.721451</td>\n",
       "      <td>-2.989994</td>\n",
       "      <td>0.889711</td>\n",
       "      <td>-1.483919</td>\n",
       "      <td>-1.102665</td>\n",
       "      <td>-0.136718</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>-1.485440</td>\n",
       "      <td>0.397786</td>\n",
       "      <td>-2.070832</td>\n",
       "      <td>0.489914</td>\n",
       "      <td>-1.491222</td>\n",
       "      <td>-4.525414</td>\n",
       "      <td>-2.391811</td>\n",
       "      <td>-1.488305</td>\n",
       "      <td>0.216442</td>\n",
       "      <td>-1.483623</td>\n",
       "      <td>-1.483645</td>\n",
       "      <td>-1.030355</td>\n",
       "      <td>-1.481442</td>\n",
       "      <td>-1.076202</td>\n",
       "      <td>-1.483214</td>\n",
       "      <td>-1.062814</td>\n",
       "      <td>-1.047183</td>\n",
       "      <td>-1.082408</td>\n",
       "      <td>-1.057290</td>\n",
       "      <td>-1.070795</td>\n",
       "      <td>-1.077186</td>\n",
       "      <td>-1.483106</td>\n",
       "      <td>-1.077155</td>\n",
       "      <td>-1.348169</td>\n",
       "      <td>-1.074557</td>\n",
       "      <td>-1.005825</td>\n",
       "      <td>-1.483853</td>\n",
       "      <td>-0.998264</td>\n",
       "      <td>-1.075246</td>\n",
       "      <td>-1.074719</td>\n",
       "      <td>-1.483446</td>\n",
       "      <td>-1.483451</td>\n",
       "      <td>-1.000564</td>\n",
       "      <td>-1.483459</td>\n",
       "      <td>-1.811006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109703</th>\n",
       "      <td>0.753172</td>\n",
       "      <td>-1.087454</td>\n",
       "      <td>-0.172401</td>\n",
       "      <td>-0.671626</td>\n",
       "      <td>0.416608</td>\n",
       "      <td>-0.605980</td>\n",
       "      <td>-0.603986</td>\n",
       "      <td>-0.787102</td>\n",
       "      <td>1.405494</td>\n",
       "      <td>-0.960323</td>\n",
       "      <td>0.122744</td>\n",
       "      <td>1.482535</td>\n",
       "      <td>0.217549</td>\n",
       "      <td>-1.818208</td>\n",
       "      <td>-0.006719</td>\n",
       "      <td>-0.725304</td>\n",
       "      <td>-0.736942</td>\n",
       "      <td>1.349400</td>\n",
       "      <td>0.214743</td>\n",
       "      <td>-0.727687</td>\n",
       "      <td>-1.691140</td>\n",
       "      <td>0.832361</td>\n",
       "      <td>-1.892583</td>\n",
       "      <td>-0.731506</td>\n",
       "      <td>-3.272527</td>\n",
       "      <td>-1.266335</td>\n",
       "      <td>-0.731930</td>\n",
       "      <td>0.941092</td>\n",
       "      <td>-0.720566</td>\n",
       "      <td>-0.720532</td>\n",
       "      <td>-0.645284</td>\n",
       "      <td>-0.724744</td>\n",
       "      <td>-0.706717</td>\n",
       "      <td>-0.723241</td>\n",
       "      <td>-0.704043</td>\n",
       "      <td>-0.710849</td>\n",
       "      <td>-0.699841</td>\n",
       "      <td>-0.697351</td>\n",
       "      <td>-0.719490</td>\n",
       "      <td>-0.712830</td>\n",
       "      <td>-0.725970</td>\n",
       "      <td>-0.712226</td>\n",
       "      <td>-0.739703</td>\n",
       "      <td>-0.709396</td>\n",
       "      <td>-0.687081</td>\n",
       "      <td>-0.725374</td>\n",
       "      <td>-0.687902</td>\n",
       "      <td>-0.708704</td>\n",
       "      <td>-0.709225</td>\n",
       "      <td>-0.725369</td>\n",
       "      <td>-0.725369</td>\n",
       "      <td>-0.684054</td>\n",
       "      <td>-0.725367</td>\n",
       "      <td>0.753172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109704</th>\n",
       "      <td>-1.688806</td>\n",
       "      <td>-2.147295</td>\n",
       "      <td>0.017075</td>\n",
       "      <td>-1.469679</td>\n",
       "      <td>-0.961748</td>\n",
       "      <td>-0.824514</td>\n",
       "      <td>-1.071355</td>\n",
       "      <td>-1.145723</td>\n",
       "      <td>-0.484022</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1.021900</td>\n",
       "      <td>-0.304297</td>\n",
       "      <td>0.720133</td>\n",
       "      <td>-3.081163</td>\n",
       "      <td>0.888471</td>\n",
       "      <td>-1.507813</td>\n",
       "      <td>-1.113767</td>\n",
       "      <td>-0.130509</td>\n",
       "      <td>0.867612</td>\n",
       "      <td>-1.509366</td>\n",
       "      <td>0.424591</td>\n",
       "      <td>-1.942305</td>\n",
       "      <td>0.494363</td>\n",
       "      <td>-1.515185</td>\n",
       "      <td>-4.630214</td>\n",
       "      <td>-2.439289</td>\n",
       "      <td>-1.515038</td>\n",
       "      <td>0.216268</td>\n",
       "      <td>-1.507740</td>\n",
       "      <td>-1.507753</td>\n",
       "      <td>-1.038492</td>\n",
       "      <td>-1.505378</td>\n",
       "      <td>-1.085195</td>\n",
       "      <td>-1.507161</td>\n",
       "      <td>-1.071938</td>\n",
       "      <td>-1.056292</td>\n",
       "      <td>-1.091933</td>\n",
       "      <td>-1.066310</td>\n",
       "      <td>-1.080554</td>\n",
       "      <td>-1.086699</td>\n",
       "      <td>-1.507026</td>\n",
       "      <td>-1.087777</td>\n",
       "      <td>-1.365862</td>\n",
       "      <td>-1.084041</td>\n",
       "      <td>-1.013990</td>\n",
       "      <td>-1.507778</td>\n",
       "      <td>-1.005437</td>\n",
       "      <td>-1.084875</td>\n",
       "      <td>-1.084221</td>\n",
       "      <td>-1.507369</td>\n",
       "      <td>-1.507383</td>\n",
       "      <td>-1.008664</td>\n",
       "      <td>-1.507384</td>\n",
       "      <td>-1.688806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109705</th>\n",
       "      <td>0.760898</td>\n",
       "      <td>-1.150622</td>\n",
       "      <td>-0.441701</td>\n",
       "      <td>-0.690957</td>\n",
       "      <td>0.747083</td>\n",
       "      <td>-0.613353</td>\n",
       "      <td>-0.699474</td>\n",
       "      <td>-0.734400</td>\n",
       "      <td>1.405871</td>\n",
       "      <td>-0.783727</td>\n",
       "      <td>0.132903</td>\n",
       "      <td>1.494039</td>\n",
       "      <td>0.229551</td>\n",
       "      <td>-1.906146</td>\n",
       "      <td>0.028314</td>\n",
       "      <td>-0.745036</td>\n",
       "      <td>-0.748550</td>\n",
       "      <td>1.345973</td>\n",
       "      <td>0.203279</td>\n",
       "      <td>-0.747514</td>\n",
       "      <td>-1.581334</td>\n",
       "      <td>0.847799</td>\n",
       "      <td>-1.795700</td>\n",
       "      <td>-0.744393</td>\n",
       "      <td>-3.384335</td>\n",
       "      <td>-1.321487</td>\n",
       "      <td>-0.747615</td>\n",
       "      <td>0.933312</td>\n",
       "      <td>-0.740309</td>\n",
       "      <td>-0.740269</td>\n",
       "      <td>-0.655384</td>\n",
       "      <td>-0.744566</td>\n",
       "      <td>-0.718997</td>\n",
       "      <td>-0.743016</td>\n",
       "      <td>-0.714902</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>-0.711319</td>\n",
       "      <td>-0.708349</td>\n",
       "      <td>-0.730993</td>\n",
       "      <td>-0.724333</td>\n",
       "      <td>-0.745783</td>\n",
       "      <td>-0.723759</td>\n",
       "      <td>-0.756948</td>\n",
       "      <td>-0.720949</td>\n",
       "      <td>-0.697247</td>\n",
       "      <td>-0.745189</td>\n",
       "      <td>-0.699182</td>\n",
       "      <td>-0.720210</td>\n",
       "      <td>-0.720815</td>\n",
       "      <td>-0.745181</td>\n",
       "      <td>-0.745175</td>\n",
       "      <td>-0.694137</td>\n",
       "      <td>-0.745181</td>\n",
       "      <td>0.760898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109706</th>\n",
       "      <td>-1.826040</td>\n",
       "      <td>-1.946252</td>\n",
       "      <td>-0.140323</td>\n",
       "      <td>-1.490113</td>\n",
       "      <td>-1.226654</td>\n",
       "      <td>-0.830248</td>\n",
       "      <td>-0.825180</td>\n",
       "      <td>-1.111304</td>\n",
       "      <td>-0.421942</td>\n",
       "      <td>0.311960</td>\n",
       "      <td>1.003101</td>\n",
       "      <td>-0.265331</td>\n",
       "      <td>0.700887</td>\n",
       "      <td>-3.043859</td>\n",
       "      <td>0.871073</td>\n",
       "      <td>-1.528086</td>\n",
       "      <td>-1.118872</td>\n",
       "      <td>-0.091912</td>\n",
       "      <td>0.865527</td>\n",
       "      <td>-1.529691</td>\n",
       "      <td>0.449575</td>\n",
       "      <td>-2.104880</td>\n",
       "      <td>0.521008</td>\n",
       "      <td>-1.530417</td>\n",
       "      <td>-4.937934</td>\n",
       "      <td>-2.202229</td>\n",
       "      <td>-1.529687</td>\n",
       "      <td>0.236812</td>\n",
       "      <td>-1.528866</td>\n",
       "      <td>-1.528966</td>\n",
       "      <td>-1.043437</td>\n",
       "      <td>-1.525830</td>\n",
       "      <td>-1.093863</td>\n",
       "      <td>-1.527443</td>\n",
       "      <td>-1.080235</td>\n",
       "      <td>-1.063649</td>\n",
       "      <td>-1.099317</td>\n",
       "      <td>-1.074473</td>\n",
       "      <td>-1.088307</td>\n",
       "      <td>-1.094444</td>\n",
       "      <td>-1.527371</td>\n",
       "      <td>-1.093381</td>\n",
       "      <td>-1.381861</td>\n",
       "      <td>-1.092859</td>\n",
       "      <td>-1.020866</td>\n",
       "      <td>-1.528064</td>\n",
       "      <td>-1.014309</td>\n",
       "      <td>-1.091924</td>\n",
       "      <td>-1.092571</td>\n",
       "      <td>-1.527824</td>\n",
       "      <td>-1.527832</td>\n",
       "      <td>-1.016333</td>\n",
       "      <td>-1.527801</td>\n",
       "      <td>-1.826040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109707 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X39       X53       X45       X13       X36       X15       X51  \\\n",
       "0      -1.582156  1.230171  2.175619  2.246967 -0.755876  3.583055  0.098955   \n",
       "1      -1.584404  1.605045 -2.204271  2.227963 -0.760217  3.535377  0.300338   \n",
       "2      -1.594548  1.172960  2.547328  2.138920 -0.779797  3.321969 -0.241806   \n",
       "3      -1.540117  1.471016 -1.464351  2.589700 -0.674728  4.499712 -0.070470   \n",
       "4      -1.601161  1.122772  2.557003  2.079653 -0.792563  3.184293  0.169903   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702 -1.811006 -2.112556 -0.131305 -1.445311 -1.197634 -0.818321 -1.049270   \n",
       "109703  0.753172 -1.087454 -0.172401 -0.671626  0.416608 -0.605980 -0.603986   \n",
       "109704 -1.688806 -2.147295  0.017075 -1.469679 -0.961748 -0.824514 -1.071355   \n",
       "109705  0.760898 -1.150622 -0.441701 -0.690957  0.747083 -0.613353 -0.699474   \n",
       "109706 -1.826040 -1.946252 -0.140323 -1.490113 -1.226654 -0.830248 -0.825180   \n",
       "\n",
       "             X49       X33       X40       X31       X34       X32       X50  \\\n",
       "0       3.040176  0.919099 -0.889421  0.484578  0.481170  0.777112 -1.798247   \n",
       "1       2.880602  0.943713 -0.890453  0.513255  0.509388  0.803151 -1.042563   \n",
       "2       2.693446  0.934271 -0.895068  0.505998  0.502247  0.793162 -1.196648   \n",
       "3       3.630669 -0.163725 -0.869522  0.771819  0.763805  1.057894 -1.013944   \n",
       "4       2.380626  0.968678 -0.898042  0.546909  0.542502  0.829559 -1.363513   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702 -1.122315 -0.508777 -0.357198  1.027011 -0.311548  0.721451 -2.989994   \n",
       "109703 -0.787102  1.405494 -0.960323  0.122744  1.482535  0.217549 -1.818208   \n",
       "109704 -1.145723 -0.484022  0.003704  1.021900 -0.304297  0.720133 -3.081163   \n",
       "109705 -0.734400  1.405871 -0.783727  0.132903  1.494039  0.229551 -1.906146   \n",
       "109706 -1.111304 -0.421942  0.311960  1.003101 -0.265331  0.700887 -3.043859   \n",
       "\n",
       "             X30       X18       X46       X35       X37       X17       X42  \\\n",
       "0       0.873301  2.270459  2.912388  0.728549 -1.295910  2.275724 -0.359744   \n",
       "1       0.900408  2.251683  2.831043  0.757081 -1.302729  2.256918 -0.368216   \n",
       "2       0.893682  2.165409  2.656538  0.750002 -1.334065  2.170502 -0.407147   \n",
       "3       1.144550  2.601021  3.586176  1.014063 -1.175844  2.588606 -0.210578   \n",
       "4      -0.469419  2.107699  2.530198  0.790754 -1.355026  2.112698 -0.433188   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702  0.889711 -1.483919 -1.102665 -0.136718  0.872560 -1.485440  0.397786   \n",
       "109703 -0.006719 -0.725304 -0.736942  1.349400  0.214743 -0.727687 -1.691140   \n",
       "109704  0.888471 -1.507813 -1.113767 -0.130509  0.867612 -1.509366  0.424591   \n",
       "109705  0.028314 -0.745036 -0.748550  1.345973  0.203279 -0.747514 -1.581334   \n",
       "109706  0.871073 -1.528086 -1.118872 -0.091912  0.865527 -1.529691  0.449575   \n",
       "\n",
       "             X38       X43       X47        X0       X52       X48       X41  \\\n",
       "0      -0.363988 -0.717875  2.265473  1.019072  0.721157  2.260484 -1.834228   \n",
       "1      -0.372547 -0.725350  2.242051  1.084322  1.273463  2.239054 -1.836103   \n",
       "2      -0.411875 -0.759699  2.150257  0.909735  0.858274  2.152557 -1.844565   \n",
       "3      -0.213300 -0.586264  2.586420  1.452937  1.030784  2.588699 -1.799160   \n",
       "4      -0.438182 -0.782676  2.091026  0.925432  0.645674  2.091058 -1.850081   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702 -2.070832  0.489914 -1.491222 -4.525414 -2.391811 -1.488305  0.216442   \n",
       "109703  0.832361 -1.892583 -0.731506 -3.272527 -1.266335 -0.731930  0.941092   \n",
       "109704 -1.942305  0.494363 -1.515185 -4.630214 -2.439289 -1.515038  0.216268   \n",
       "109705  0.847799 -1.795700 -0.744393 -3.384335 -1.321487 -0.747615  0.933312   \n",
       "109706 -2.104880  0.521008 -1.530417 -4.937934 -2.202229 -1.529687  0.236812   \n",
       "\n",
       "              X2        X3       X25       X23        X9       X27       X14  \\\n",
       "0       2.254474  2.254450  3.042155  2.259637  2.946272  2.258885  3.401480   \n",
       "1       2.235786  2.235849  2.995526  2.240825  2.901227  2.240080  3.359956   \n",
       "2       2.150014  2.149977  2.786986  2.154385  2.734335  2.153668  3.172415   \n",
       "3       2.583419  2.583483  3.941371  2.590836  3.676733  2.589976  4.174776   \n",
       "4       2.092565  2.092535  2.652600  2.096564  2.618092  2.095867  2.443792   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702 -1.483623 -1.483645 -1.030355 -1.481442 -1.076202 -1.483214 -1.062814   \n",
       "109703 -0.720566 -0.720532 -0.645284 -0.724744 -0.706717 -0.723241 -0.704043   \n",
       "109704 -1.507740 -1.507753 -1.038492 -1.505378 -1.085195 -1.507161 -1.071938   \n",
       "109705 -0.740309 -0.740269 -0.655384 -0.744566 -0.718997 -0.743016 -0.714902   \n",
       "109706 -1.528866 -1.528966 -1.043437 -1.525830 -1.093863 -1.527443 -1.080235   \n",
       "\n",
       "             X19       X24       X16       X29       X26       X28       X21  \\\n",
       "0       3.345012  2.937407  3.343820  2.935077  2.941428  2.259203  2.938547   \n",
       "1       3.304083  2.898066  3.302884  2.895731  2.902085  2.240400  2.910457   \n",
       "2       3.119236  2.720591  3.118004  2.718235  2.724605  2.154003  2.731983   \n",
       "3       4.107204  3.672950  4.106144  3.670702  3.676991  2.590239  3.685219   \n",
       "4       2.998522  2.604873  2.997269  2.602504  2.608884  2.096211  2.615890   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702 -1.047183 -1.082408 -1.057290 -1.070795 -1.077186 -1.483106 -1.077155   \n",
       "109703 -0.710849 -0.699841 -0.697351 -0.719490 -0.712830 -0.725970 -0.712226   \n",
       "109704 -1.056292 -1.091933 -1.066310 -1.080554 -1.086699 -1.507026 -1.087777   \n",
       "109705 -0.721721 -0.711319 -0.708349 -0.730993 -0.724333 -0.745783 -0.723759   \n",
       "109706 -1.063649 -1.099317 -1.074473 -1.088307 -1.094444 -1.527371 -1.093381   \n",
       "\n",
       "              X1       X11       X20       X22        X5        X4        X6  \\\n",
       "0       2.469231  2.945955  3.044228  2.259368  3.073369  2.943574  2.945749   \n",
       "1       2.446440  2.903884  3.007216  2.240566  3.004351  2.907020  2.906517   \n",
       "2       2.340769  2.729945  2.798645  2.154173  2.832534  2.727687  2.728807   \n",
       "3       2.879247  3.681201  3.949020  2.590387  3.953846  3.682269  3.681686   \n",
       "4       2.270373  2.613631  2.663541  2.096384  2.693932  2.611509  2.613064   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "109702 -1.348169 -1.074557 -1.005825 -1.483853 -0.998264 -1.075246 -1.074719   \n",
       "109703 -0.739703 -0.709396 -0.687081 -0.725374 -0.687902 -0.708704 -0.709225   \n",
       "109704 -1.365862 -1.084041 -1.013990 -1.507778 -1.005437 -1.084875 -1.084221   \n",
       "109705 -0.756948 -0.720949 -0.697247 -0.745189 -0.699182 -0.720210 -0.720815   \n",
       "109706 -1.381861 -1.092859 -1.020866 -1.528064 -1.014309 -1.091924 -1.092571   \n",
       "\n",
       "             X12        X7       X10        X8       X62  \n",
       "0       2.259507  2.259501  3.071520  2.259504 -1.582156  \n",
       "1       2.240683  2.240706  3.025257  2.240732 -1.584404  \n",
       "2       2.154322  2.154294  2.816114  2.154304 -1.594548  \n",
       "3       2.590519  2.590540  3.971859  2.590523 -1.540117  \n",
       "4       2.096538  2.096525  2.682193  2.096533 -1.601161  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "109702 -1.483446 -1.483451 -1.000564 -1.483459 -1.811006  \n",
       "109703 -0.725369 -0.725369 -0.684054 -0.725367  0.753172  \n",
       "109704 -1.507369 -1.507383 -1.008664 -1.507384 -1.688806  \n",
       "109705 -0.745181 -0.745175 -0.694137 -0.745181  0.760898  \n",
       "109706 -1.527824 -1.527832 -1.016333 -1.527801 -1.826040  \n",
       "\n",
       "[109707 rows x 54 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_seurat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1f1207-e8b6-4503-8c77-d2edcd5e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seurat = train_test_seurat.iloc[:90000, :]\n",
    "test_seurat = train_test_seurat.iloc[90000:, :]\n",
    "\n",
    "test_seurat.reset_index(inplace=True) \n",
    "test_seurat.drop(\"index\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b5f81e6-1e1c-4e7a-b3b3-9adfaa705711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./MLR_Project_train.csv')\n",
    "test = pd.read_csv('./MLR_Project_test.csv')\n",
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b3d485-48d1-4865-bcf5-183f4d9cace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seurat['TARGET'] = train['TARGET']\n",
    "test_seurat['TARGET'] = test['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7977ae18-a45c-4536-b316-cae25b47c068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X39</th>\n",
       "      <th>X53</th>\n",
       "      <th>X45</th>\n",
       "      <th>X13</th>\n",
       "      <th>X36</th>\n",
       "      <th>X15</th>\n",
       "      <th>X51</th>\n",
       "      <th>X49</th>\n",
       "      <th>X33</th>\n",
       "      <th>X40</th>\n",
       "      <th>X31</th>\n",
       "      <th>X34</th>\n",
       "      <th>X32</th>\n",
       "      <th>X50</th>\n",
       "      <th>X30</th>\n",
       "      <th>X18</th>\n",
       "      <th>X46</th>\n",
       "      <th>X35</th>\n",
       "      <th>X37</th>\n",
       "      <th>X17</th>\n",
       "      <th>X42</th>\n",
       "      <th>X38</th>\n",
       "      <th>X43</th>\n",
       "      <th>X47</th>\n",
       "      <th>X0</th>\n",
       "      <th>X52</th>\n",
       "      <th>X48</th>\n",
       "      <th>X41</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X25</th>\n",
       "      <th>X23</th>\n",
       "      <th>X9</th>\n",
       "      <th>X27</th>\n",
       "      <th>X14</th>\n",
       "      <th>X19</th>\n",
       "      <th>X24</th>\n",
       "      <th>X16</th>\n",
       "      <th>X29</th>\n",
       "      <th>X26</th>\n",
       "      <th>X28</th>\n",
       "      <th>X21</th>\n",
       "      <th>X1</th>\n",
       "      <th>X11</th>\n",
       "      <th>X20</th>\n",
       "      <th>X22</th>\n",
       "      <th>X5</th>\n",
       "      <th>X4</th>\n",
       "      <th>X6</th>\n",
       "      <th>X12</th>\n",
       "      <th>X7</th>\n",
       "      <th>X10</th>\n",
       "      <th>X8</th>\n",
       "      <th>X62</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.303619</td>\n",
       "      <td>-1.368098</td>\n",
       "      <td>0.318033</td>\n",
       "      <td>-0.664706</td>\n",
       "      <td>0.031642</td>\n",
       "      <td>-0.637868</td>\n",
       "      <td>-0.901035</td>\n",
       "      <td>-0.789375</td>\n",
       "      <td>-0.896748</td>\n",
       "      <td>-0.626164</td>\n",
       "      <td>-0.752216</td>\n",
       "      <td>-1.400652</td>\n",
       "      <td>-0.490283</td>\n",
       "      <td>-2.085998</td>\n",
       "      <td>-0.213036</td>\n",
       "      <td>-0.730901</td>\n",
       "      <td>-0.721203</td>\n",
       "      <td>-1.097359</td>\n",
       "      <td>1.065540</td>\n",
       "      <td>-0.730536</td>\n",
       "      <td>0.097683</td>\n",
       "      <td>0.585017</td>\n",
       "      <td>-0.309425</td>\n",
       "      <td>-0.733860</td>\n",
       "      <td>0.162955</td>\n",
       "      <td>-1.752634</td>\n",
       "      <td>-0.736883</td>\n",
       "      <td>0.265702</td>\n",
       "      <td>-0.732184</td>\n",
       "      <td>-0.732188</td>\n",
       "      <td>-0.691366</td>\n",
       "      <td>-0.726183</td>\n",
       "      <td>-0.704597</td>\n",
       "      <td>-0.728807</td>\n",
       "      <td>-0.710549</td>\n",
       "      <td>-0.708777</td>\n",
       "      <td>-0.706969</td>\n",
       "      <td>-0.709911</td>\n",
       "      <td>-0.709945</td>\n",
       "      <td>-0.706522</td>\n",
       "      <td>-0.728111</td>\n",
       "      <td>-0.709008</td>\n",
       "      <td>-0.754861</td>\n",
       "      <td>-0.704447</td>\n",
       "      <td>-0.684198</td>\n",
       "      <td>-0.728199</td>\n",
       "      <td>-0.680463</td>\n",
       "      <td>-0.704870</td>\n",
       "      <td>-0.704561</td>\n",
       "      <td>-0.728003</td>\n",
       "      <td>-0.728004</td>\n",
       "      <td>-0.679034</td>\n",
       "      <td>-0.728009</td>\n",
       "      <td>0.303619</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.270544</td>\n",
       "      <td>-0.293297</td>\n",
       "      <td>-0.800283</td>\n",
       "      <td>0.205806</td>\n",
       "      <td>-0.817904</td>\n",
       "      <td>-0.267324</td>\n",
       "      <td>-0.575346</td>\n",
       "      <td>-0.088519</td>\n",
       "      <td>0.252945</td>\n",
       "      <td>0.516988</td>\n",
       "      <td>-0.888595</td>\n",
       "      <td>-0.183459</td>\n",
       "      <td>-0.612266</td>\n",
       "      <td>-1.281293</td>\n",
       "      <td>-0.365916</td>\n",
       "      <td>0.108370</td>\n",
       "      <td>-0.104701</td>\n",
       "      <td>0.079553</td>\n",
       "      <td>0.531337</td>\n",
       "      <td>0.111701</td>\n",
       "      <td>1.092659</td>\n",
       "      <td>0.558323</td>\n",
       "      <td>0.561124</td>\n",
       "      <td>0.114858</td>\n",
       "      <td>0.637649</td>\n",
       "      <td>-0.454719</td>\n",
       "      <td>0.113503</td>\n",
       "      <td>1.172467</td>\n",
       "      <td>0.110364</td>\n",
       "      <td>0.110365</td>\n",
       "      <td>-0.143167</td>\n",
       "      <td>0.113187</td>\n",
       "      <td>-0.092878</td>\n",
       "      <td>0.115930</td>\n",
       "      <td>-0.129771</td>\n",
       "      <td>-0.128735</td>\n",
       "      <td>-0.094559</td>\n",
       "      <td>-0.137471</td>\n",
       "      <td>-0.090028</td>\n",
       "      <td>-0.090889</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>-0.093737</td>\n",
       "      <td>0.036367</td>\n",
       "      <td>-0.092026</td>\n",
       "      <td>-0.128428</td>\n",
       "      <td>0.113681</td>\n",
       "      <td>-0.124515</td>\n",
       "      <td>-0.092511</td>\n",
       "      <td>-0.091235</td>\n",
       "      <td>0.113804</td>\n",
       "      <td>0.113818</td>\n",
       "      <td>-0.122337</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.270544</td>\n",
       "      <td>-0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408199</td>\n",
       "      <td>-1.086199</td>\n",
       "      <td>0.476146</td>\n",
       "      <td>-0.632996</td>\n",
       "      <td>0.711765</td>\n",
       "      <td>-0.619817</td>\n",
       "      <td>-0.865759</td>\n",
       "      <td>-0.691751</td>\n",
       "      <td>-0.650123</td>\n",
       "      <td>-0.491680</td>\n",
       "      <td>-0.751901</td>\n",
       "      <td>-1.105154</td>\n",
       "      <td>-0.523756</td>\n",
       "      <td>-1.759521</td>\n",
       "      <td>-1.664908</td>\n",
       "      <td>-0.695644</td>\n",
       "      <td>-0.693250</td>\n",
       "      <td>-0.846007</td>\n",
       "      <td>1.020084</td>\n",
       "      <td>-0.695285</td>\n",
       "      <td>0.317099</td>\n",
       "      <td>0.571099</td>\n",
       "      <td>-0.093730</td>\n",
       "      <td>-0.691992</td>\n",
       "      <td>-0.162791</td>\n",
       "      <td>-1.178735</td>\n",
       "      <td>-0.693527</td>\n",
       "      <td>0.322890</td>\n",
       "      <td>-0.696694</td>\n",
       "      <td>-0.696709</td>\n",
       "      <td>-0.664141</td>\n",
       "      <td>-0.691275</td>\n",
       "      <td>-0.680123</td>\n",
       "      <td>-0.693348</td>\n",
       "      <td>-0.691434</td>\n",
       "      <td>-0.689171</td>\n",
       "      <td>-0.681219</td>\n",
       "      <td>-0.686197</td>\n",
       "      <td>-0.688039</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.692878</td>\n",
       "      <td>-0.683230</td>\n",
       "      <td>-0.722643</td>\n",
       "      <td>-0.680831</td>\n",
       "      <td>-0.662410</td>\n",
       "      <td>-0.692527</td>\n",
       "      <td>-0.657241</td>\n",
       "      <td>-0.680773</td>\n",
       "      <td>-0.681192</td>\n",
       "      <td>-0.692505</td>\n",
       "      <td>-0.692521</td>\n",
       "      <td>-0.657914</td>\n",
       "      <td>-0.692516</td>\n",
       "      <td>0.408199</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.320914</td>\n",
       "      <td>-0.331991</td>\n",
       "      <td>-0.839812</td>\n",
       "      <td>0.407444</td>\n",
       "      <td>0.304629</td>\n",
       "      <td>-0.155761</td>\n",
       "      <td>-0.566030</td>\n",
       "      <td>0.025930</td>\n",
       "      <td>-1.156736</td>\n",
       "      <td>1.424197</td>\n",
       "      <td>-0.719807</td>\n",
       "      <td>-0.181216</td>\n",
       "      <td>-0.422551</td>\n",
       "      <td>-0.963558</td>\n",
       "      <td>-0.236426</td>\n",
       "      <td>0.301783</td>\n",
       "      <td>0.072858</td>\n",
       "      <td>0.096051</td>\n",
       "      <td>0.556588</td>\n",
       "      <td>0.303111</td>\n",
       "      <td>1.065361</td>\n",
       "      <td>0.716041</td>\n",
       "      <td>0.727413</td>\n",
       "      <td>0.300229</td>\n",
       "      <td>0.981367</td>\n",
       "      <td>-0.507419</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>1.175421</td>\n",
       "      <td>0.300083</td>\n",
       "      <td>0.300104</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>0.305106</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>0.308407</td>\n",
       "      <td>0.038148</td>\n",
       "      <td>0.039061</td>\n",
       "      <td>0.082331</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>0.089581</td>\n",
       "      <td>0.086617</td>\n",
       "      <td>0.305908</td>\n",
       "      <td>0.083850</td>\n",
       "      <td>0.227740</td>\n",
       "      <td>0.085438</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.305550</td>\n",
       "      <td>0.042203</td>\n",
       "      <td>0.085026</td>\n",
       "      <td>0.085652</td>\n",
       "      <td>0.305663</td>\n",
       "      <td>0.305680</td>\n",
       "      <td>0.043227</td>\n",
       "      <td>0.305671</td>\n",
       "      <td>0.320914</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.399412</td>\n",
       "      <td>-0.842829</td>\n",
       "      <td>0.232809</td>\n",
       "      <td>-0.624319</td>\n",
       "      <td>1.072002</td>\n",
       "      <td>-0.617945</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>-0.669520</td>\n",
       "      <td>-0.532747</td>\n",
       "      <td>-0.602045</td>\n",
       "      <td>-0.789767</td>\n",
       "      <td>-0.971547</td>\n",
       "      <td>-0.547541</td>\n",
       "      <td>-1.815948</td>\n",
       "      <td>-1.508678</td>\n",
       "      <td>-0.686499</td>\n",
       "      <td>-0.687388</td>\n",
       "      <td>-0.725842</td>\n",
       "      <td>1.004514</td>\n",
       "      <td>-0.686138</td>\n",
       "      <td>0.421749</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>-0.680960</td>\n",
       "      <td>-0.271713</td>\n",
       "      <td>-1.155744</td>\n",
       "      <td>-0.683218</td>\n",
       "      <td>0.399896</td>\n",
       "      <td>-0.688086</td>\n",
       "      <td>-0.688104</td>\n",
       "      <td>-0.661634</td>\n",
       "      <td>-0.682309</td>\n",
       "      <td>-0.675430</td>\n",
       "      <td>-0.684194</td>\n",
       "      <td>-0.686846</td>\n",
       "      <td>-0.684348</td>\n",
       "      <td>-0.676160</td>\n",
       "      <td>-0.681855</td>\n",
       "      <td>-0.682763</td>\n",
       "      <td>-0.678181</td>\n",
       "      <td>-0.683735</td>\n",
       "      <td>-0.678193</td>\n",
       "      <td>-0.715838</td>\n",
       "      <td>-0.676215</td>\n",
       "      <td>-0.658180</td>\n",
       "      <td>-0.683386</td>\n",
       "      <td>-0.653732</td>\n",
       "      <td>-0.675658</td>\n",
       "      <td>-0.676009</td>\n",
       "      <td>-0.683393</td>\n",
       "      <td>-0.683402</td>\n",
       "      <td>-0.653591</td>\n",
       "      <td>-0.683384</td>\n",
       "      <td>0.399412</td>\n",
       "      <td>-0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19702</th>\n",
       "      <td>-1.811006</td>\n",
       "      <td>-2.112556</td>\n",
       "      <td>-0.131305</td>\n",
       "      <td>-1.445311</td>\n",
       "      <td>-1.197634</td>\n",
       "      <td>-0.818321</td>\n",
       "      <td>-1.049270</td>\n",
       "      <td>-1.122315</td>\n",
       "      <td>-0.508777</td>\n",
       "      <td>-0.357198</td>\n",
       "      <td>1.027011</td>\n",
       "      <td>-0.311548</td>\n",
       "      <td>0.721451</td>\n",
       "      <td>-2.989994</td>\n",
       "      <td>0.889711</td>\n",
       "      <td>-1.483919</td>\n",
       "      <td>-1.102665</td>\n",
       "      <td>-0.136718</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>-1.485440</td>\n",
       "      <td>0.397786</td>\n",
       "      <td>-2.070832</td>\n",
       "      <td>0.489914</td>\n",
       "      <td>-1.491222</td>\n",
       "      <td>-4.525414</td>\n",
       "      <td>-2.391811</td>\n",
       "      <td>-1.488305</td>\n",
       "      <td>0.216442</td>\n",
       "      <td>-1.483623</td>\n",
       "      <td>-1.483645</td>\n",
       "      <td>-1.030355</td>\n",
       "      <td>-1.481442</td>\n",
       "      <td>-1.076202</td>\n",
       "      <td>-1.483214</td>\n",
       "      <td>-1.062814</td>\n",
       "      <td>-1.047183</td>\n",
       "      <td>-1.082408</td>\n",
       "      <td>-1.057290</td>\n",
       "      <td>-1.070795</td>\n",
       "      <td>-1.077186</td>\n",
       "      <td>-1.483106</td>\n",
       "      <td>-1.077155</td>\n",
       "      <td>-1.348169</td>\n",
       "      <td>-1.074557</td>\n",
       "      <td>-1.005825</td>\n",
       "      <td>-1.483853</td>\n",
       "      <td>-0.998264</td>\n",
       "      <td>-1.075246</td>\n",
       "      <td>-1.074719</td>\n",
       "      <td>-1.483446</td>\n",
       "      <td>-1.483451</td>\n",
       "      <td>-1.000564</td>\n",
       "      <td>-1.483459</td>\n",
       "      <td>-1.811006</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19703</th>\n",
       "      <td>0.753172</td>\n",
       "      <td>-1.087454</td>\n",
       "      <td>-0.172401</td>\n",
       "      <td>-0.671626</td>\n",
       "      <td>0.416608</td>\n",
       "      <td>-0.605980</td>\n",
       "      <td>-0.603986</td>\n",
       "      <td>-0.787102</td>\n",
       "      <td>1.405494</td>\n",
       "      <td>-0.960323</td>\n",
       "      <td>0.122744</td>\n",
       "      <td>1.482535</td>\n",
       "      <td>0.217549</td>\n",
       "      <td>-1.818208</td>\n",
       "      <td>-0.006719</td>\n",
       "      <td>-0.725304</td>\n",
       "      <td>-0.736942</td>\n",
       "      <td>1.349400</td>\n",
       "      <td>0.214743</td>\n",
       "      <td>-0.727687</td>\n",
       "      <td>-1.691140</td>\n",
       "      <td>0.832361</td>\n",
       "      <td>-1.892583</td>\n",
       "      <td>-0.731506</td>\n",
       "      <td>-3.272527</td>\n",
       "      <td>-1.266335</td>\n",
       "      <td>-0.731930</td>\n",
       "      <td>0.941092</td>\n",
       "      <td>-0.720566</td>\n",
       "      <td>-0.720532</td>\n",
       "      <td>-0.645284</td>\n",
       "      <td>-0.724744</td>\n",
       "      <td>-0.706717</td>\n",
       "      <td>-0.723241</td>\n",
       "      <td>-0.704043</td>\n",
       "      <td>-0.710849</td>\n",
       "      <td>-0.699841</td>\n",
       "      <td>-0.697351</td>\n",
       "      <td>-0.719490</td>\n",
       "      <td>-0.712830</td>\n",
       "      <td>-0.725970</td>\n",
       "      <td>-0.712226</td>\n",
       "      <td>-0.739703</td>\n",
       "      <td>-0.709396</td>\n",
       "      <td>-0.687081</td>\n",
       "      <td>-0.725374</td>\n",
       "      <td>-0.687902</td>\n",
       "      <td>-0.708704</td>\n",
       "      <td>-0.709225</td>\n",
       "      <td>-0.725369</td>\n",
       "      <td>-0.725369</td>\n",
       "      <td>-0.684054</td>\n",
       "      <td>-0.725367</td>\n",
       "      <td>0.753172</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19704</th>\n",
       "      <td>-1.688806</td>\n",
       "      <td>-2.147295</td>\n",
       "      <td>0.017075</td>\n",
       "      <td>-1.469679</td>\n",
       "      <td>-0.961748</td>\n",
       "      <td>-0.824514</td>\n",
       "      <td>-1.071355</td>\n",
       "      <td>-1.145723</td>\n",
       "      <td>-0.484022</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1.021900</td>\n",
       "      <td>-0.304297</td>\n",
       "      <td>0.720133</td>\n",
       "      <td>-3.081163</td>\n",
       "      <td>0.888471</td>\n",
       "      <td>-1.507813</td>\n",
       "      <td>-1.113767</td>\n",
       "      <td>-0.130509</td>\n",
       "      <td>0.867612</td>\n",
       "      <td>-1.509366</td>\n",
       "      <td>0.424591</td>\n",
       "      <td>-1.942305</td>\n",
       "      <td>0.494363</td>\n",
       "      <td>-1.515185</td>\n",
       "      <td>-4.630214</td>\n",
       "      <td>-2.439289</td>\n",
       "      <td>-1.515038</td>\n",
       "      <td>0.216268</td>\n",
       "      <td>-1.507740</td>\n",
       "      <td>-1.507753</td>\n",
       "      <td>-1.038492</td>\n",
       "      <td>-1.505378</td>\n",
       "      <td>-1.085195</td>\n",
       "      <td>-1.507161</td>\n",
       "      <td>-1.071938</td>\n",
       "      <td>-1.056292</td>\n",
       "      <td>-1.091933</td>\n",
       "      <td>-1.066310</td>\n",
       "      <td>-1.080554</td>\n",
       "      <td>-1.086699</td>\n",
       "      <td>-1.507026</td>\n",
       "      <td>-1.087777</td>\n",
       "      <td>-1.365862</td>\n",
       "      <td>-1.084041</td>\n",
       "      <td>-1.013990</td>\n",
       "      <td>-1.507778</td>\n",
       "      <td>-1.005437</td>\n",
       "      <td>-1.084875</td>\n",
       "      <td>-1.084221</td>\n",
       "      <td>-1.507369</td>\n",
       "      <td>-1.507383</td>\n",
       "      <td>-1.008664</td>\n",
       "      <td>-1.507384</td>\n",
       "      <td>-1.688806</td>\n",
       "      <td>0.000799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>0.760898</td>\n",
       "      <td>-1.150622</td>\n",
       "      <td>-0.441701</td>\n",
       "      <td>-0.690957</td>\n",
       "      <td>0.747083</td>\n",
       "      <td>-0.613353</td>\n",
       "      <td>-0.699474</td>\n",
       "      <td>-0.734400</td>\n",
       "      <td>1.405871</td>\n",
       "      <td>-0.783727</td>\n",
       "      <td>0.132903</td>\n",
       "      <td>1.494039</td>\n",
       "      <td>0.229551</td>\n",
       "      <td>-1.906146</td>\n",
       "      <td>0.028314</td>\n",
       "      <td>-0.745036</td>\n",
       "      <td>-0.748550</td>\n",
       "      <td>1.345973</td>\n",
       "      <td>0.203279</td>\n",
       "      <td>-0.747514</td>\n",
       "      <td>-1.581334</td>\n",
       "      <td>0.847799</td>\n",
       "      <td>-1.795700</td>\n",
       "      <td>-0.744393</td>\n",
       "      <td>-3.384335</td>\n",
       "      <td>-1.321487</td>\n",
       "      <td>-0.747615</td>\n",
       "      <td>0.933312</td>\n",
       "      <td>-0.740309</td>\n",
       "      <td>-0.740269</td>\n",
       "      <td>-0.655384</td>\n",
       "      <td>-0.744566</td>\n",
       "      <td>-0.718997</td>\n",
       "      <td>-0.743016</td>\n",
       "      <td>-0.714902</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>-0.711319</td>\n",
       "      <td>-0.708349</td>\n",
       "      <td>-0.730993</td>\n",
       "      <td>-0.724333</td>\n",
       "      <td>-0.745783</td>\n",
       "      <td>-0.723759</td>\n",
       "      <td>-0.756948</td>\n",
       "      <td>-0.720949</td>\n",
       "      <td>-0.697247</td>\n",
       "      <td>-0.745189</td>\n",
       "      <td>-0.699182</td>\n",
       "      <td>-0.720210</td>\n",
       "      <td>-0.720815</td>\n",
       "      <td>-0.745181</td>\n",
       "      <td>-0.745175</td>\n",
       "      <td>-0.694137</td>\n",
       "      <td>-0.745181</td>\n",
       "      <td>0.760898</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19706</th>\n",
       "      <td>-1.826040</td>\n",
       "      <td>-1.946252</td>\n",
       "      <td>-0.140323</td>\n",
       "      <td>-1.490113</td>\n",
       "      <td>-1.226654</td>\n",
       "      <td>-0.830248</td>\n",
       "      <td>-0.825180</td>\n",
       "      <td>-1.111304</td>\n",
       "      <td>-0.421942</td>\n",
       "      <td>0.311960</td>\n",
       "      <td>1.003101</td>\n",
       "      <td>-0.265331</td>\n",
       "      <td>0.700887</td>\n",
       "      <td>-3.043859</td>\n",
       "      <td>0.871073</td>\n",
       "      <td>-1.528086</td>\n",
       "      <td>-1.118872</td>\n",
       "      <td>-0.091912</td>\n",
       "      <td>0.865527</td>\n",
       "      <td>-1.529691</td>\n",
       "      <td>0.449575</td>\n",
       "      <td>-2.104880</td>\n",
       "      <td>0.521008</td>\n",
       "      <td>-1.530417</td>\n",
       "      <td>-4.937934</td>\n",
       "      <td>-2.202229</td>\n",
       "      <td>-1.529687</td>\n",
       "      <td>0.236812</td>\n",
       "      <td>-1.528866</td>\n",
       "      <td>-1.528966</td>\n",
       "      <td>-1.043437</td>\n",
       "      <td>-1.525830</td>\n",
       "      <td>-1.093863</td>\n",
       "      <td>-1.527443</td>\n",
       "      <td>-1.080235</td>\n",
       "      <td>-1.063649</td>\n",
       "      <td>-1.099317</td>\n",
       "      <td>-1.074473</td>\n",
       "      <td>-1.088307</td>\n",
       "      <td>-1.094444</td>\n",
       "      <td>-1.527371</td>\n",
       "      <td>-1.093381</td>\n",
       "      <td>-1.381861</td>\n",
       "      <td>-1.092859</td>\n",
       "      <td>-1.020866</td>\n",
       "      <td>-1.528064</td>\n",
       "      <td>-1.014309</td>\n",
       "      <td>-1.091924</td>\n",
       "      <td>-1.092571</td>\n",
       "      <td>-1.527824</td>\n",
       "      <td>-1.527832</td>\n",
       "      <td>-1.016333</td>\n",
       "      <td>-1.527801</td>\n",
       "      <td>-1.826040</td>\n",
       "      <td>-0.009259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19707 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X39       X53       X45       X13       X36       X15       X51  \\\n",
       "0      0.303619 -1.368098  0.318033 -0.664706  0.031642 -0.637868 -0.901035   \n",
       "1      0.270544 -0.293297 -0.800283  0.205806 -0.817904 -0.267324 -0.575346   \n",
       "2      0.408199 -1.086199  0.476146 -0.632996  0.711765 -0.619817 -0.865759   \n",
       "3      0.320914 -0.331991 -0.839812  0.407444  0.304629 -0.155761 -0.566030   \n",
       "4      0.399412 -0.842829  0.232809 -0.624319  1.072002 -0.617945  0.000683   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702 -1.811006 -2.112556 -0.131305 -1.445311 -1.197634 -0.818321 -1.049270   \n",
       "19703  0.753172 -1.087454 -0.172401 -0.671626  0.416608 -0.605980 -0.603986   \n",
       "19704 -1.688806 -2.147295  0.017075 -1.469679 -0.961748 -0.824514 -1.071355   \n",
       "19705  0.760898 -1.150622 -0.441701 -0.690957  0.747083 -0.613353 -0.699474   \n",
       "19706 -1.826040 -1.946252 -0.140323 -1.490113 -1.226654 -0.830248 -0.825180   \n",
       "\n",
       "            X49       X33       X40       X31       X34       X32       X50  \\\n",
       "0     -0.789375 -0.896748 -0.626164 -0.752216 -1.400652 -0.490283 -2.085998   \n",
       "1     -0.088519  0.252945  0.516988 -0.888595 -0.183459 -0.612266 -1.281293   \n",
       "2     -0.691751 -0.650123 -0.491680 -0.751901 -1.105154 -0.523756 -1.759521   \n",
       "3      0.025930 -1.156736  1.424197 -0.719807 -0.181216 -0.422551 -0.963558   \n",
       "4     -0.669520 -0.532747 -0.602045 -0.789767 -0.971547 -0.547541 -1.815948   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702 -1.122315 -0.508777 -0.357198  1.027011 -0.311548  0.721451 -2.989994   \n",
       "19703 -0.787102  1.405494 -0.960323  0.122744  1.482535  0.217549 -1.818208   \n",
       "19704 -1.145723 -0.484022  0.003704  1.021900 -0.304297  0.720133 -3.081163   \n",
       "19705 -0.734400  1.405871 -0.783727  0.132903  1.494039  0.229551 -1.906146   \n",
       "19706 -1.111304 -0.421942  0.311960  1.003101 -0.265331  0.700887 -3.043859   \n",
       "\n",
       "            X30       X18       X46       X35       X37       X17       X42  \\\n",
       "0     -0.213036 -0.730901 -0.721203 -1.097359  1.065540 -0.730536  0.097683   \n",
       "1     -0.365916  0.108370 -0.104701  0.079553  0.531337  0.111701  1.092659   \n",
       "2     -1.664908 -0.695644 -0.693250 -0.846007  1.020084 -0.695285  0.317099   \n",
       "3     -0.236426  0.301783  0.072858  0.096051  0.556588  0.303111  1.065361   \n",
       "4     -1.508678 -0.686499 -0.687388 -0.725842  1.004514 -0.686138  0.421749   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702  0.889711 -1.483919 -1.102665 -0.136718  0.872560 -1.485440  0.397786   \n",
       "19703 -0.006719 -0.725304 -0.736942  1.349400  0.214743 -0.727687 -1.691140   \n",
       "19704  0.888471 -1.507813 -1.113767 -0.130509  0.867612 -1.509366  0.424591   \n",
       "19705  0.028314 -0.745036 -0.748550  1.345973  0.203279 -0.747514 -1.581334   \n",
       "19706  0.871073 -1.528086 -1.118872 -0.091912  0.865527 -1.529691  0.449575   \n",
       "\n",
       "            X38       X43       X47        X0       X52       X48       X41  \\\n",
       "0      0.585017 -0.309425 -0.733860  0.162955 -1.752634 -0.736883  0.265702   \n",
       "1      0.558323  0.561124  0.114858  0.637649 -0.454719  0.113503  1.172467   \n",
       "2      0.571099 -0.093730 -0.691992 -0.162791 -1.178735 -0.693527  0.322890   \n",
       "3      0.716041  0.727413  0.300229  0.981367 -0.507419  0.303556  1.175421   \n",
       "4      0.566900  0.010782 -0.680960 -0.271713 -1.155744 -0.683218  0.399896   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702 -2.070832  0.489914 -1.491222 -4.525414 -2.391811 -1.488305  0.216442   \n",
       "19703  0.832361 -1.892583 -0.731506 -3.272527 -1.266335 -0.731930  0.941092   \n",
       "19704 -1.942305  0.494363 -1.515185 -4.630214 -2.439289 -1.515038  0.216268   \n",
       "19705  0.847799 -1.795700 -0.744393 -3.384335 -1.321487 -0.747615  0.933312   \n",
       "19706 -2.104880  0.521008 -1.530417 -4.937934 -2.202229 -1.529687  0.236812   \n",
       "\n",
       "             X2        X3       X25       X23        X9       X27       X14  \\\n",
       "0     -0.732184 -0.732188 -0.691366 -0.726183 -0.704597 -0.728807 -0.710549   \n",
       "1      0.110364  0.110365 -0.143167  0.113187 -0.092878  0.115930 -0.129771   \n",
       "2     -0.696694 -0.696709 -0.664141 -0.691275 -0.680123 -0.693348 -0.691434   \n",
       "3      0.300083  0.300104  0.026371  0.305106  0.084091  0.308407  0.038148   \n",
       "4     -0.688086 -0.688104 -0.661634 -0.682309 -0.675430 -0.684194 -0.686846   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702 -1.483623 -1.483645 -1.030355 -1.481442 -1.076202 -1.483214 -1.062814   \n",
       "19703 -0.720566 -0.720532 -0.645284 -0.724744 -0.706717 -0.723241 -0.704043   \n",
       "19704 -1.507740 -1.507753 -1.038492 -1.505378 -1.085195 -1.507161 -1.071938   \n",
       "19705 -0.740309 -0.740269 -0.655384 -0.744566 -0.718997 -0.743016 -0.714902   \n",
       "19706 -1.528866 -1.528966 -1.043437 -1.525830 -1.093863 -1.527443 -1.080235   \n",
       "\n",
       "            X19       X24       X16       X29       X26       X28       X21  \\\n",
       "0     -0.708777 -0.706969 -0.709911 -0.709945 -0.706522 -0.728111 -0.709008   \n",
       "1     -0.128735 -0.094559 -0.137471 -0.090028 -0.090889  0.114014 -0.093737   \n",
       "2     -0.689171 -0.681219 -0.686197 -0.688039 -0.683237 -0.692878 -0.683230   \n",
       "3      0.039061  0.082331  0.028317  0.089581  0.086617  0.305908  0.083850   \n",
       "4     -0.684348 -0.676160 -0.681855 -0.682763 -0.678181 -0.683735 -0.678193   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702 -1.047183 -1.082408 -1.057290 -1.070795 -1.077186 -1.483106 -1.077155   \n",
       "19703 -0.710849 -0.699841 -0.697351 -0.719490 -0.712830 -0.725970 -0.712226   \n",
       "19704 -1.056292 -1.091933 -1.066310 -1.080554 -1.086699 -1.507026 -1.087777   \n",
       "19705 -0.721721 -0.711319 -0.708349 -0.730993 -0.724333 -0.745783 -0.723759   \n",
       "19706 -1.063649 -1.099317 -1.074473 -1.088307 -1.094444 -1.527371 -1.093381   \n",
       "\n",
       "             X1       X11       X20       X22        X5        X4        X6  \\\n",
       "0     -0.754861 -0.704447 -0.684198 -0.728199 -0.680463 -0.704870 -0.704561   \n",
       "1      0.036367 -0.092026 -0.128428  0.113681 -0.124515 -0.092511 -0.091235   \n",
       "2     -0.722643 -0.680831 -0.662410 -0.692527 -0.657241 -0.680773 -0.681192   \n",
       "3      0.227740  0.085438  0.038418  0.305550  0.042203  0.085026  0.085652   \n",
       "4     -0.715838 -0.676215 -0.658180 -0.683386 -0.653732 -0.675658 -0.676009   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19702 -1.348169 -1.074557 -1.005825 -1.483853 -0.998264 -1.075246 -1.074719   \n",
       "19703 -0.739703 -0.709396 -0.687081 -0.725374 -0.687902 -0.708704 -0.709225   \n",
       "19704 -1.365862 -1.084041 -1.013990 -1.507778 -1.005437 -1.084875 -1.084221   \n",
       "19705 -0.756948 -0.720949 -0.697247 -0.745189 -0.699182 -0.720210 -0.720815   \n",
       "19706 -1.381861 -1.092859 -1.020866 -1.528064 -1.014309 -1.091924 -1.092571   \n",
       "\n",
       "            X12        X7       X10        X8       X62    TARGET  \n",
       "0     -0.728003 -0.728004 -0.679034 -0.728009  0.303619  0.010438  \n",
       "1      0.113804  0.113818 -0.122337  0.113839  0.270544 -0.000532  \n",
       "2     -0.692505 -0.692521 -0.657914 -0.692516  0.408199  0.000726  \n",
       "3      0.305663  0.305680  0.043227  0.305671  0.320914  0.001459  \n",
       "4     -0.683393 -0.683402 -0.653591 -0.683384  0.399412 -0.001462  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "19702 -1.483446 -1.483451 -1.000564 -1.483459 -1.811006  0.000061  \n",
       "19703 -0.725369 -0.725369 -0.684054 -0.725367  0.753172  0.000250  \n",
       "19704 -1.507369 -1.507383 -1.008664 -1.507384 -1.688806  0.000799  \n",
       "19705 -0.745181 -0.745175 -0.694137 -0.745181  0.760898  0.009734  \n",
       "19706 -1.527824 -1.527832 -1.016333 -1.527801 -1.826040 -0.009259  \n",
       "\n",
       "[19707 rows x 55 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seurat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0404bc17-40e7-423f-87c3-a050e7fbd448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum return of training set: 195.6927566509\n",
      "Maximum return of testing set: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "train_max = np.sum(train['TARGET'][train['TARGET']>0])\n",
    "test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Maximum return of training set:', train_max)\n",
    "print('Maximum return of testing set:', test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9149c4-b51c-435c-9e81-744f395e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=0.5).fit(pd.DataFrame(train_seurat.iloc[:, :-1]), train_seurat['TARGET'])\n",
    "train_pred = reg.predict(pd.DataFrame(train_seurat.iloc[:, :-1]))\n",
    "\n",
    "test_pred = reg.predict(pd.DataFrame(test_seurat.iloc[:, :-1]))\n",
    "\n",
    "train_res = np.sum(train['TARGET'][train_pred>0])\n",
    "test_res = np.sum(test['TARGET'][test_pred>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4db722a-05ac-4744-bd74-10c3aa0102b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train naive random selection percentage return: 6.326525261528151%\n",
      "Test naive random selection percentage return: 4.010962118285209%\n"
     ]
    }
   ],
   "source": [
    "print(f'Train naive random selection percentage return: {train_res/train_max*100}%')\n",
    "print(f'Test naive random selection percentage return: {test_res/test_max*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bb5b0f0-c09f-44ee-975c-8613c205ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_seurat\n",
    "test = test_seurat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd80365c-a512-42fd-a21c-1858ea6fd192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X39</th>\n",
       "      <th>X53</th>\n",
       "      <th>X45</th>\n",
       "      <th>X13</th>\n",
       "      <th>X36</th>\n",
       "      <th>X15</th>\n",
       "      <th>X51</th>\n",
       "      <th>X49</th>\n",
       "      <th>X33</th>\n",
       "      <th>X40</th>\n",
       "      <th>X31</th>\n",
       "      <th>X34</th>\n",
       "      <th>X32</th>\n",
       "      <th>X50</th>\n",
       "      <th>X30</th>\n",
       "      <th>X18</th>\n",
       "      <th>X46</th>\n",
       "      <th>X35</th>\n",
       "      <th>X37</th>\n",
       "      <th>X17</th>\n",
       "      <th>X42</th>\n",
       "      <th>X38</th>\n",
       "      <th>X43</th>\n",
       "      <th>X47</th>\n",
       "      <th>X0</th>\n",
       "      <th>X52</th>\n",
       "      <th>X48</th>\n",
       "      <th>X41</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X25</th>\n",
       "      <th>X23</th>\n",
       "      <th>X9</th>\n",
       "      <th>X27</th>\n",
       "      <th>X14</th>\n",
       "      <th>X19</th>\n",
       "      <th>X24</th>\n",
       "      <th>X16</th>\n",
       "      <th>X29</th>\n",
       "      <th>X26</th>\n",
       "      <th>X28</th>\n",
       "      <th>X21</th>\n",
       "      <th>X1</th>\n",
       "      <th>X11</th>\n",
       "      <th>X20</th>\n",
       "      <th>X22</th>\n",
       "      <th>X5</th>\n",
       "      <th>X4</th>\n",
       "      <th>X6</th>\n",
       "      <th>X12</th>\n",
       "      <th>X7</th>\n",
       "      <th>X10</th>\n",
       "      <th>X8</th>\n",
       "      <th>X62</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.582156</td>\n",
       "      <td>1.230171</td>\n",
       "      <td>2.175619</td>\n",
       "      <td>2.246967</td>\n",
       "      <td>-0.755876</td>\n",
       "      <td>3.583055</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>3.040176</td>\n",
       "      <td>0.919099</td>\n",
       "      <td>-0.889421</td>\n",
       "      <td>0.484578</td>\n",
       "      <td>0.481170</td>\n",
       "      <td>0.777112</td>\n",
       "      <td>-1.798247</td>\n",
       "      <td>0.873301</td>\n",
       "      <td>2.270459</td>\n",
       "      <td>2.912388</td>\n",
       "      <td>0.728549</td>\n",
       "      <td>-1.295910</td>\n",
       "      <td>2.275724</td>\n",
       "      <td>-0.359744</td>\n",
       "      <td>-0.363988</td>\n",
       "      <td>-0.717875</td>\n",
       "      <td>2.265473</td>\n",
       "      <td>1.019072</td>\n",
       "      <td>0.721157</td>\n",
       "      <td>2.260484</td>\n",
       "      <td>-1.834228</td>\n",
       "      <td>2.254474</td>\n",
       "      <td>2.254450</td>\n",
       "      <td>3.042155</td>\n",
       "      <td>2.259637</td>\n",
       "      <td>2.946272</td>\n",
       "      <td>2.258885</td>\n",
       "      <td>3.401480</td>\n",
       "      <td>3.345012</td>\n",
       "      <td>2.937407</td>\n",
       "      <td>3.343820</td>\n",
       "      <td>2.935077</td>\n",
       "      <td>2.941428</td>\n",
       "      <td>2.259203</td>\n",
       "      <td>2.938547</td>\n",
       "      <td>2.469231</td>\n",
       "      <td>2.945955</td>\n",
       "      <td>3.044228</td>\n",
       "      <td>2.259368</td>\n",
       "      <td>3.073369</td>\n",
       "      <td>2.943574</td>\n",
       "      <td>2.945749</td>\n",
       "      <td>2.259507</td>\n",
       "      <td>2.259501</td>\n",
       "      <td>3.071520</td>\n",
       "      <td>2.259504</td>\n",
       "      <td>-1.582156</td>\n",
       "      <td>0.013314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.584404</td>\n",
       "      <td>1.605045</td>\n",
       "      <td>-2.204271</td>\n",
       "      <td>2.227963</td>\n",
       "      <td>-0.760217</td>\n",
       "      <td>3.535377</td>\n",
       "      <td>0.300338</td>\n",
       "      <td>2.880602</td>\n",
       "      <td>0.943713</td>\n",
       "      <td>-0.890453</td>\n",
       "      <td>0.513255</td>\n",
       "      <td>0.509388</td>\n",
       "      <td>0.803151</td>\n",
       "      <td>-1.042563</td>\n",
       "      <td>0.900408</td>\n",
       "      <td>2.251683</td>\n",
       "      <td>2.831043</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>-1.302729</td>\n",
       "      <td>2.256918</td>\n",
       "      <td>-0.368216</td>\n",
       "      <td>-0.372547</td>\n",
       "      <td>-0.725350</td>\n",
       "      <td>2.242051</td>\n",
       "      <td>1.084322</td>\n",
       "      <td>1.273463</td>\n",
       "      <td>2.239054</td>\n",
       "      <td>-1.836103</td>\n",
       "      <td>2.235786</td>\n",
       "      <td>2.235849</td>\n",
       "      <td>2.995526</td>\n",
       "      <td>2.240825</td>\n",
       "      <td>2.901227</td>\n",
       "      <td>2.240080</td>\n",
       "      <td>3.359956</td>\n",
       "      <td>3.304083</td>\n",
       "      <td>2.898066</td>\n",
       "      <td>3.302884</td>\n",
       "      <td>2.895731</td>\n",
       "      <td>2.902085</td>\n",
       "      <td>2.240400</td>\n",
       "      <td>2.910457</td>\n",
       "      <td>2.446440</td>\n",
       "      <td>2.903884</td>\n",
       "      <td>3.007216</td>\n",
       "      <td>2.240566</td>\n",
       "      <td>3.004351</td>\n",
       "      <td>2.907020</td>\n",
       "      <td>2.906517</td>\n",
       "      <td>2.240683</td>\n",
       "      <td>2.240706</td>\n",
       "      <td>3.025257</td>\n",
       "      <td>2.240732</td>\n",
       "      <td>-1.584404</td>\n",
       "      <td>-0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.594548</td>\n",
       "      <td>1.172960</td>\n",
       "      <td>2.547328</td>\n",
       "      <td>2.138920</td>\n",
       "      <td>-0.779797</td>\n",
       "      <td>3.321969</td>\n",
       "      <td>-0.241806</td>\n",
       "      <td>2.693446</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>-0.895068</td>\n",
       "      <td>0.505998</td>\n",
       "      <td>0.502247</td>\n",
       "      <td>0.793162</td>\n",
       "      <td>-1.196648</td>\n",
       "      <td>0.893682</td>\n",
       "      <td>2.165409</td>\n",
       "      <td>2.656538</td>\n",
       "      <td>0.750002</td>\n",
       "      <td>-1.334065</td>\n",
       "      <td>2.170502</td>\n",
       "      <td>-0.407147</td>\n",
       "      <td>-0.411875</td>\n",
       "      <td>-0.759699</td>\n",
       "      <td>2.150257</td>\n",
       "      <td>0.909735</td>\n",
       "      <td>0.858274</td>\n",
       "      <td>2.152557</td>\n",
       "      <td>-1.844565</td>\n",
       "      <td>2.150014</td>\n",
       "      <td>2.149977</td>\n",
       "      <td>2.786986</td>\n",
       "      <td>2.154385</td>\n",
       "      <td>2.734335</td>\n",
       "      <td>2.153668</td>\n",
       "      <td>3.172415</td>\n",
       "      <td>3.119236</td>\n",
       "      <td>2.720591</td>\n",
       "      <td>3.118004</td>\n",
       "      <td>2.718235</td>\n",
       "      <td>2.724605</td>\n",
       "      <td>2.154003</td>\n",
       "      <td>2.731983</td>\n",
       "      <td>2.340769</td>\n",
       "      <td>2.729945</td>\n",
       "      <td>2.798645</td>\n",
       "      <td>2.154173</td>\n",
       "      <td>2.832534</td>\n",
       "      <td>2.727687</td>\n",
       "      <td>2.728807</td>\n",
       "      <td>2.154322</td>\n",
       "      <td>2.154294</td>\n",
       "      <td>2.816114</td>\n",
       "      <td>2.154304</td>\n",
       "      <td>-1.594548</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.540117</td>\n",
       "      <td>1.471016</td>\n",
       "      <td>-1.464351</td>\n",
       "      <td>2.589700</td>\n",
       "      <td>-0.674728</td>\n",
       "      <td>4.499712</td>\n",
       "      <td>-0.070470</td>\n",
       "      <td>3.630669</td>\n",
       "      <td>-0.163725</td>\n",
       "      <td>-0.869522</td>\n",
       "      <td>0.771819</td>\n",
       "      <td>0.763805</td>\n",
       "      <td>1.057894</td>\n",
       "      <td>-1.013944</td>\n",
       "      <td>1.144550</td>\n",
       "      <td>2.601021</td>\n",
       "      <td>3.586176</td>\n",
       "      <td>1.014063</td>\n",
       "      <td>-1.175844</td>\n",
       "      <td>2.588606</td>\n",
       "      <td>-0.210578</td>\n",
       "      <td>-0.213300</td>\n",
       "      <td>-0.586264</td>\n",
       "      <td>2.586420</td>\n",
       "      <td>1.452937</td>\n",
       "      <td>1.030784</td>\n",
       "      <td>2.588699</td>\n",
       "      <td>-1.799160</td>\n",
       "      <td>2.583419</td>\n",
       "      <td>2.583483</td>\n",
       "      <td>3.941371</td>\n",
       "      <td>2.590836</td>\n",
       "      <td>3.676733</td>\n",
       "      <td>2.589976</td>\n",
       "      <td>4.174776</td>\n",
       "      <td>4.107204</td>\n",
       "      <td>3.672950</td>\n",
       "      <td>4.106144</td>\n",
       "      <td>3.670702</td>\n",
       "      <td>3.676991</td>\n",
       "      <td>2.590239</td>\n",
       "      <td>3.685219</td>\n",
       "      <td>2.879247</td>\n",
       "      <td>3.681201</td>\n",
       "      <td>3.949020</td>\n",
       "      <td>2.590387</td>\n",
       "      <td>3.953846</td>\n",
       "      <td>3.682269</td>\n",
       "      <td>3.681686</td>\n",
       "      <td>2.590519</td>\n",
       "      <td>2.590540</td>\n",
       "      <td>3.971859</td>\n",
       "      <td>2.590523</td>\n",
       "      <td>-1.540117</td>\n",
       "      <td>-0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.601161</td>\n",
       "      <td>1.122772</td>\n",
       "      <td>2.557003</td>\n",
       "      <td>2.079653</td>\n",
       "      <td>-0.792563</td>\n",
       "      <td>3.184293</td>\n",
       "      <td>0.169903</td>\n",
       "      <td>2.380626</td>\n",
       "      <td>0.968678</td>\n",
       "      <td>-0.898042</td>\n",
       "      <td>0.546909</td>\n",
       "      <td>0.542502</td>\n",
       "      <td>0.829559</td>\n",
       "      <td>-1.363513</td>\n",
       "      <td>-0.469419</td>\n",
       "      <td>2.107699</td>\n",
       "      <td>2.530198</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>-1.355026</td>\n",
       "      <td>2.112698</td>\n",
       "      <td>-0.433188</td>\n",
       "      <td>-0.438182</td>\n",
       "      <td>-0.782676</td>\n",
       "      <td>2.091026</td>\n",
       "      <td>0.925432</td>\n",
       "      <td>0.645674</td>\n",
       "      <td>2.091058</td>\n",
       "      <td>-1.850081</td>\n",
       "      <td>2.092565</td>\n",
       "      <td>2.092535</td>\n",
       "      <td>2.652600</td>\n",
       "      <td>2.096564</td>\n",
       "      <td>2.618092</td>\n",
       "      <td>2.095867</td>\n",
       "      <td>2.443792</td>\n",
       "      <td>2.998522</td>\n",
       "      <td>2.604873</td>\n",
       "      <td>2.997269</td>\n",
       "      <td>2.602504</td>\n",
       "      <td>2.608884</td>\n",
       "      <td>2.096211</td>\n",
       "      <td>2.615890</td>\n",
       "      <td>2.270373</td>\n",
       "      <td>2.613631</td>\n",
       "      <td>2.663541</td>\n",
       "      <td>2.096384</td>\n",
       "      <td>2.693932</td>\n",
       "      <td>2.611509</td>\n",
       "      <td>2.613064</td>\n",
       "      <td>2.096538</td>\n",
       "      <td>2.096525</td>\n",
       "      <td>2.682193</td>\n",
       "      <td>2.096533</td>\n",
       "      <td>-1.601161</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>0.103760</td>\n",
       "      <td>-0.323209</td>\n",
       "      <td>-0.033912</td>\n",
       "      <td>0.504407</td>\n",
       "      <td>0.932129</td>\n",
       "      <td>-0.098634</td>\n",
       "      <td>-0.687340</td>\n",
       "      <td>0.052372</td>\n",
       "      <td>0.175746</td>\n",
       "      <td>-0.657793</td>\n",
       "      <td>-1.040164</td>\n",
       "      <td>-0.270910</td>\n",
       "      <td>-0.719094</td>\n",
       "      <td>-0.871932</td>\n",
       "      <td>-0.530427</td>\n",
       "      <td>0.405426</td>\n",
       "      <td>0.159104</td>\n",
       "      <td>-0.021451</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.407638</td>\n",
       "      <td>1.068273</td>\n",
       "      <td>0.466155</td>\n",
       "      <td>0.542083</td>\n",
       "      <td>0.404342</td>\n",
       "      <td>0.637946</td>\n",
       "      <td>-0.699337</td>\n",
       "      <td>0.402187</td>\n",
       "      <td>1.206836</td>\n",
       "      <td>0.403753</td>\n",
       "      <td>0.403796</td>\n",
       "      <td>0.111838</td>\n",
       "      <td>0.409407</td>\n",
       "      <td>0.165668</td>\n",
       "      <td>0.412404</td>\n",
       "      <td>0.118302</td>\n",
       "      <td>0.112101</td>\n",
       "      <td>0.166753</td>\n",
       "      <td>0.107222</td>\n",
       "      <td>0.167445</td>\n",
       "      <td>0.164019</td>\n",
       "      <td>0.409890</td>\n",
       "      <td>0.160662</td>\n",
       "      <td>0.327742</td>\n",
       "      <td>0.165710</td>\n",
       "      <td>0.112724</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.121034</td>\n",
       "      <td>0.165462</td>\n",
       "      <td>0.165721</td>\n",
       "      <td>0.409705</td>\n",
       "      <td>0.409703</td>\n",
       "      <td>0.115440</td>\n",
       "      <td>0.409704</td>\n",
       "      <td>0.103760</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>0.270911</td>\n",
       "      <td>-1.178868</td>\n",
       "      <td>-0.215649</td>\n",
       "      <td>-0.526271</td>\n",
       "      <td>-0.831779</td>\n",
       "      <td>-0.589190</td>\n",
       "      <td>-0.853648</td>\n",
       "      <td>-0.699739</td>\n",
       "      <td>-0.942459</td>\n",
       "      <td>-0.612974</td>\n",
       "      <td>-0.772829</td>\n",
       "      <td>-1.455120</td>\n",
       "      <td>-0.514554</td>\n",
       "      <td>-1.595189</td>\n",
       "      <td>-0.289078</td>\n",
       "      <td>-0.589598</td>\n",
       "      <td>-0.636953</td>\n",
       "      <td>-1.152731</td>\n",
       "      <td>1.088975</td>\n",
       "      <td>-0.589015</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>0.642626</td>\n",
       "      <td>-0.349257</td>\n",
       "      <td>-0.591799</td>\n",
       "      <td>-0.636469</td>\n",
       "      <td>-1.493630</td>\n",
       "      <td>-0.593950</td>\n",
       "      <td>0.298781</td>\n",
       "      <td>-0.592508</td>\n",
       "      <td>-0.592574</td>\n",
       "      <td>-0.619011</td>\n",
       "      <td>-0.584295</td>\n",
       "      <td>-0.627197</td>\n",
       "      <td>-0.587219</td>\n",
       "      <td>-0.635496</td>\n",
       "      <td>-0.635929</td>\n",
       "      <td>-0.626819</td>\n",
       "      <td>-0.635882</td>\n",
       "      <td>-0.631618</td>\n",
       "      <td>-0.629097</td>\n",
       "      <td>-0.586609</td>\n",
       "      <td>-0.630493</td>\n",
       "      <td>-0.639392</td>\n",
       "      <td>-0.627132</td>\n",
       "      <td>-0.614351</td>\n",
       "      <td>-0.586671</td>\n",
       "      <td>-0.614544</td>\n",
       "      <td>-0.626798</td>\n",
       "      <td>-0.627063</td>\n",
       "      <td>-0.586522</td>\n",
       "      <td>-0.586520</td>\n",
       "      <td>-0.611206</td>\n",
       "      <td>-0.586518</td>\n",
       "      <td>0.270911</td>\n",
       "      <td>-0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>0.162217</td>\n",
       "      <td>-0.419919</td>\n",
       "      <td>0.693772</td>\n",
       "      <td>0.434187</td>\n",
       "      <td>-0.573241</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>-0.742473</td>\n",
       "      <td>-0.011175</td>\n",
       "      <td>0.196149</td>\n",
       "      <td>-0.250024</td>\n",
       "      <td>-0.946669</td>\n",
       "      <td>-0.246502</td>\n",
       "      <td>-0.638050</td>\n",
       "      <td>-1.088193</td>\n",
       "      <td>-0.446876</td>\n",
       "      <td>0.337099</td>\n",
       "      <td>0.088322</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.614071</td>\n",
       "      <td>0.339199</td>\n",
       "      <td>1.086706</td>\n",
       "      <td>0.536077</td>\n",
       "      <td>0.558347</td>\n",
       "      <td>0.335925</td>\n",
       "      <td>0.281215</td>\n",
       "      <td>-0.787438</td>\n",
       "      <td>0.333698</td>\n",
       "      <td>1.190530</td>\n",
       "      <td>0.335457</td>\n",
       "      <td>0.335491</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0.340944</td>\n",
       "      <td>0.098978</td>\n",
       "      <td>0.343962</td>\n",
       "      <td>0.054459</td>\n",
       "      <td>0.049077</td>\n",
       "      <td>0.099541</td>\n",
       "      <td>0.044285</td>\n",
       "      <td>0.100122</td>\n",
       "      <td>0.097013</td>\n",
       "      <td>0.341461</td>\n",
       "      <td>0.093592</td>\n",
       "      <td>0.256660</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.341165</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.098176</td>\n",
       "      <td>0.098726</td>\n",
       "      <td>0.341287</td>\n",
       "      <td>0.341285</td>\n",
       "      <td>0.052269</td>\n",
       "      <td>0.341284</td>\n",
       "      <td>0.162217</td>\n",
       "      <td>-0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>0.293136</td>\n",
       "      <td>-1.281370</td>\n",
       "      <td>-0.420295</td>\n",
       "      <td>-0.568879</td>\n",
       "      <td>-0.518779</td>\n",
       "      <td>-0.604738</td>\n",
       "      <td>-0.910118</td>\n",
       "      <td>-0.724082</td>\n",
       "      <td>-0.860496</td>\n",
       "      <td>-0.316399</td>\n",
       "      <td>-0.737283</td>\n",
       "      <td>-1.361772</td>\n",
       "      <td>-0.481393</td>\n",
       "      <td>-1.783958</td>\n",
       "      <td>-0.256901</td>\n",
       "      <td>-0.631227</td>\n",
       "      <td>-0.663933</td>\n",
       "      <td>-1.067223</td>\n",
       "      <td>1.077085</td>\n",
       "      <td>-0.630712</td>\n",
       "      <td>0.132091</td>\n",
       "      <td>0.671236</td>\n",
       "      <td>-0.283923</td>\n",
       "      <td>-0.633482</td>\n",
       "      <td>-0.834242</td>\n",
       "      <td>-1.611994</td>\n",
       "      <td>-0.635652</td>\n",
       "      <td>0.296660</td>\n",
       "      <td>-0.634312</td>\n",
       "      <td>-0.634379</td>\n",
       "      <td>-0.642176</td>\n",
       "      <td>-0.626008</td>\n",
       "      <td>-0.652879</td>\n",
       "      <td>-0.628916</td>\n",
       "      <td>-0.660001</td>\n",
       "      <td>-0.660042</td>\n",
       "      <td>-0.652617</td>\n",
       "      <td>-0.660039</td>\n",
       "      <td>-0.657376</td>\n",
       "      <td>-0.654767</td>\n",
       "      <td>-0.628299</td>\n",
       "      <td>-0.656370</td>\n",
       "      <td>-0.676444</td>\n",
       "      <td>-0.652756</td>\n",
       "      <td>-0.637118</td>\n",
       "      <td>-0.628359</td>\n",
       "      <td>-0.636736</td>\n",
       "      <td>-0.652425</td>\n",
       "      <td>-0.652732</td>\n",
       "      <td>-0.628208</td>\n",
       "      <td>-0.628205</td>\n",
       "      <td>-0.633950</td>\n",
       "      <td>-0.628206</td>\n",
       "      <td>0.293136</td>\n",
       "      <td>0.001793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>0.210972</td>\n",
       "      <td>-0.513759</td>\n",
       "      <td>-0.035841</td>\n",
       "      <td>0.366002</td>\n",
       "      <td>-0.691927</td>\n",
       "      <td>-0.181877</td>\n",
       "      <td>-0.767623</td>\n",
       "      <td>-0.070869</td>\n",
       "      <td>0.213926</td>\n",
       "      <td>0.237483</td>\n",
       "      <td>-0.868693</td>\n",
       "      <td>-0.225208</td>\n",
       "      <td>-0.569781</td>\n",
       "      <td>-1.323778</td>\n",
       "      <td>-0.376795</td>\n",
       "      <td>0.270913</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.023959</td>\n",
       "      <td>0.602033</td>\n",
       "      <td>0.272905</td>\n",
       "      <td>1.102766</td>\n",
       "      <td>0.594978</td>\n",
       "      <td>0.572517</td>\n",
       "      <td>0.269653</td>\n",
       "      <td>0.469417</td>\n",
       "      <td>-0.918191</td>\n",
       "      <td>0.267422</td>\n",
       "      <td>1.174767</td>\n",
       "      <td>0.269286</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.035641</td>\n",
       "      <td>0.277665</td>\n",
       "      <td>-0.005518</td>\n",
       "      <td>-0.010130</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>-0.014841</td>\n",
       "      <td>0.036875</td>\n",
       "      <td>0.034066</td>\n",
       "      <td>0.275177</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.188415</td>\n",
       "      <td>0.035605</td>\n",
       "      <td>-0.009769</td>\n",
       "      <td>0.274885</td>\n",
       "      <td>-0.003718</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>0.035834</td>\n",
       "      <td>0.274997</td>\n",
       "      <td>0.274996</td>\n",
       "      <td>-0.006977</td>\n",
       "      <td>0.275004</td>\n",
       "      <td>0.210972</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X39       X53       X45       X13       X36       X15       X51  \\\n",
       "0     -1.582156  1.230171  2.175619  2.246967 -0.755876  3.583055  0.098955   \n",
       "1     -1.584404  1.605045 -2.204271  2.227963 -0.760217  3.535377  0.300338   \n",
       "2     -1.594548  1.172960  2.547328  2.138920 -0.779797  3.321969 -0.241806   \n",
       "3     -1.540117  1.471016 -1.464351  2.589700 -0.674728  4.499712 -0.070470   \n",
       "4     -1.601161  1.122772  2.557003  2.079653 -0.792563  3.184293  0.169903   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995  0.103760 -0.323209 -0.033912  0.504407  0.932129 -0.098634 -0.687340   \n",
       "89996  0.270911 -1.178868 -0.215649 -0.526271 -0.831779 -0.589190 -0.853648   \n",
       "89997  0.162217 -0.419919  0.693772  0.434187 -0.573241 -0.141667 -0.742473   \n",
       "89998  0.293136 -1.281370 -0.420295 -0.568879 -0.518779 -0.604738 -0.910118   \n",
       "89999  0.210972 -0.513759 -0.035841  0.366002 -0.691927 -0.181877 -0.767623   \n",
       "\n",
       "            X49       X33       X40       X31       X34       X32       X50  \\\n",
       "0      3.040176  0.919099 -0.889421  0.484578  0.481170  0.777112 -1.798247   \n",
       "1      2.880602  0.943713 -0.890453  0.513255  0.509388  0.803151 -1.042563   \n",
       "2      2.693446  0.934271 -0.895068  0.505998  0.502247  0.793162 -1.196648   \n",
       "3      3.630669 -0.163725 -0.869522  0.771819  0.763805  1.057894 -1.013944   \n",
       "4      2.380626  0.968678 -0.898042  0.546909  0.542502  0.829559 -1.363513   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995  0.052372  0.175746 -0.657793 -1.040164 -0.270910 -0.719094 -0.871932   \n",
       "89996 -0.699739 -0.942459 -0.612974 -0.772829 -1.455120 -0.514554 -1.595189   \n",
       "89997 -0.011175  0.196149 -0.250024 -0.946669 -0.246502 -0.638050 -1.088193   \n",
       "89998 -0.724082 -0.860496 -0.316399 -0.737283 -1.361772 -0.481393 -1.783958   \n",
       "89999 -0.070869  0.213926  0.237483 -0.868693 -0.225208 -0.569781 -1.323778   \n",
       "\n",
       "            X30       X18       X46       X35       X37       X17       X42  \\\n",
       "0      0.873301  2.270459  2.912388  0.728549 -1.295910  2.275724 -0.359744   \n",
       "1      0.900408  2.251683  2.831043  0.757081 -1.302729  2.256918 -0.368216   \n",
       "2      0.893682  2.165409  2.656538  0.750002 -1.334065  2.170502 -0.407147   \n",
       "3      1.144550  2.601021  3.586176  1.014063 -1.175844  2.588606 -0.210578   \n",
       "4     -0.469419  2.107699  2.530198  0.790754 -1.355026  2.112698 -0.433188   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995 -0.530427  0.405426  0.159104 -0.021451  0.626506  0.407638  1.068273   \n",
       "89996 -0.289078 -0.589598 -0.636953 -1.152731  1.088975 -0.589015  0.058042   \n",
       "89997 -0.446876  0.337099  0.088322  0.002791  0.614071  0.339199  1.086706   \n",
       "89998 -0.256901 -0.631227 -0.663933 -1.067223  1.077085 -0.630712  0.132091   \n",
       "89999 -0.376795  0.270913  0.025022  0.023959  0.602033  0.272905  1.102766   \n",
       "\n",
       "            X38       X43       X47        X0       X52       X48       X41  \\\n",
       "0     -0.363988 -0.717875  2.265473  1.019072  0.721157  2.260484 -1.834228   \n",
       "1     -0.372547 -0.725350  2.242051  1.084322  1.273463  2.239054 -1.836103   \n",
       "2     -0.411875 -0.759699  2.150257  0.909735  0.858274  2.152557 -1.844565   \n",
       "3     -0.213300 -0.586264  2.586420  1.452937  1.030784  2.588699 -1.799160   \n",
       "4     -0.438182 -0.782676  2.091026  0.925432  0.645674  2.091058 -1.850081   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995  0.466155  0.542083  0.404342  0.637946 -0.699337  0.402187  1.206836   \n",
       "89996  0.642626 -0.349257 -0.591799 -0.636469 -1.493630 -0.593950  0.298781   \n",
       "89997  0.536077  0.558347  0.335925  0.281215 -0.787438  0.333698  1.190530   \n",
       "89998  0.671236 -0.283923 -0.633482 -0.834242 -1.611994 -0.635652  0.296660   \n",
       "89999  0.594978  0.572517  0.269653  0.469417 -0.918191  0.267422  1.174767   \n",
       "\n",
       "             X2        X3       X25       X23        X9       X27       X14  \\\n",
       "0      2.254474  2.254450  3.042155  2.259637  2.946272  2.258885  3.401480   \n",
       "1      2.235786  2.235849  2.995526  2.240825  2.901227  2.240080  3.359956   \n",
       "2      2.150014  2.149977  2.786986  2.154385  2.734335  2.153668  3.172415   \n",
       "3      2.583419  2.583483  3.941371  2.590836  3.676733  2.589976  4.174776   \n",
       "4      2.092565  2.092535  2.652600  2.096564  2.618092  2.095867  2.443792   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995  0.403753  0.403796  0.111838  0.409407  0.165668  0.412404  0.118302   \n",
       "89996 -0.592508 -0.592574 -0.619011 -0.584295 -0.627197 -0.587219 -0.635496   \n",
       "89997  0.335457  0.335491  0.047725  0.340944  0.098978  0.343962  0.054459   \n",
       "89998 -0.634312 -0.634379 -0.642176 -0.626008 -0.652879 -0.628916 -0.660001   \n",
       "89999  0.269286  0.269333 -0.012181  0.274627  0.035641  0.277665 -0.005518   \n",
       "\n",
       "            X19       X24       X16       X29       X26       X28       X21  \\\n",
       "0      3.345012  2.937407  3.343820  2.935077  2.941428  2.259203  2.938547   \n",
       "1      3.304083  2.898066  3.302884  2.895731  2.902085  2.240400  2.910457   \n",
       "2      3.119236  2.720591  3.118004  2.718235  2.724605  2.154003  2.731983   \n",
       "3      4.107204  3.672950  4.106144  3.670702  3.676991  2.590239  3.685219   \n",
       "4      2.998522  2.604873  2.997269  2.602504  2.608884  2.096211  2.615890   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995  0.112101  0.166753  0.107222  0.167445  0.164019  0.409890  0.160662   \n",
       "89996 -0.635929 -0.626819 -0.635882 -0.631618 -0.629097 -0.586609 -0.630493   \n",
       "89997  0.049077  0.099541  0.044285  0.100122  0.097013  0.341461  0.093592   \n",
       "89998 -0.660042 -0.652617 -0.660039 -0.657376 -0.654767 -0.628299 -0.656370   \n",
       "89999 -0.010130  0.036400 -0.014841  0.036875  0.034066  0.275177  0.031052   \n",
       "\n",
       "             X1       X11       X20       X22        X5        X4        X6  \\\n",
       "0      2.469231  2.945955  3.044228  2.259368  3.073369  2.943574  2.945749   \n",
       "1      2.446440  2.903884  3.007216  2.240566  3.004351  2.907020  2.906517   \n",
       "2      2.340769  2.729945  2.798645  2.154173  2.832534  2.727687  2.728807   \n",
       "3      2.879247  3.681201  3.949020  2.590387  3.953846  3.682269  3.681686   \n",
       "4      2.270373  2.613631  2.663541  2.096384  2.693932  2.611509  2.613064   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89995  0.327742  0.165710  0.112724  0.409590  0.121034  0.165462  0.165721   \n",
       "89996 -0.639392 -0.627132 -0.614351 -0.586671 -0.614544 -0.626798 -0.627063   \n",
       "89997  0.256660  0.098853  0.049408  0.341165  0.057471  0.098176  0.098726   \n",
       "89998 -0.676444 -0.652756 -0.637118 -0.628359 -0.636736 -0.652425 -0.652732   \n",
       "89999  0.188415  0.035605 -0.009769  0.274885 -0.003718  0.035665  0.035834   \n",
       "\n",
       "            X12        X7       X10        X8       X62    TARGET  \n",
       "0      2.259507  2.259501  3.071520  2.259504 -1.582156  0.013314  \n",
       "1      2.240683  2.240706  3.025257  2.240732 -1.584404 -0.000448  \n",
       "2      2.154322  2.154294  2.816114  2.154304 -1.594548  0.000244  \n",
       "3      2.590519  2.590540  3.971859  2.590523 -1.540117 -0.000628  \n",
       "4      2.096538  2.096525  2.682193  2.096533 -1.601161  0.003811  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "89995  0.409705  0.409703  0.115440  0.409704  0.103760  0.001185  \n",
       "89996 -0.586522 -0.586520 -0.611206 -0.586518  0.270911 -0.000382  \n",
       "89997  0.341287  0.341285  0.052269  0.341284  0.162217 -0.000313  \n",
       "89998 -0.628208 -0.628205 -0.633950 -0.628206  0.293136  0.001793  \n",
       "89999  0.274997  0.274996 -0.006977  0.275004  0.210972 -0.001421  \n",
       "\n",
       "[90000 rows x 55 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18b19d7-31bb-4904-8c99-fd0c206e925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame()\n",
    "\n",
    "for i in range(train.shape[1]-1):\n",
    "    for j in range(train.shape[1]-1):\n",
    "        train_[str(i)+'_'+str(j)+'_feat'] = train.iloc[:, i] * train.iloc[:, j]\n",
    "        \n",
    "train_target = pd.DataFrame(train['TARGET'])\n",
    "\n",
    "train = train.drop(['TARGET'], axis = 1)\n",
    "\n",
    "train = pd.concat([train, train_], axis = 1)\n",
    "\n",
    "train = (train-train.mean())/train.std()\n",
    "\n",
    "train['TARGET'] = train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d0774cd-7482-41db-977f-13a02a109d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.DataFrame()\n",
    "\n",
    "for i in range(test.shape[1]-1):\n",
    "    for j in range(test.shape[1]-1):\n",
    "        test_[str(i)+'_'+str(j)+'_feat'] = test.iloc[:, i] * test.iloc[:, j]\n",
    "        \n",
    "test_target = pd.DataFrame(test['TARGET'])\n",
    "\n",
    "test = test.drop(['TARGET'], axis = 1)\n",
    "\n",
    "test = pd.concat([test, test_], axis = 1)\n",
    "\n",
    "test = (test-test.mean())/test.std()\n",
    "\n",
    "test['TARGET'] = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91132f5e-e1fc-4108-b2c0-ed422db1125f",
   "metadata": {},
   "source": [
    "## 5.5 Autoencoder Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89088448-dca9-42d3-a9c8-916d1ff6a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = train.drop(['TARGET'], axis=1).to_numpy()\n",
    "output_features = pd.DataFrame((np.sign(train['TARGET'])+1)//2).to_numpy()\n",
    "# output_features = train['TARGET'].to_numpy()\n",
    "\n",
    "X_test = test.drop(['TARGET'], axis=1).to_numpy()\n",
    "Y_test = pd.DataFrame((np.sign(test['TARGET'])+1)//2).to_numpy()\n",
    "# Y_test = test['TARGET'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40a6de18-80b7-4545-a69b-87d29fc9dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_mean = np.mean(input_features, axis = 1)\n",
    "input_features_mean = input_features_mean.reshape((input_features_mean.shape[0], 1))\n",
    "input_features = input_features- input_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8baf52a4-ca84-4b07-9019-223f6adaea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mean = np.mean(X_test, axis = 1)\n",
    "X_test_mean = X_test_mean.reshape((X_test_mean.shape[0], 1))\n",
    "X_test = X_test - X_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e90eb95-282e-415e-a6bb-81d93114b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (90000, 2970)\n",
      "Validation X shape: (90000, 2970)\n",
      "Test X shape: (19707, 2970)\n",
      "Train Y shape: (90000, 1)\n",
      "Val Y shape: (90000, 1)\n",
      "Test Y shape: (19707, 1)\n",
      "train_max: 195.6927566509\n",
      "val_max: 195.6927566509\n",
      "test_max: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_val, Y_train, Y_val = train_test_split(input_features, output_features, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_val = input_features\n",
    "Y_train = Y_val = output_features\n",
    "\n",
    "####\n",
    "# train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train_data = val_data = train\n",
    "test_data = test\n",
    "####\n",
    "\n",
    "auto_train_max = np.sum(train_data['TARGET'][train_data['TARGET']>0])\n",
    "auto_val_max = np.sum(val_data['TARGET'][val_data['TARGET']>0])\n",
    "auto_test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Train X shape:', X_train.shape)\n",
    "print('Validation X shape:', X_val.shape)\n",
    "print('Test X shape:', X_test.shape)\n",
    "\n",
    "print('Train Y shape:', Y_train.shape)\n",
    "print('Val Y shape:', Y_val.shape)\n",
    "print('Test Y shape:', Y_test.shape)\n",
    "\n",
    "print('train_max:', auto_train_max)\n",
    "print('val_max:', auto_val_max)\n",
    "print('test_max:', auto_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f7a19ea-d496-4ee3-b66d-7f33a40361be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_feature: 2970\n",
      "output_feature: 1\n"
     ]
    }
   ],
   "source": [
    "train_input = torch.from_numpy(X_train)\n",
    "train_output = torch.from_numpy(Y_train)\n",
    "val_input = torch.from_numpy(X_val)\n",
    "val_output = torch.from_numpy(Y_val)\n",
    "test_input = torch.from_numpy(X_test)\n",
    "test_output = torch.from_numpy(Y_test)\n",
    "\n",
    "# train_input = torch.unsqueeze(train_input, 1)\n",
    "# val_input = torch.unsqueeze(val_input, 1)\n",
    "# test_input = torch.unsqueeze(test_input, 1)\n",
    "\n",
    "train_input = train_input.float()\n",
    "train_output = train_output.float()\n",
    "val_input = val_input.float()\n",
    "val_output = val_output.float()\n",
    "test_input = test_input.float()\n",
    "test_output = test_output.float()\n",
    "\n",
    "input_feature = train_input.shape[1]\n",
    "output_feature = 1\n",
    "\n",
    "print('input_feature:', input_feature)\n",
    "print('output_feature:', output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b5591cb-674f-4288-9119-9b6c5ee1b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.to(device)\n",
    "train_output = train_output.to(device)\n",
    "val_input = val_input.to(device)\n",
    "val_output = val_output.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_output = test_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "852276a1-d972-4d87-8724-e9c1c973513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9a0344a-7b96-48ff-852a-999ea40a6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_feature, input_feature*2)\n",
    "        self.linear2 = nn.Linear(input_feature*2, input_feature//16)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        self.linear = nn.Linear(input_feature//16, 2)\n",
    "        \n",
    "        # nn.init.constant_(self.linear1.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear2.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear3.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear4.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear.weight, 0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "#         x = self.batchnorm_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "#         x = self.batchnorm_2(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        output = self.linear(x)\n",
    "                \n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22bb31c5-cbce-4611-ab4a-3f33902bdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "train_ds = TensorDataset(train_input, train_output)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed2d7429-de64-4f06-8610-93c27155281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path):\n",
    "    best_loss = float('inf')\n",
    "    train_pred_output = []\n",
    "    val_pred_output = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    test_error = []\n",
    "    epochs = []\n",
    "    \n",
    "    train_returns = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "    \n",
    "    train_sum = []\n",
    "    val_sum = []\n",
    "    test_sum = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_dl:\n",
    "            model = model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            # y = torch.reshape(y, (y.shape[0], 1))\n",
    "            loss = loss_fn(pred, y.long().squeeze())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            \n",
    "            train_pred = model(train_input)\n",
    "            train_output_index = (torch.sign(train_output)+1)//2\n",
    "            train_pred_index = (torch.sign(train_pred)+1)//2\n",
    "            train_output = torch.reshape(train_output, (train_output_index.shape[0], 1))\n",
    "            # train_loss = loss_fn(train_output, train_pred)\n",
    "            train_loss = loss_fn(train_pred, train_output.long().squeeze())\n",
    "            train_loss = train_loss.cpu().detach().numpy()\n",
    "            \n",
    "            val_pred = model(val_input)\n",
    "            val_pred_index = (torch.sign(val_pred)+1)//2\n",
    "            val_output = torch.reshape(val_output, (val_output.shape[0], 1))\n",
    "            # val_loss = loss_fn(val_output, val_pred)\n",
    "            val_loss = loss_fn(val_pred, val_output.long().squeeze())\n",
    "            val_loss = val_loss.cpu().detach().numpy()\n",
    "        \n",
    "            test_pred = model(test_input)\n",
    "            test_pred_index = (torch.sign(test_pred)+1)//2\n",
    "            test_output = torch.reshape(test_output, (test_output.shape[0], 1))\n",
    "            # test_loss = loss_fn(test_output, test_pred)\n",
    "            test_loss = loss_fn(test_pred, test_output.long().squeeze())\n",
    "            test_loss = test_loss.cpu().detach().numpy()\n",
    "    \n",
    "            epochs.append(epoch)\n",
    "            train_error.append(math.log(train_loss+1))\n",
    "            val_error.append(math.log(val_loss+1))\n",
    "            test_error.append(math.log(test_loss+1))\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 2, figsize = (20, 7))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 4, figsize = (22, 5))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # train_conf = confusion_matrix(train_output.cpu().detach().numpy(), train_pred_index.cpu().detach().numpy())\n",
    "#             g1 = sns.heatmap(train_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[0], annot = True)\n",
    "#             g1.set_ylabel('True Target')\n",
    "#             g1.set_xlabel('Predict Target')\n",
    "#             g1.set_title('Train dataset')\n",
    "\n",
    "#             plt.grid(False)\n",
    "            # val_conf = confusion_matrix(val_output.cpu().detach().numpy(), val_pred_index.cpu().detach().numpy())\n",
    "#             g2 = sns.heatmap(val_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[1], annot = True)\n",
    "#             g2.set_ylabel('True Target')\n",
    "#             g2.set_xlabel('Predict Target')\n",
    "#             g2.set_title('Val dataset')\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # test_conf = confusion_matrix(test_output.cpu().detach().numpy(), test_pred_index.cpu().detach().numpy())\n",
    "#             g3 = sns.heatmap(test_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[2], annot = True)\n",
    "#             g3.set_ylabel('True Target')\n",
    "#             g3.set_xlabel('Predict Target')\n",
    "#             g3.set_title('Test dataset')\n",
    "            \n",
    "            \n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            train_pred_np = torch.argmax(softmax(train_pred), 1)\n",
    "            val_pred_np = torch.argmax(softmax(val_pred), 1)\n",
    "            test_pred_np = torch.argmax(softmax(test_pred), 1)\n",
    "            # print(train_pred_np)\n",
    "                            \n",
    "            # train_pred_np = train_pred_index.cpu().detach().numpy()\n",
    "            train_output_np = train_output.cpu().detach().numpy()\n",
    "            # val_pred_np = val_pred_index.cpu().detach().numpy()\n",
    "            val_output_np = val_output.cpu().detach().numpy()\n",
    "            # test_pred_np = test_pred_index.cpu().detach().numpy()\n",
    "            test_output_np = test_output.cpu().detach().numpy()\n",
    "            \n",
    "#             train_max_value = max(max(train_output_np), max(train_pred_np))\n",
    "#             train_min_value = min(min(train_output_np), min(train_pred_np))\n",
    "#             val_max_value = max(max(val_output_np), max(val_pred_np))\n",
    "#             val_min_value = min(min(val_output_np), min(val_pred_np))\n",
    "#             test_max_value = max(max(test_output_np), max(test_pred_np))\n",
    "#             test_min_value = min(min(test_output_np), min(test_pred_np))\n",
    "            \n",
    "#             ax[0].scatter(train_output_np, train_pred_np, s = 20, alpha=0.3, c='blue')\n",
    "#             ax[1].scatter(val_output_np, val_pred_np, s = 20, alpha=0.3, c='red')\n",
    "#             ax[2].scatter(test_output_np, test_pred_np, s = 20, alpha=0.3, c='green')\n",
    "            \n",
    "#             ax[0].plot(epochs, train_error, c='blue')\n",
    "#             ax[0].plot(epochs, val_error, c='red')\n",
    "#             ax[0].plot(epochs, test_error, c='green')\n",
    "#             ax[0].set_title('Errors vs Epochs', fontsize=15)\n",
    "#             ax[0].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[0].set_ylabel('Errors', fontsize=10)\n",
    "\n",
    "#             ax[0].legend(['train', 'valid', 'test'])\n",
    "            \n",
    "#             ax[0].set_xlim([train_min_value, train_max_value])\n",
    "#             ax[0].set_ylim([train_min_value, train_max_value])\n",
    "#             ax[0].set_title('Trainig data', fontsize=15)\n",
    "#             ax[0].set_xlabel('Target', fontsize=10)\n",
    "#             ax[0].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[0].plot([train_min_value, train_max_value], [train_min_value, train_max_value], 'k-')\n",
    "            \n",
    "#             ax[1].set_xlim([val_min_value, val_max_value])\n",
    "#             ax[1].set_ylim([val_min_value, val_max_value])\n",
    "#             ax[1].set_title('Validation data', fontsize=15)\n",
    "#             ax[1].set_xlabel('Target', fontsize=10)\n",
    "#             ax[1].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[1].plot([val_min_value, val_max_value], [val_min_value, val_max_value], 'k-')\n",
    "            \n",
    "#             ax[2].set_xlim([test_min_value, test_max_value])\n",
    "#             ax[2].set_ylim([test_min_value, test_max_value])\n",
    "#             ax[2].set_title('Testing data', fontsize=15)\n",
    "#             ax[2].set_xlabel('Target', fontsize=10)\n",
    "#             ax[2].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[2].plot([test_min_value, test_max_value], [test_min_value, test_max_value], 'k-')\n",
    "            \n",
    "#             ax[3].plot(epochs, train_error, c='blue')\n",
    "#             ax[3].plot(epochs, val_error, c='red')\n",
    "#             ax[3].plot(epochs, test_error, c='green')\n",
    "#             ax[3].set_title('Training and Validation error', fontsize=15)\n",
    "#             ax[3].set_xlabel('Epochs', fontsize=10)\n",
    "#             ax[3].set_ylabel('MSE error', fontsize=10)\n",
    "            \n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "#             print('Epoch ', epoch, 'Train_loss: ', train_loss*1000, ' Validation_loss: ', val_loss*100, ' Test_loss: ', test_loss*100)\n",
    "            # print(train_pred_np.shape, train_pred_np)\n",
    "            # print(train_pred, train_pred_np)\n",
    "            # train_pred_np = np.squeeze(train_pred_np)\n",
    "            # val_pred_np = np.squeeze(val_pred_np)\n",
    "            # test_pred_np = np.squeeze(test_pred_np)\n",
    "            train_pred_np = train_pred_np.cpu().detach().numpy()\n",
    "            val_pred_np = val_pred_np.cpu().detach().numpy()\n",
    "            test_pred_np = test_pred_np.cpu().detach().numpy()\n",
    "            \n",
    "            train_res = np.sum(train_data['TARGET'][train_pred_np>0])\n",
    "            train_output_check = np.squeeze(train_output_np)\n",
    "            train_check = np.sum(train_data['TARGET'][train_output_check>0])\n",
    "            \n",
    "            val_res = np.sum(val_data['TARGET'][val_pred_np>0])\n",
    "            val_output_check = np.squeeze(val_output_np)\n",
    "            val_check = np.sum(val_data['TARGET'][val_output_check>0])\n",
    "            \n",
    "            test_res = np.sum(test_data['TARGET'][test_pred_np>0])\n",
    "            test_output_check = np.squeeze(test_output_np)\n",
    "            test_check = np.sum(test_data['TARGET'][test_output_check>0])\n",
    "            \n",
    "#             train_returns.append(train_res)\n",
    "#             val_returns.append(val_res)\n",
    "#             test_returns.append(test_res)\n",
    "            \n",
    "#             ax[1].plot(epochs, train_returns, c='blu`e')\n",
    "#             ax[1].plot(epochs, val_returns, c='red')\n",
    "#             ax[1].plot(epochs, test_returns, c='green')\n",
    "#             ax[1].legend(['train', 'valid', 'test'])\n",
    "#             ax[1].set_title('Return vs Epochs', fontsize=15)\n",
    "#             ax[1].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[1].set_ylabel('Returns', fontsize=10)\n",
    "\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "            train_sum.append(train_res)\n",
    "            val_sum.append(val_res)\n",
    "            test_sum.append(test_res)\n",
    "            # print(f'Checks: {train_check/auto_train_max*100}%, {val_check/auto_val_max*100}%, {test_check/auto_test_max*100}%')\n",
    "#             print(f'Maximum sum train return {train_res}, Total train return: {auto_train_max}, Maximum train percentage return: {train_res/auto_train_max*100}%')\n",
    "#             print(f'Maximum sum train return {val_res}, Total train return: {auto_val_max}, Maximum train percentage return: {val_res/auto_val_max*100}%')\n",
    "#             print(f'Maximum sum test return {test_res}, Total test return: {auto_test_max}, Maximum test percentage return: {test_res/auto_test_max*100}%')\n",
    "#             print('Epoch:', epoch, 'Train loss:', train_loss, 'Val loss:', val_loss, 'Test loss:', test_loss)\n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss}, Train return: {train_res/auto_train_max*100}%, Val loss: {val_loss}, Val return: {val_res/auto_val_max*100}%, Test loss: {test_loss}, Test return: {test_res/auto_test_max*100}%')\n",
    "            # print(np.squeeze(train_output.cpu().detach().numpy()))\n",
    "            # print(train_pred_np)\n",
    "            # print(confusion_matrix(np.squeeze(train_output.cpu().detach().numpy()), train_pred_np))\n",
    "            # print(confusion_matrix(np.squeeze(val_output.cpu().detach().numpy()), val_pred_np))\n",
    "            # print(confusion_matrix(np.squeeze(test_output.cpu().detach().numpy()), test_pred_np))\n",
    "            # print(train_conf)\n",
    "            # print(val_conf)\n",
    "            # print(test_conf)\n",
    "            # print(train_output, train_pred)\n",
    "            # if val_loss < best_loss:\n",
    "            #     torch.save(model.state_dict(), model_path)\n",
    "            #     best_loss = val_loss\n",
    "                \n",
    "#             train_pred_output.append([train_pred.cpu().detach().numpy(), train_output.cpu().detach().numpy()])\n",
    "#             val_pred_output.append([val_pred.cpu().detach().numpy(), val_output.cpu().detach().numpy()])\n",
    "    return train_sum, val_sum, test_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "966e1596-f5a6-4719-8056-f538126e72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.6955394744873047, Train return: -1.0641574543379546%, Val loss: 0.6955394744873047, Val return: -1.0641574543379546%, Test loss: 0.6950516700744629, Test return: -0.6055377436664738%\n",
      "Epoch: 500, Train loss: 0.6917592287063599, Train return: 2.9345145906674532%, Val loss: 0.6917592287063599, Val return: 2.9345145906674532%, Test loss: 0.6930858492851257, Test return: 0.26887408046624756%\n",
      "Epoch: 1000, Train loss: 0.6910677552223206, Train return: 4.214865951944307%, Val loss: 0.6910677552223206, Val return: 4.214865951944307%, Test loss: 0.6930437684059143, Test return: -0.07565629977353451%\n",
      "Epoch: 1500, Train loss: 0.6904795169830322, Train return: 4.841359555786318%, Val loss: 0.6904795169830322, Val return: 4.841359555786318%, Test loss: 0.6930810213088989, Test return: -0.7782185505502005%\n",
      "Epoch: 2000, Train loss: 0.6899368762969971, Train return: 5.569457669576901%, Val loss: 0.6899368762969971, Val return: 5.569457669576901%, Test loss: 0.6931464672088623, Test return: -0.839374163279355%\n",
      "Epoch: 2500, Train loss: 0.6894224286079407, Train return: 6.14252224855903%, Val loss: 0.6894224286079407, Val return: 6.14252224855903%, Test loss: 0.6932612061500549, Test return: -0.53153564287493%\n",
      "Epoch: 3000, Train loss: 0.6889228224754333, Train return: 6.934383000852681%, Val loss: 0.6889228224754333, Val return: 6.934383000852681%, Test loss: 0.6933994889259338, Test return: -0.7844938269830428%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-aa409c53d256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sum_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_path_seurat_classsify'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-2d167e686c30>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "learning_rate = 0.001\n",
    "# loss_fn = F.mse_loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_seurat_classsify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2e089-815d-4596-ab7f-c16d8ca96272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
