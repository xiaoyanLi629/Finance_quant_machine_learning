{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c70773-809f-470a-a9a4-b5907273e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython import display\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8b0a34-3d3c-4a77-8a53-a01326ec1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a475f5-7b98-40da-be68-1770720796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22db1e1e-0226-489b-8e26-805b8c8d23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = pd.read_csv('./integrate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff9203-6526-4815-a36c-c9df9dcca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6eac3f-02b5-4c75-bca7-7e266b51af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat_std = train_test_seurat.std()\n",
    "column_names = list(train_test_seurat.columns)\n",
    "columns_remove = []\n",
    "for i in range(train_test_seurat.shape[1]):\n",
    "    if train_test_seurat_std[i] == 0:\n",
    "        columns_remove.append(column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6a640d-17ca-4197-9120-b68b3a8d8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.drop(columns_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0866b5b-8359-4a8d-aa6d-e3ae91eb6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat[columns_remove[0]] = train_test_seurat.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc71a28-7056-4114-b6cf-fd9f9454edb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109707, 54)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_seurat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1f1207-e8b6-4503-8c77-d2edcd5e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seurat = train_test_seurat.iloc[:90000, :]\n",
    "test_seurat = train_test_seurat.iloc[90000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9614e-73b8-4b4d-8ab0-7a7ca7ab07d1",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e7a64-bad8-4cf8-8ad0-00cd3ccf1f8b",
   "metadata": {},
   "source": [
    "# 1. Load data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75a9ad-7160-472c-9860-cd8b46ba1ac2",
   "metadata": {},
   "source": [
    "## 1.1 Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5f81e6-1e1c-4e7a-b3b3-9adfaa705711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./MLR_Project_train.csv')\n",
    "test = pd.read_csv('./MLR_Project_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0ca5-9892-4287-8e9d-41a273c0e932",
   "metadata": {},
   "source": [
    "Show the data format and dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d204a102-98ba-4502-bc62-b2219ccbba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5874.524387</td>\n",
       "      <td>1072.671848</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41440.23732</td>\n",
       "      <td>70.405148</td>\n",
       "      <td>7.392780</td>\n",
       "      <td>70.377281</td>\n",
       "      <td>23229.69262</td>\n",
       "      <td>23229.72655</td>\n",
       "      <td>70.378864</td>\n",
       "      <td>7.389173</td>\n",
       "      <td>70.380160</td>\n",
       "      <td>23229.76782</td>\n",
       "      <td>23379.81637</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.425618</td>\n",
       "      <td>70.494241</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>4898.757333</td>\n",
       "      <td>15165.92759</td>\n",
       "      <td>297487.1654</td>\n",
       "      <td>297487.16540</td>\n",
       "      <td>15165.92759</td>\n",
       "      <td>4898.757333</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>64.192982</td>\n",
       "      <td>20.940618</td>\n",
       "      <td>76.070270</td>\n",
       "      <td>23376.73707</td>\n",
       "      <td>23343.13291</td>\n",
       "      <td>77.290965</td>\n",
       "      <td>347.308164</td>\n",
       "      <td>102.380501</td>\n",
       "      <td>24823.08137</td>\n",
       "      <td>24120.94894</td>\n",
       "      <td>332.757607</td>\n",
       "      <td>17.386711</td>\n",
       "      <td>129.622187</td>\n",
       "      <td>23936.99077</td>\n",
       "      <td>21670.19233</td>\n",
       "      <td>71.518948</td>\n",
       "      <td>11.399004</td>\n",
       "      <td>78.006816</td>\n",
       "      <td>26437.161240</td>\n",
       "      <td>23811.09670</td>\n",
       "      <td>141.997532</td>\n",
       "      <td>22.474794</td>\n",
       "      <td>0.013314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6124.154099</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41442.22458</td>\n",
       "      <td>70.456758</td>\n",
       "      <td>7.356050</td>\n",
       "      <td>70.379576</td>\n",
       "      <td>23229.76020</td>\n",
       "      <td>23230.10472</td>\n",
       "      <td>70.273618</td>\n",
       "      <td>7.389813</td>\n",
       "      <td>70.329906</td>\n",
       "      <td>23229.46908</td>\n",
       "      <td>23384.98219</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.441223</td>\n",
       "      <td>70.702624</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5100.350569</td>\n",
       "      <td>15788.07683</td>\n",
       "      <td>308790.4312</td>\n",
       "      <td>308790.43120</td>\n",
       "      <td>15788.07683</td>\n",
       "      <td>5100.350569</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>2.937218</td>\n",
       "      <td>-14.428210</td>\n",
       "      <td>75.204973</td>\n",
       "      <td>23317.46049</td>\n",
       "      <td>23309.42032</td>\n",
       "      <td>74.880368</td>\n",
       "      <td>689.670872</td>\n",
       "      <td>127.070357</td>\n",
       "      <td>36746.26762</td>\n",
       "      <td>31012.79786</td>\n",
       "      <td>-310.576721</td>\n",
       "      <td>4.532673</td>\n",
       "      <td>-47.045260</td>\n",
       "      <td>22053.23653</td>\n",
       "      <td>14626.73339</td>\n",
       "      <td>48.124991</td>\n",
       "      <td>-99.618253</td>\n",
       "      <td>-115.120518</td>\n",
       "      <td>7705.543821</td>\n",
       "      <td>22665.35143</td>\n",
       "      <td>-377.287072</td>\n",
       "      <td>-73.700375</td>\n",
       "      <td>-0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5905.732593</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41443.14358</td>\n",
       "      <td>41442.30403</td>\n",
       "      <td>70.422472</td>\n",
       "      <td>7.417794</td>\n",
       "      <td>70.376448</td>\n",
       "      <td>23229.48142</td>\n",
       "      <td>23229.61008</td>\n",
       "      <td>70.474265</td>\n",
       "      <td>7.388979</td>\n",
       "      <td>70.397301</td>\n",
       "      <td>23229.83396</td>\n",
       "      <td>23387.39168</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.440201</td>\n",
       "      <td>70.686178</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5308.548086</td>\n",
       "      <td>16430.60795</td>\n",
       "      <td>320463.9968</td>\n",
       "      <td>320463.99680</td>\n",
       "      <td>16430.60795</td>\n",
       "      <td>5308.548086</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>141.231442</td>\n",
       "      <td>25.855392</td>\n",
       "      <td>75.184756</td>\n",
       "      <td>23248.26780</td>\n",
       "      <td>23307.91084</td>\n",
       "      <td>74.635569</td>\n",
       "      <td>634.523047</td>\n",
       "      <td>71.705965</td>\n",
       "      <td>28917.00549</td>\n",
       "      <td>24632.17456</td>\n",
       "      <td>419.071308</td>\n",
       "      <td>7.403187</td>\n",
       "      <td>118.846496</td>\n",
       "      <td>23430.24573</td>\n",
       "      <td>31251.55292</td>\n",
       "      <td>71.535567</td>\n",
       "      <td>53.482719</td>\n",
       "      <td>106.179152</td>\n",
       "      <td>37586.677270</td>\n",
       "      <td>23251.62576</td>\n",
       "      <td>261.098973</td>\n",
       "      <td>22.565621</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6029.325221</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41442.25682</td>\n",
       "      <td>70.458533</td>\n",
       "      <td>7.366031</td>\n",
       "      <td>70.379221</td>\n",
       "      <td>23230.08433</td>\n",
       "      <td>23229.87971</td>\n",
       "      <td>70.288944</td>\n",
       "      <td>7.390120</td>\n",
       "      <td>70.370247</td>\n",
       "      <td>23229.81662</td>\n",
       "      <td>23390.02296</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.439191</td>\n",
       "      <td>70.692085</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5510.265781</td>\n",
       "      <td>17053.14128</td>\n",
       "      <td>331774.2409</td>\n",
       "      <td>75710.89648</td>\n",
       "      <td>17053.14128</td>\n",
       "      <td>5510.265781</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>-1.580478</td>\n",
       "      <td>-7.499746</td>\n",
       "      <td>75.184756</td>\n",
       "      <td>23248.26836</td>\n",
       "      <td>23307.91084</td>\n",
       "      <td>74.635569</td>\n",
       "      <td>583.001082</td>\n",
       "      <td>70.384022</td>\n",
       "      <td>25642.94219</td>\n",
       "      <td>23482.26269</td>\n",
       "      <td>-249.671869</td>\n",
       "      <td>7.388974</td>\n",
       "      <td>49.809681</td>\n",
       "      <td>23193.67720</td>\n",
       "      <td>15867.01579</td>\n",
       "      <td>70.369496</td>\n",
       "      <td>-12.169114</td>\n",
       "      <td>63.930236</td>\n",
       "      <td>10052.351290</td>\n",
       "      <td>23229.64352</td>\n",
       "      <td>-10.549985</td>\n",
       "      <td>4.656636</td>\n",
       "      <td>-0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6142.360146</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41443.14358</td>\n",
       "      <td>41442.46480</td>\n",
       "      <td>70.413623</td>\n",
       "      <td>7.411287</td>\n",
       "      <td>70.376788</td>\n",
       "      <td>23229.70975</td>\n",
       "      <td>23229.82255</td>\n",
       "      <td>70.467206</td>\n",
       "      <td>7.389910</td>\n",
       "      <td>70.386986</td>\n",
       "      <td>23229.87603</td>\n",
       "      <td>23392.66710</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.438244</td>\n",
       "      <td>70.680386</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>1152.667997</td>\n",
       "      <td>17699.01445</td>\n",
       "      <td>343508.5253</td>\n",
       "      <td>343508.52530</td>\n",
       "      <td>17699.01445</td>\n",
       "      <td>5719.546213</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>149.144800</td>\n",
       "      <td>26.789196</td>\n",
       "      <td>74.904988</td>\n",
       "      <td>23229.77108</td>\n",
       "      <td>23260.32245</td>\n",
       "      <td>70.506980</td>\n",
       "      <td>566.549008</td>\n",
       "      <td>120.694073</td>\n",
       "      <td>25765.82131</td>\n",
       "      <td>24618.79392</td>\n",
       "      <td>363.188022</td>\n",
       "      <td>7.389057</td>\n",
       "      <td>78.826922</td>\n",
       "      <td>23235.86490</td>\n",
       "      <td>31790.06425</td>\n",
       "      <td>120.694068</td>\n",
       "      <td>42.971377</td>\n",
       "      <td>145.572170</td>\n",
       "      <td>37109.895810</td>\n",
       "      <td>24143.94971</td>\n",
       "      <td>188.639704</td>\n",
       "      <td>31.863254</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            0            1            2            3          4  \\\n",
       "0           0  5874.524387  1072.671848  41440.76212  41440.23732  70.405148   \n",
       "1           1  6124.154099  1072.802927  41440.76212  41442.22458  70.456758   \n",
       "2           2  5905.732593  1072.802927  41443.14358  41442.30403  70.422472   \n",
       "3           3  6029.325221  1072.802927  41440.76212  41442.25682  70.458533   \n",
       "4           4  6142.360146  1072.802927  41443.14358  41442.46480  70.413623   \n",
       "\n",
       "          5          6            7            8          9        10  \\\n",
       "0  7.392780  70.377281  23229.69262  23229.72655  70.378864  7.389173   \n",
       "1  7.356050  70.379576  23229.76020  23230.10472  70.273618  7.389813   \n",
       "2  7.417794  70.376448  23229.48142  23229.61008  70.474265  7.388979   \n",
       "3  7.366031  70.379221  23230.08433  23229.87971  70.288944  7.390120   \n",
       "4  7.411287  70.376788  23229.70975  23229.82255  70.467206  7.389910   \n",
       "\n",
       "          11           12           13         14         15         16  \\\n",
       "0  70.380160  23229.76782  23379.81637  83.418623  11.615135  83.418623   \n",
       "1  70.329906  23229.46908  23384.98219  83.418623  11.615135  83.418623   \n",
       "2  70.397301  23229.83396  23387.39168  83.418623  11.615135  83.418623   \n",
       "3  70.370247  23229.81662  23390.02296  83.418623  11.615135  83.418623   \n",
       "4  70.386986  23229.87603  23392.66710  70.572881  11.615135  83.418623   \n",
       "\n",
       "            17          18         19        20         21           22  \\\n",
       "0  23466.72590  23466.7259  83.418623  7.425618  70.494241  23229.77107   \n",
       "1  23466.72590  23466.7259  83.418623  7.441223  70.702624  23229.77107   \n",
       "2  23466.72590  23466.7259  83.418623  7.440201  70.686178  23229.77107   \n",
       "3  23233.34325  23466.7259  83.418623  7.439191  70.692085  23229.77107   \n",
       "4  23466.72590  23466.7259  83.418623  7.438244  70.680386  23229.77107   \n",
       "\n",
       "            23         24        25         26           27           28  \\\n",
       "0  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "1  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "2  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "3  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "4  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "\n",
       "          29           30           31           32            33  \\\n",
       "0  70.376262  4898.757333  15165.92759  297487.1654  297487.16540   \n",
       "1  70.376262  5100.350569  15788.07683  308790.4312  308790.43120   \n",
       "2  70.376262  5308.548086  16430.60795  320463.9968  320463.99680   \n",
       "3  70.376262  5510.265781  17053.14128  331774.2409   75710.89648   \n",
       "4  70.376262  1152.667997  17699.01445  343508.5253  343508.52530   \n",
       "\n",
       "            34           35         36           37           38         39  \\\n",
       "0  15165.92759  4898.757333  70.376262  23229.77107  23229.77107  70.376262   \n",
       "1  15788.07683  5100.350569  70.376262  23229.77107  23229.77107  70.376262   \n",
       "2  16430.60795  5308.548086  70.376262  23229.77107  23229.77107  70.376262   \n",
       "3  17053.14128  5510.265781  70.376262  23229.77107  23229.77107  70.376262   \n",
       "4  17699.01445  5719.546213  70.376262  23229.77107  23229.77107  70.376262   \n",
       "\n",
       "         40         41           42           43          44         45  \\\n",
       "0  7.389056  70.376262  23229.77107  23229.77107   64.192982  20.940618   \n",
       "1  7.389056  70.376262  23229.77107  23229.77107    2.937218 -14.428210   \n",
       "2  7.389056  70.376262  23229.77107  23229.77107  141.231442  25.855392   \n",
       "3  7.389056  70.376262  23229.77107  23229.77107   -1.580478  -7.499746   \n",
       "4  7.389056  70.376262  23229.77107  23229.77107  149.144800  26.789196   \n",
       "\n",
       "          46           47           48         49          50          51  \\\n",
       "0  76.070270  23376.73707  23343.13291  77.290965  347.308164  102.380501   \n",
       "1  75.204973  23317.46049  23309.42032  74.880368  689.670872  127.070357   \n",
       "2  75.184756  23248.26780  23307.91084  74.635569  634.523047   71.705965   \n",
       "3  75.184756  23248.26836  23307.91084  74.635569  583.001082   70.384022   \n",
       "4  74.904988  23229.77108  23260.32245  70.506980  566.549008  120.694073   \n",
       "\n",
       "            52           53          54         55          56           57  \\\n",
       "0  24823.08137  24120.94894  332.757607  17.386711  129.622187  23936.99077   \n",
       "1  36746.26762  31012.79786 -310.576721   4.532673  -47.045260  22053.23653   \n",
       "2  28917.00549  24632.17456  419.071308   7.403187  118.846496  23430.24573   \n",
       "3  25642.94219  23482.26269 -249.671869   7.388974   49.809681  23193.67720   \n",
       "4  25765.82131  24618.79392  363.188022   7.389057   78.826922  23235.86490   \n",
       "\n",
       "            58          59         60          61            62           63  \\\n",
       "0  21670.19233   71.518948  11.399004   78.006816  26437.161240  23811.09670   \n",
       "1  14626.73339   48.124991 -99.618253 -115.120518   7705.543821  22665.35143   \n",
       "2  31251.55292   71.535567  53.482719  106.179152  37586.677270  23251.62576   \n",
       "3  15867.01579   70.369496 -12.169114   63.930236  10052.351290  23229.64352   \n",
       "4  31790.06425  120.694068  42.971377  145.572170  37109.895810  24143.94971   \n",
       "\n",
       "           64         65    TARGET  \n",
       "0  141.997532  22.474794  0.013314  \n",
       "1 -377.287072 -73.700375 -0.000448  \n",
       "2  261.098973  22.565621  0.000244  \n",
       "3  -10.549985   4.656636 -0.000628  \n",
       "4  188.639704  31.863254  0.003811  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b677e07a-0a62-4e5f-921b-89861aa36a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16335.49448</td>\n",
       "      <td>1061.530132</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41237.30921</td>\n",
       "      <td>70.432910</td>\n",
       "      <td>7.379175</td>\n",
       "      <td>70.376460</td>\n",
       "      <td>23229.86828</td>\n",
       "      <td>23229.78910</td>\n",
       "      <td>70.397639</td>\n",
       "      <td>7.389850</td>\n",
       "      <td>70.389234</td>\n",
       "      <td>23229.90881</td>\n",
       "      <td>25602.51370</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>7.537712</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.703960</td>\n",
       "      <td>7.467846</td>\n",
       "      <td>70.433077</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23278.59091</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>7.537712</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23238.10616</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.703960</td>\n",
       "      <td>7133.700113</td>\n",
       "      <td>22063.32144</td>\n",
       "      <td>422799.6647</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>11589.95489</td>\n",
       "      <td>3740.045136</td>\n",
       "      <td>1860.710635</td>\n",
       "      <td>4138543.879</td>\n",
       "      <td>422799.6647</td>\n",
       "      <td>22063.32144</td>\n",
       "      <td>137.249904</td>\n",
       "      <td>66395.12033</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>258.079242</td>\n",
       "      <td>30.226219</td>\n",
       "      <td>75.913405</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.83301</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>1473.581679</td>\n",
       "      <td>71.758838</td>\n",
       "      <td>23874.69807</td>\n",
       "      <td>23499.73762</td>\n",
       "      <td>-155.498907</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.371005</td>\n",
       "      <td>23229.77106</td>\n",
       "      <td>12139.388420</td>\n",
       "      <td>71.758829</td>\n",
       "      <td>14.703740</td>\n",
       "      <td>84.826620</td>\n",
       "      <td>8035.667142</td>\n",
       "      <td>23254.88967</td>\n",
       "      <td>92.945298</td>\n",
       "      <td>12.071364</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13166.20458</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41238.66844</td>\n",
       "      <td>70.394428</td>\n",
       "      <td>7.378583</td>\n",
       "      <td>70.391576</td>\n",
       "      <td>23229.87463</td>\n",
       "      <td>23230.21191</td>\n",
       "      <td>70.288403</td>\n",
       "      <td>7.392921</td>\n",
       "      <td>70.343310</td>\n",
       "      <td>23229.64193</td>\n",
       "      <td>25607.58558</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.469469</td>\n",
       "      <td>70.576552</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>3945.011279</td>\n",
       "      <td>12222.51346</td>\n",
       "      <td>244010.9410</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>22695.88002</td>\n",
       "      <td>7338.666257</td>\n",
       "      <td>139.167102</td>\n",
       "      <td>1239716.334</td>\n",
       "      <td>244010.9410</td>\n",
       "      <td>12222.51346</td>\n",
       "      <td>792.472120</td>\n",
       "      <td>227216.19080</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>-147.808334</td>\n",
       "      <td>-20.702534</td>\n",
       "      <td>76.035317</td>\n",
       "      <td>23313.93717</td>\n",
       "      <td>23332.97615</td>\n",
       "      <td>75.852147</td>\n",
       "      <td>1828.336140</td>\n",
       "      <td>140.129854</td>\n",
       "      <td>37438.79508</td>\n",
       "      <td>30329.40335</td>\n",
       "      <td>167.006374</td>\n",
       "      <td>16.564437</td>\n",
       "      <td>34.929416</td>\n",
       "      <td>23264.53239</td>\n",
       "      <td>39398.972520</td>\n",
       "      <td>84.750309</td>\n",
       "      <td>60.805885</td>\n",
       "      <td>155.880284</td>\n",
       "      <td>41154.557460</td>\n",
       "      <td>24005.38063</td>\n",
       "      <td>199.782364</td>\n",
       "      <td>35.714646</td>\n",
       "      <td>-0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12897.58082</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41240.71985</td>\n",
       "      <td>41238.89069</td>\n",
       "      <td>70.476942</td>\n",
       "      <td>7.400935</td>\n",
       "      <td>70.375313</td>\n",
       "      <td>23229.84429</td>\n",
       "      <td>23229.89405</td>\n",
       "      <td>70.469979</td>\n",
       "      <td>7.391477</td>\n",
       "      <td>70.407134</td>\n",
       "      <td>23230.09254</td>\n",
       "      <td>25610.09095</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.470161</td>\n",
       "      <td>70.605117</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>995.797053</td>\n",
       "      <td>23323.37355</td>\n",
       "      <td>445692.4097</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>12850.00699</td>\n",
       "      <td>4148.336212</td>\n",
       "      <td>3120.762743</td>\n",
       "      <td>4161436.624</td>\n",
       "      <td>445692.4097</td>\n",
       "      <td>23323.37355</td>\n",
       "      <td>233.004088</td>\n",
       "      <td>67655.17244</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>289.867668</td>\n",
       "      <td>32.185249</td>\n",
       "      <td>75.966899</td>\n",
       "      <td>23326.54724</td>\n",
       "      <td>23327.60253</td>\n",
       "      <td>75.649862</td>\n",
       "      <td>1924.171252</td>\n",
       "      <td>111.147939</td>\n",
       "      <td>37691.96604</td>\n",
       "      <td>29511.34868</td>\n",
       "      <td>53.482595</td>\n",
       "      <td>13.727445</td>\n",
       "      <td>165.790143</td>\n",
       "      <td>24499.54560</td>\n",
       "      <td>7180.863302</td>\n",
       "      <td>74.919427</td>\n",
       "      <td>-22.153780</td>\n",
       "      <td>54.137349</td>\n",
       "      <td>6873.937564</td>\n",
       "      <td>23667.70306</td>\n",
       "      <td>74.616186</td>\n",
       "      <td>24.773579</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12887.42863</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41239.00250</td>\n",
       "      <td>70.412815</td>\n",
       "      <td>7.378490</td>\n",
       "      <td>70.377339</td>\n",
       "      <td>23229.91979</td>\n",
       "      <td>23229.86689</td>\n",
       "      <td>70.283757</td>\n",
       "      <td>7.389189</td>\n",
       "      <td>70.359007</td>\n",
       "      <td>23229.67957</td>\n",
       "      <td>25612.79171</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.469667</td>\n",
       "      <td>70.602493</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>4329.804019</td>\n",
       "      <td>13410.04591</td>\n",
       "      <td>265586.1421</td>\n",
       "      <td>88824.3188</td>\n",
       "      <td>23883.41247</td>\n",
       "      <td>7723.458997</td>\n",
       "      <td>1326.699554</td>\n",
       "      <td>1261291.535</td>\n",
       "      <td>265586.1421</td>\n",
       "      <td>13410.04591</td>\n",
       "      <td>1177.264861</td>\n",
       "      <td>228403.72320</td>\n",
       "      <td>455867.2494</td>\n",
       "      <td>455867.2494</td>\n",
       "      <td>-141.955445</td>\n",
       "      <td>-15.581110</td>\n",
       "      <td>75.883198</td>\n",
       "      <td>23230.50078</td>\n",
       "      <td>23310.57289</td>\n",
       "      <td>73.052689</td>\n",
       "      <td>1808.877007</td>\n",
       "      <td>123.263599</td>\n",
       "      <td>31327.33150</td>\n",
       "      <td>25876.72169</td>\n",
       "      <td>86.016065</td>\n",
       "      <td>7.327061</td>\n",
       "      <td>26.252334</td>\n",
       "      <td>22958.21980</td>\n",
       "      <td>39599.779840</td>\n",
       "      <td>122.820383</td>\n",
       "      <td>46.190366</td>\n",
       "      <td>145.600464</td>\n",
       "      <td>39883.925250</td>\n",
       "      <td>24179.10475</td>\n",
       "      <td>145.999434</td>\n",
       "      <td>26.920634</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11994.73779</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41240.71985</td>\n",
       "      <td>41238.81077</td>\n",
       "      <td>70.481341</td>\n",
       "      <td>7.392913</td>\n",
       "      <td>70.385266</td>\n",
       "      <td>23229.57329</td>\n",
       "      <td>23229.79297</td>\n",
       "      <td>70.450114</td>\n",
       "      <td>7.390389</td>\n",
       "      <td>70.377065</td>\n",
       "      <td>23229.74335</td>\n",
       "      <td>25615.17798</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.469671</td>\n",
       "      <td>70.607659</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>1408.908041</td>\n",
       "      <td>24598.30068</td>\n",
       "      <td>468855.4055</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>14124.93413</td>\n",
       "      <td>4561.447200</td>\n",
       "      <td>4395.689873</td>\n",
       "      <td>4184599.619</td>\n",
       "      <td>468855.4055</td>\n",
       "      <td>24598.30068</td>\n",
       "      <td>175.464314</td>\n",
       "      <td>68930.09957</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>276.189378</td>\n",
       "      <td>21.701184</td>\n",
       "      <td>76.055710</td>\n",
       "      <td>23354.81165</td>\n",
       "      <td>23345.67063</td>\n",
       "      <td>77.102636</td>\n",
       "      <td>1906.984098</td>\n",
       "      <td>598.329736</td>\n",
       "      <td>40066.97972</td>\n",
       "      <td>36345.65544</td>\n",
       "      <td>-605.228200</td>\n",
       "      <td>-132.958333</td>\n",
       "      <td>-529.575904</td>\n",
       "      <td>12869.21582</td>\n",
       "      <td>6244.418503</td>\n",
       "      <td>-23.787704</td>\n",
       "      <td>-46.168214</td>\n",
       "      <td>-57.984418</td>\n",
       "      <td>-6030.026819</td>\n",
       "      <td>13649.75983</td>\n",
       "      <td>-694.862277</td>\n",
       "      <td>-218.983324</td>\n",
       "      <td>-0.001462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            0            1            2            3          4  \\\n",
       "0           0  16335.49448  1061.530132  41238.33840  41237.30921  70.432910   \n",
       "1           1  13166.20458  1061.661211  41238.33840  41238.66844  70.394428   \n",
       "2           2  12897.58082  1061.661211  41240.71985  41238.89069  70.476942   \n",
       "3           3  12887.42863  1061.661211  41238.33840  41239.00250  70.412815   \n",
       "4           4  11994.73779  1061.661211  41240.71985  41238.81077  70.481341   \n",
       "\n",
       "          5          6            7            8          9        10  \\\n",
       "0  7.379175  70.376460  23229.86828  23229.78910  70.397639  7.389850   \n",
       "1  7.378583  70.391576  23229.87463  23230.21191  70.288403  7.392921   \n",
       "2  7.400935  70.375313  23229.84429  23229.89405  70.469979  7.391477   \n",
       "3  7.378490  70.377339  23229.91979  23229.86689  70.283757  7.389189   \n",
       "4  7.392913  70.385266  23229.57329  23229.79297  70.450114  7.390389   \n",
       "\n",
       "          11           12           13         14        15         16  \\\n",
       "0  70.389234  23229.90881  25602.51370  70.835039  7.537712  70.835039   \n",
       "1  70.343310  23229.64193  25607.58558  70.572881  7.452766  70.572881   \n",
       "2  70.407134  23230.09254  25610.09095  70.572881  7.580185  70.966118   \n",
       "3  70.359007  23229.67957  25612.79171  70.572881  7.452766  70.572881   \n",
       "4  70.377065  23229.74335  25615.17798  70.572881  7.580185  70.966118   \n",
       "\n",
       "            17           18         19        20         21           22  \\\n",
       "0  23235.72471  23235.72471  70.703960  7.467846  70.433077  23230.96180   \n",
       "1  23240.48762  23240.48762  70.966118  7.469469  70.576552  23230.96180   \n",
       "2  23233.34325  23233.34325  70.572881  7.470161  70.605117  23233.34325   \n",
       "3  23233.34325  23240.48762  70.966118  7.469667  70.602493  23230.96180   \n",
       "4  23233.34325  23233.34325  70.572881  7.469671  70.607659  23233.34325   \n",
       "\n",
       "            23         24        25         26           27           28  \\\n",
       "0  23278.59091  70.835039  7.537712  70.441802  23238.10616  23235.72471   \n",
       "1  23235.72471  70.572881  7.452766  70.572881  23280.97236  23240.48762   \n",
       "2  23280.97236  70.966118  7.580185  70.441802  23235.72471  23233.34325   \n",
       "3  23235.72471  70.572881  7.452766  70.572881  23280.97236  23240.48762   \n",
       "4  23280.97236  70.966118  7.580185  70.441802  23235.72471  23233.34325   \n",
       "\n",
       "          29           30           31           32           33           34  \\\n",
       "0  70.703960  7133.700113  22063.32144  422799.6647  232518.5574  11589.95489   \n",
       "1  70.966118  3945.011279  12222.51346  244010.9410  434292.0482  22695.88002   \n",
       "2  70.572881   995.797053  23323.37355  445692.4097  255411.3025  12850.00699   \n",
       "3  70.966118  4329.804019  13410.04591  265586.1421   88824.3188  23883.41247   \n",
       "4  70.572881  1408.908041  24598.30068  468855.4055  278574.2983  14124.93413   \n",
       "\n",
       "            35           36           37           38           39  \\\n",
       "0  3740.045136  1860.710635  4138543.879  422799.6647  22063.32144   \n",
       "1  7338.666257   139.167102  1239716.334  244010.9410  12222.51346   \n",
       "2  4148.336212  3120.762743  4161436.624  445692.4097  23323.37355   \n",
       "3  7723.458997  1326.699554  1261291.535  265586.1421  13410.04591   \n",
       "4  4561.447200  4395.689873  4184599.619  468855.4055  24598.30068   \n",
       "\n",
       "            40            41           42           43          44         45  \\\n",
       "0   137.249904   66395.12033  232518.5574  232518.5574  258.079242  30.226219   \n",
       "1   792.472120  227216.19080  434292.0482  434292.0482 -147.808334 -20.702534   \n",
       "2   233.004088   67655.17244  255411.3025  255411.3025  289.867668  32.185249   \n",
       "3  1177.264861  228403.72320  455867.2494  455867.2494 -141.955445 -15.581110   \n",
       "4   175.464314   68930.09957  278574.2983  278574.2983  276.189378  21.701184   \n",
       "\n",
       "          46           47           48         49           50          51  \\\n",
       "0  75.913405  23229.77107  23229.83301  70.376262  1473.581679   71.758838   \n",
       "1  76.035317  23313.93717  23332.97615  75.852147  1828.336140  140.129854   \n",
       "2  75.966899  23326.54724  23327.60253  75.649862  1924.171252  111.147939   \n",
       "3  75.883198  23230.50078  23310.57289  73.052689  1808.877007  123.263599   \n",
       "4  76.055710  23354.81165  23345.67063  77.102636  1906.984098  598.329736   \n",
       "\n",
       "            52           53          54          55          56           57  \\\n",
       "0  23874.69807  23499.73762 -155.498907    7.389056   70.371005  23229.77106   \n",
       "1  37438.79508  30329.40335  167.006374   16.564437   34.929416  23264.53239   \n",
       "2  37691.96604  29511.34868   53.482595   13.727445  165.790143  24499.54560   \n",
       "3  31327.33150  25876.72169   86.016065    7.327061   26.252334  22958.21980   \n",
       "4  40066.97972  36345.65544 -605.228200 -132.958333 -529.575904  12869.21582   \n",
       "\n",
       "             58          59         60          61            62           63  \\\n",
       "0  12139.388420   71.758829  14.703740   84.826620   8035.667142  23254.88967   \n",
       "1  39398.972520   84.750309  60.805885  155.880284  41154.557460  24005.38063   \n",
       "2   7180.863302   74.919427 -22.153780   54.137349   6873.937564  23667.70306   \n",
       "3  39599.779840  122.820383  46.190366  145.600464  39883.925250  24179.10475   \n",
       "4   6244.418503  -23.787704 -46.168214  -57.984418  -6030.026819  13649.75983   \n",
       "\n",
       "           64          65    TARGET  \n",
       "0   92.945298   12.071364  0.010438  \n",
       "1  199.782364   35.714646 -0.000532  \n",
       "2   74.616186   24.773579  0.000726  \n",
       "3  145.999434   26.920634  0.001459  \n",
       "4 -694.862277 -218.983324 -0.001462  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b0f25-fdff-4ce6-a2a4-26bc5f7336a0",
   "metadata": {},
   "source": [
    "## 1.3 Show the maximum return of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0404bc17-40e7-423f-87c3-a050e7fbd448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum return of training set: 195.6927566509\n",
      "Maximum return of testing set: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "train_max = np.sum(train['TARGET'][train['TARGET']>0])\n",
    "test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Maximum return of training set:', train_max)\n",
    "print('Maximum return of testing set:', test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9149c4-b51c-435c-9e81-744f395e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=0.5).fit(pd.DataFrame(train_seurat.iloc[:, :]), train['TARGET'])\n",
    "pred = reg.predict(pd.DataFrame(train_seurat.iloc[:, :]))\n",
    "\n",
    "pred_test = reg.predict(pd.DataFrame(test_seurat.iloc[:, :]))\n",
    "\n",
    "train_res = np.sum(train['TARGET'][pred>0])\n",
    "test_res = np.sum(test['TARGET'][pred_test>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4db722a-05ac-4744-bd74-10c3aa0102b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train naive random selection percentage return: 6.32652526152815%\n",
      "Test naive random selection percentage return: 4.010962118285209%\n"
     ]
    }
   ],
   "source": [
    "print(f'Train naive random selection percentage return: {train_res/train_max*100}%')\n",
    "print(f'Test naive random selection percentage return: {test_res/test_max*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbf2b1-9cdc-476e-9975-8e12dcbdee2b",
   "metadata": {},
   "source": [
    "### 1.3.1 Remove the Unnamed columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb5b0f0-c09f-44ee-975c-8613c205ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562744d-bf00-48c2-9489-bb3953b6f438",
   "metadata": {},
   "source": [
    "## 1.4 Naive random selection experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c2bdc66-19b4-40cc-b27f-c676864e2863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of train naive random selection: -0.8170112755425528\n",
      "Result of test naive random selection: -0.6496402211191966\n"
     ]
    }
   ],
   "source": [
    "train_random = 0\n",
    "for j in range(10000):\n",
    "    ind = np.random.randint(2, size=train.shape[0])\n",
    "    train_random = train_random + sum(train['TARGET'][ind>0])\n",
    "\n",
    "print('Result of train naive random selection:', train_random/10000)\n",
    "\n",
    "test_random = 0\n",
    "for j in range(10000):\n",
    "    ind = np.random.randint(2, size=test.shape[0])\n",
    "    test_random = test_random + sum(test['TARGET'][ind>0])\n",
    "\n",
    "print('Result of test naive random selection:', test_random/10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f271e7a5-ff2d-4e48-b332-1a91b3656b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train naive random selection percentage return: -0.41749694241368096%\n",
      "Test naive random selection percentage return: -1.1608543258092974%\n"
     ]
    }
   ],
   "source": [
    "print(f'Train naive random selection percentage return: {(train_random/10000)/train_max*100}%')\n",
    "print(f'Test naive random selection percentage return: {(test_random/10000)/test_max*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fe714-f925-46d1-9702-3baee74a3729",
   "metadata": {},
   "source": [
    "## 1.5 Get data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c335643-2336-4593-a3ed-e48e4eec7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Train shape:', train.shape)\n",
    "# print('Test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51177e3-5eab-4917-a56f-86a76c413ee2",
   "metadata": {},
   "source": [
    "## 3.1 Remove extreme Target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a272254a-5bc3-4735-ac3b-27c14ac8f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.sort_values(by=['TARGET'])\n",
    "# # remove samples with extreme large target values and samples with extreme negative values\n",
    "# num_remove = 10\n",
    "# train_remove = train.iloc[num_remove:-num_remove, :]\n",
    "# # train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e3f63-001d-4ef6-b4d7-688667fe377b",
   "metadata": {},
   "source": [
    "## 3.2 Remove extreme features values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75ffdc30-d8ad-439f-9ec6-23632c274ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(train.shape[1]-1):\n",
    "#     train_remove = train_remove.sort_values(by=[str(i)])\n",
    "#     num_remove = 5\n",
    "#     train_remove = train_remove.iloc[num_remove:-num_remove, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "696e4faf-b945-45ee-ab9a-1d71e876d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_remove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "717936d5-b7db-4666-90ea-6180f63efaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure, ax = plt.subplots(1, 1, figsize = (35, 7))\n",
    "\n",
    "# train_remove_mean_values = train_remove.mean()\n",
    "# train_remove_std_values = train_remove.std()\n",
    "\n",
    "# train_remove_std = (train_remove-train_remove.mean())/train_remove.std()\n",
    "\n",
    "# # train_remove_std.boxplot()\n",
    "# # ax.set_xlabel('Features', fontsize=20)\n",
    "# ax.set_ylabel('Values', fontsize=20)\n",
    "# ax.set_title('Train standardized features variability', fontsize=25)\n",
    "\n",
    "# figure.savefig(\"features_std_dis.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8fd75-41a0-41bb-bc59-1ff78a0c53ff",
   "metadata": {},
   "source": [
    "### The devil is in the details, we need to standardize test dataset by using train mean and train standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7497dc7a-2b83-4de8-bc95-09f24744f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_std = (test-train_remove_mean_values)/train_remove_std_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91132f5e-e1fc-4108-b2c0-ed422db1125f",
   "metadata": {},
   "source": [
    "## 5.5 Autoencoder Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e90eb95-282e-415e-a6bb-81d93114b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (72000, 9, 6)\n",
      "Validation X shape: (18000, 9, 6)\n",
      "Test X shape: (19707, 9, 6)\n",
      "Train Y shape: (72000, 1)\n",
      "Val Y shape: (18000, 1)\n",
      "Test Y shape: (19707, 1)\n",
      "train_max: 156.45405680890002\n",
      "val_max: 39.238699842\n",
      "test_max: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "input_features = train_seurat.to_numpy()\n",
    "output_features = pd.DataFrame(train['TARGET']).to_numpy()\n",
    "\n",
    "#######\n",
    "for i in range(input_features.shape[0]):\n",
    "    input_features[i, :] = (input_features[i, :] - np.mean(input_features[i, :]))/np.std(input_features[i, :])\n",
    "#######\n",
    "\n",
    "input_ = np.zeros((input_features.shape[0], 9, 6))\n",
    "output_ = np.zeros((output_features.shape[0], 9, 6))\n",
    "\n",
    "for i in range(input_.shape[0]):\n",
    "    input_[i, :, :] = np.reshape(input_features[i, :], (9, 6))\n",
    "    \n",
    "input_features = input_\n",
    "    \n",
    "# input_features = torch.tensor(input_features, 1)\n",
    "\n",
    "X_test = test_seurat.to_numpy()\n",
    "Y_test = pd.DataFrame(test['TARGET']).to_numpy()\n",
    "\n",
    "#######\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i, :] = (X_test[i, :] - np.mean(X_test[i, :]))/np.std(X_test[i, :])\n",
    "#######\n",
    "\n",
    "\n",
    "X_test_ = np.zeros((X_test.shape[0], 9, 6))\n",
    "for i in range(X_test_.shape[0]):\n",
    "    X_test_[i, :, :] = np.reshape(X_test[i, :], (9, 6))\n",
    "X_test = X_test_\n",
    "    \n",
    "X_train, X_val, Y_train, Y_val = train_test_split(input_features, output_features, test_size=0.2, random_state=42)\n",
    "\n",
    "#####\n",
    "# to calculate returns\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "test_data = test\n",
    "#####\n",
    "\n",
    "auto_train_max = np.sum(train_data['TARGET'][train_data['TARGET']>0])\n",
    "auto_val_max = np.sum(val_data['TARGET'][val_data['TARGET']>0])\n",
    "auto_test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Train X shape:', X_train.shape)\n",
    "print('Validation X shape:', X_val.shape)\n",
    "print('Test X shape:', X_test.shape)\n",
    "\n",
    "print('Train Y shape:', Y_train.shape)\n",
    "print('Val Y shape:', Y_val.shape)\n",
    "print('Test Y shape:', Y_test.shape)\n",
    "\n",
    "print('train_max:', auto_train_max)\n",
    "print('val_max:', auto_val_max)\n",
    "print('test_max:', auto_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f7a19ea-d496-4ee3-b66d-7f33a40361be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.from_numpy(X_train)\n",
    "train_output = torch.from_numpy(Y_train)\n",
    "val_input = torch.from_numpy(X_val)\n",
    "val_output = torch.from_numpy(Y_val)\n",
    "test_input = torch.from_numpy(X_test)\n",
    "test_output = torch.from_numpy(Y_test)\n",
    "\n",
    "train_input = torch.unsqueeze(train_input, 1)\n",
    "val_input = torch.unsqueeze(val_input, 1)\n",
    "test_input = torch.unsqueeze(test_input, 1)\n",
    "\n",
    "train_input = train_input.float()\n",
    "train_output = train_output.float()\n",
    "val_input = val_input.float()\n",
    "val_output = val_output.float()\n",
    "test_input = test_input.float()\n",
    "test_output = test_output.float()\n",
    "\n",
    "input_feature = train_input.shape[1]\n",
    "output_feature = 1\n",
    "\n",
    "# print('input_feature:', input_feature)\n",
    "# print('output_feature:', output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b5591cb-674f-4288-9119-9b6c5ee1b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.to(device)\n",
    "train_output = train_output.to(device)\n",
    "val_input = val_input.to(device)\n",
    "val_output = val_output.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_output = test_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "852276a1-d972-4d87-8724-e9c1c973513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a0344a-7b96-48ff-852a-999ea40a6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 2)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.linear = nn.Linear(96, 1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(input_feature//2, input_feature//4)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16+input_feature, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x_):\n",
    "        x = self.conv1(x_)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(x)\n",
    "        output = self.linear(x)\n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22bb31c5-cbce-4611-ab4a-3f33902bdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "train_ds = TensorDataset(train_input, train_output)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed2d7429-de64-4f06-8610-93c27155281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path):\n",
    "    best_loss = float('inf')\n",
    "    train_pred_output = []\n",
    "    val_pred_output = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    test_error = []\n",
    "    epochs = []\n",
    "    \n",
    "    train_returns = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "    \n",
    "    train_sum = []\n",
    "    val_sum = []\n",
    "    test_sum = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_dl:\n",
    "            model = model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            y = torch.reshape(y, (y.shape[0], 1))\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            \n",
    "            train_pred = model(train_input)\n",
    "            train_pred_index = (torch.sign(train_pred)+1)//2\n",
    "            train_output = torch.reshape(train_output, (train_output.shape[0], 1))\n",
    "            train_loss = loss_fn(train_output, train_pred)\n",
    "            # train_loss = loss_fn(train_pred, train_output.long().squeeze())\n",
    "            train_loss = train_loss.cpu().detach().numpy()\n",
    "            \n",
    "            val_pred = model(val_input)\n",
    "            val_pred_index = (torch.sign(val_pred)+1)//2\n",
    "            val_output = torch.reshape(val_output, (val_output.shape[0], 1))\n",
    "            val_loss = loss_fn(val_output, val_pred)\n",
    "            # val_loss = loss_fn(val_pred, val_output.long().squeeze())\n",
    "            val_loss = val_loss.cpu().detach().numpy()\n",
    "        \n",
    "            test_pred = model(test_input)\n",
    "            test_pred_index = (torch.sign(test_pred)+1)//2\n",
    "            test_output = torch.reshape(test_output, (test_output.shape[0], 1))\n",
    "            test_loss = loss_fn(test_output, test_pred)\n",
    "            # test_loss = loss_fn(test_pred, test_output.long().squeeze())\n",
    "            test_loss = test_loss.cpu().detach().numpy()\n",
    "    \n",
    "            epochs.append(epoch)\n",
    "            train_error.append(math.log(train_loss+1))\n",
    "            val_error.append(math.log(val_loss+1))\n",
    "            test_error.append(math.log(test_loss+1))\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 2, figsize = (20, 7))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 4, figsize = (22, 5))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # train_conf = confusion_matrix(train_output, train_pred_index)\n",
    "#             g1 = sns.heatmap(train_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[0], annot = True)\n",
    "#             g1.set_ylabel('True Target')\n",
    "#             g1.set_xlabel('Predict Target')\n",
    "#             g1.set_title('Train dataset')\n",
    "\n",
    "#             plt.grid(False)\n",
    "            # val_conf = confusion_matrix(val_output, val_pred_index)\n",
    "#             g2 = sns.heatmap(val_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[1], annot = True)\n",
    "#             g2.set_ylabel('True Target')\n",
    "#             g2.set_xlabel('Predict Target')\n",
    "#             g2.set_title('Val dataset')\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # test_conf = confusion_matrix(test_output, test_pred_index)\n",
    "#             g3 = sns.heatmap(test_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[2], annot = True)\n",
    "#             g3.set_ylabel('True Target')\n",
    "#             g3.set_xlabel('Predict Target')\n",
    "#             g3.set_title('Test dataset')\n",
    "            \n",
    "            train_pred_np = train_pred_index.cpu().detach().numpy()\n",
    "            train_output_np = train_output.cpu().detach().numpy()\n",
    "            val_pred_np = val_pred_index.cpu().detach().numpy()\n",
    "            val_output_np = val_output.cpu().detach().numpy()\n",
    "            test_pred_np = test_pred_index.cpu().detach().numpy()\n",
    "            test_output_np = test_output.cpu().detach().numpy()\n",
    "            \n",
    "#             train_max_value = max(max(train_output_np), max(train_pred_np))\n",
    "#             train_min_value = min(min(train_output_np), min(train_pred_np))\n",
    "#             val_max_value = max(max(val_output_np), max(val_pred_np))\n",
    "#             val_min_value = min(min(val_output_np), min(val_pred_np))\n",
    "#             test_max_value = max(max(test_output_np), max(test_pred_np))\n",
    "#             test_min_value = min(min(test_output_np), min(test_pred_np))\n",
    "            \n",
    "#             ax[0].scatter(train_output_np, train_pred_np, s = 20, alpha=0.3, c='blue')\n",
    "#             ax[1].scatter(val_output_np, val_pred_np, s = 20, alpha=0.3, c='red')\n",
    "#             ax[2].scatter(test_output_np, test_pred_np, s = 20, alpha=0.3, c='green')\n",
    "            \n",
    "#             ax[0].plot(epochs, train_error, c='blue')\n",
    "#             ax[0].plot(epochs, val_error, c='red')\n",
    "#             ax[0].plot(epochs, test_error, c='green')\n",
    "#             ax[0].set_title('Errors vs Epochs', fontsize=15)\n",
    "#             ax[0].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[0].set_ylabel('Errors', fontsize=10)\n",
    "\n",
    "#             ax[0].legend(['train', 'valid', 'test'])\n",
    "            \n",
    "#             ax[0].set_xlim([train_min_value, train_max_value])\n",
    "#             ax[0].set_ylim([train_min_value, train_max_value])\n",
    "#             ax[0].set_title('Trainig data', fontsize=15)\n",
    "#             ax[0].set_xlabel('Target', fontsize=10)\n",
    "#             ax[0].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[0].plot([train_min_value, train_max_value], [train_min_value, train_max_value], 'k-')\n",
    "            \n",
    "#             ax[1].set_xlim([val_min_value, val_max_value])\n",
    "#             ax[1].set_ylim([val_min_value, val_max_value])\n",
    "#             ax[1].set_title('Validation data', fontsize=15)\n",
    "#             ax[1].set_xlabel('Target', fontsize=10)\n",
    "#             ax[1].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[1].plot([val_min_value, val_max_value], [val_min_value, val_max_value], 'k-')\n",
    "            \n",
    "#             ax[2].set_xlim([test_min_value, test_max_value])\n",
    "#             ax[2].set_ylim([test_min_value, test_max_value])\n",
    "#             ax[2].set_title('Testing data', fontsize=15)\n",
    "#             ax[2].set_xlabel('Target', fontsize=10)\n",
    "#             ax[2].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[2].plot([test_min_value, test_max_value], [test_min_value, test_max_value], 'k-')\n",
    "            \n",
    "#             ax[3].plot(epochs, train_error, c='blue')\n",
    "#             ax[3].plot(epochs, val_error, c='red')\n",
    "#             ax[3].plot(epochs, test_error, c='green')\n",
    "#             ax[3].set_title('Training and Validation error', fontsize=15)\n",
    "#             ax[3].set_xlabel('Epochs', fontsize=10)\n",
    "#             ax[3].set_ylabel('MSE error', fontsize=10)\n",
    "            \n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "#             print('Epoch ', epoch, 'Train_loss: ', train_loss*1000, ' Validation_loss: ', val_loss*100, ' Test_loss: ', test_loss*100)\n",
    "            # print(train_pred_np.shape, train_pred_np)\n",
    "            # print(train_pred, train_pred_np)\n",
    "            train_pred_np = np.squeeze(train_pred_np)\n",
    "            val_pred_np = np.squeeze(val_pred_np)\n",
    "            test_pred_np = np.squeeze(test_pred_np)\n",
    "            \n",
    "            train_res = np.sum(train_data['TARGET'][train_pred_np>0])\n",
    "            train_output_check = np.squeeze(train_output_np)\n",
    "            train_check = np.sum(train_data['TARGET'][train_output_check>0])\n",
    "            \n",
    "            val_res = np.sum(val_data['TARGET'][val_pred_np>0])\n",
    "            val_output_check = np.squeeze(val_output_np)\n",
    "            val_check = np.sum(val_data['TARGET'][val_output_check>0])\n",
    "            \n",
    "            test_res = np.sum(test_data['TARGET'][test_pred_np>0])\n",
    "            test_output_check = np.squeeze(test_output_np)\n",
    "            test_check = np.sum(test_data['TARGET'][test_output_check>0])\n",
    "            \n",
    "#             train_returns.append(train_res)\n",
    "#             val_returns.append(val_res)\n",
    "#             test_returns.append(test_res)\n",
    "            \n",
    "#             ax[1].plot(epochs, train_returns, c='blu`e')\n",
    "#             ax[1].plot(epochs, val_returns, c='red')\n",
    "#             ax[1].plot(epochs, test_returns, c='green')\n",
    "#             ax[1].legend(['train', 'valid', 'test'])\n",
    "#             ax[1].set_title('Return vs Epochs', fontsize=15)\n",
    "#             ax[1].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[1].set_ylabel('Returns', fontsize=10)\n",
    "\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "            train_sum.append(train_res)\n",
    "            val_sum.append(val_res)\n",
    "            test_sum.append(test_res)\n",
    "            # print(f'Checks: {train_check/auto_train_max*100}%, {val_check/auto_val_max*100}%, {test_check/auto_test_max*100}%')\n",
    "#             print(f'Maximum sum train return {train_res}, Total train return: {auto_train_max}, Maximum train percentage return: {train_res/auto_train_max*100}%')\n",
    "#             print(f'Maximum sum train return {val_res}, Total train return: {auto_val_max}, Maximum train percentage return: {val_res/auto_val_max*100}%')\n",
    "#             print(f'Maximum sum test return {test_res}, Total test return: {auto_test_max}, Maximum test percentage return: {test_res/auto_test_max*100}%')\n",
    "#             print('Epoch:', epoch, 'Train loss:', train_loss, 'Val loss:', val_loss, 'Test loss:', test_loss)\n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss}, Train return: {train_res/auto_train_max*100}%, Val loss: {val_loss}, Val return: {val_res/auto_val_max*100}%, Test loss: {test_loss}, Test return: {test_res/auto_test_max*100}%')\n",
    "        \n",
    "            if val_loss < best_loss:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                best_loss = val_loss\n",
    "                \n",
    "#             train_pred_output.append([train_pred.cpu().detach().numpy(), train_output.cpu().detach().numpy()])\n",
    "#             val_pred_output.append([val_pred.cpu().detach().numpy(), val_output.cpu().detach().numpy()])\n",
    "    return train_sum, val_sum, test_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "966e1596-f5a6-4719-8056-f538126e72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.00842222385108471, Train return: -0.6734364015162209%, Val loss: 0.008379928767681122, Val return: -0.7255568205531266%, Test loss: 0.008193804882466793, Test return: -0.9211383123416649%\n",
      "Epoch: 50, Train loss: 0.004782435018569231, Train return: -1.6590978959213156%, Val loss: 0.004779001232236624, Val return: -2.0249661181421565%, Test loss: 0.004752246662974358, Test return: 0.6427081492917412%\n",
      "Epoch: 100, Train loss: 0.0033749272115528584, Train return: -1.4823133729492781%, Val loss: 0.0033766848500818014, Val return: -2.027651507832037%, Test loss: 0.0033532101660966873, Test return: 0.6526274141874032%\n",
      "Epoch: 150, Train loss: 0.002630552975460887, Train return: -1.4579789081379864%, Val loss: 0.002631840528920293, Val return: -1.7021610774297176%, Test loss: 0.002607585396617651, Test return: 0.6699481771732899%\n",
      "Epoch: 200, Train loss: 0.002166937105357647, Train return: -1.3340090258888442%, Val loss: 0.002166402991861105, Val return: -1.587414426339595%, Test loss: 0.002143685705959797, Test return: 1.5856418712219247%\n",
      "Epoch: 250, Train loss: 0.0018506728811189532, Train return: -1.2511445360543911%, Val loss: 0.0018483124440535903, Val return: -1.237479766035109%, Test loss: 0.001828427193686366, Test return: 1.8715686697060732%\n",
      "Epoch: 300, Train loss: 0.0016218476230278611, Train return: -1.264916947227033%, Val loss: 0.0016179063823074102, Val return: -1.2865742698733327%, Test loss: 0.00160136295016855, Test return: 1.634932066131792%\n",
      "Epoch: 350, Train loss: 0.0014487841399386525, Train return: -1.2725056350771036%, Val loss: 0.001443664194084704, Val return: -0.6543401591639315%, Test loss: 0.001430515549145639, Test return: 1.9861990033848347%\n",
      "Epoch: 400, Train loss: 0.0013132691383361816, Train return: -1.2614598320135835%, Val loss: 0.001307287602685392, Val return: -0.023209762394450606%, Test loss: 0.001297317212447524, Test return: 1.7292655342810492%\n",
      "Epoch: 450, Train loss: 0.0012042010203003883, Train return: -1.2151847686009216%, Val loss: 0.0011975915404036641, Val return: 0.25629969750515025%, Test loss: 0.0011904087150469422, Test return: 1.8934503981942545%\n",
      "Epoch: 500, Train loss: 0.001114390091970563, Train return: -1.1251058745955451%, Val loss: 0.0011073605855926871, Val return: 0.33678902087002544%, Test loss: 0.0011025480926036835, Test return: 2.0790731807204055%\n",
      "Epoch: 550, Train loss: 0.001038982649333775, Train return: -1.2088458380533333%, Val loss: 0.001031686319038272, Val return: 0.23401715747399157%, Test loss: 0.0010288925841450691, Test return: 2.0352166681604973%\n",
      "Epoch: 600, Train loss: 0.0009746382129378617, Train return: -0.9897961106189058%, Val loss: 0.0009672141750343144, Val return: -0.15032638246811375%, Test loss: 0.0009660659125074744, Test return: 2.195687589313614%\n",
      "Epoch: 650, Train loss: 0.0009190088021568954, Train return: -1.097642474236114%, Val loss: 0.0009115498978644609, Val return: -0.5450072985626729%, Test loss: 0.000911739538423717, Test return: 2.103731293198434%\n",
      "Epoch: 700, Train loss: 0.0008703664061613381, Train return: -1.0521794370667554%, Val loss: 0.00086293212370947, Val return: -0.5957306356766519%, Test loss: 0.0008642214233987033, Test return: 2.12058089036914%\n",
      "Epoch: 750, Train loss: 0.0008274155552498996, Train return: -1.1068124819001044%, Val loss: 0.0008200693409889936, Val return: -0.4294292386814497%, Test loss: 0.0008222663309425116, Test return: 2.1578365195128164%\n",
      "Epoch: 800, Train loss: 0.00078918959479779, Train return: -1.0965176187763739%, Val loss: 0.0007819752208888531, Val return: -0.6391057884430096%, Test loss: 0.0007849091198295355, Test return: 2.054082360043668%\n",
      "Epoch: 850, Train loss: 0.0007549162255600095, Train return: -1.0207796146511527%, Val loss: 0.0007478424813598394, Val return: -1.2194349759974394%, Test loss: 0.000751394487451762, Test return: 2.1076385591298994%\n",
      "Epoch: 900, Train loss: 0.0007239857804961503, Train return: -1.0450175575805067%, Val loss: 0.0007170765893533826, Val return: -1.332968872837506%, Test loss: 0.0007211480406112969, Test return: 1.9092086221980613%\n",
      "Epoch: 950, Train loss: 0.0006959224701859057, Train return: -1.0359648035075972%, Val loss: 0.0006892004166729748, Val return: -1.136707943423198%, Test loss: 0.0006937049329280853, Test return: 1.6840271366561295%\n",
      "Epoch: 1000, Train loss: 0.0006703307153657079, Train return: -1.1899274113894898%, Val loss: 0.0006638119812123477, Val return: -1.200870117759698%, Test loss: 0.0006686683045700192, Test return: 1.2536747113151683%\n",
      "Epoch: 1050, Train loss: 0.0006468916544690728, Train return: -1.2221990450114109%, Val loss: 0.0006405721651390195, Val return: -1.3409349726639193%, Test loss: 0.000645727792289108, Test return: 1.4675705037440665%\n",
      "Epoch: 1100, Train loss: 0.000625337939709425, Train return: -1.2842439609310836%, Val loss: 0.0006192165310494602, Val return: -1.4549058207808905%, Test loss: 0.0006246393313631415, Test return: 1.5704043053233658%\n",
      "Epoch: 1150, Train loss: 0.0006054478581063449, Train return: -1.376230070422502%, Val loss: 0.0005995180690661073, Val return: -1.6439961507326024%, Test loss: 0.000605169974733144, Test return: 1.3734408604879869%\n",
      "Epoch: 1200, Train loss: 0.0005870298482477665, Train return: -1.283375636371332%, Val loss: 0.0005812924937345088, Val return: -1.137088045211995%, Test loss: 0.0005871423636563122, Test return: 1.0307428421824547%\n",
      "Epoch: 1250, Train loss: 0.0005699068424291909, Train return: -1.2385550203833096%, Val loss: 0.0005643743788823485, Val return: -1.1791702142606375%, Test loss: 0.0005703903152607381, Test return: 0.8278093516625177%\n",
      "Epoch: 1300, Train loss: 0.0005539453122764826, Train return: -1.1716702069535005%, Val loss: 0.0005486198351718485, Val return: -1.1377491654862257%, Test loss: 0.0005547802429646254, Test return: 0.8750180452710011%\n",
      "Epoch: 1350, Train loss: 0.0005390369915403426, Train return: -1.1328305646716623%, Val loss: 0.0005339172203093767, Val return: -1.205392048932608%, Test loss: 0.0005402067909017205, Test return: 0.6107506396899843%\n",
      "Epoch: 1400, Train loss: 0.0005250736721791327, Train return: -1.043337648440656%, Val loss: 0.0005201628082431853, Val return: -1.1228559936341223%, Test loss: 0.000526564137544483, Test return: 0.843765164212882%\n",
      "Epoch: 1450, Train loss: 0.0005119646084494889, Train return: -1.1067455581001495%, Val loss: 0.0005072629428468645, Val return: -1.0187659927817443%, Test loss: 0.000513765262439847, Test return: 0.8281203934707483%\n",
      "Epoch: 1500, Train loss: 0.0004996335483156145, Train return: -1.0734930550579656%, Val loss: 0.000495143816806376, Val return: -0.9306320710686106%, Test loss: 0.0005017322837375104, Test return: 0.5617027241659196%\n",
      "Epoch: 1550, Train loss: 0.0004880098276771605, Train return: -1.0725851994043822%, Val loss: 0.00048372766468673944, Val return: -0.8504536525005076%, Test loss: 0.0004903998924419284, Test return: 0.36417182718262014%\n",
      "Epoch: 1600, Train loss: 0.00047703448217362165, Train return: -1.0353723918955848%, Val loss: 0.0004729498759843409, Val return: -0.5810459722622124%, Test loss: 0.00047970321611501276, Test return: 0.3965593016127089%\n",
      "Epoch: 1650, Train loss: 0.00046665206900797784, Train return: -1.112838477768985%, Val loss: 0.0004627601883839816, Val return: -0.5298015195128444%, Test loss: 0.0004695894895121455, Test return: 0.30032892623509655%\n",
      "Epoch: 1700, Train loss: 0.00045681677875109017, Train return: -1.1128984442549383%, Val loss: 0.0004531070007942617, Val return: -0.7362975714367453%, Test loss: 0.00046000874135643244, Test return: 0.2388687063923179%\n",
      "Epoch: 1750, Train loss: 0.00044747383799403906, Train return: -1.1682863969660953%, Val loss: 0.00044394098222255707, Val return: -0.8773567227921%, Test loss: 0.0004509148420765996, Test return: 0.3555336008024464%\n",
      "Epoch: 1800, Train loss: 0.00043859565630555153, Train return: -1.1603143838049188%, Val loss: 0.00043523169006220996, Val return: -0.9427967070509959%, Test loss: 0.00044227647595107555, Test return: -0.08634203311182319%\n",
      "Epoch: 1850, Train loss: 0.00043013953836634755, Train return: -1.144198101163045%, Val loss: 0.0004269435303285718, Val return: -1.0858112978146108%, Test loss: 0.00043405534233897924, Test return: -0.13358706550106897%\n",
      "Epoch: 1900, Train loss: 0.00042207096703350544, Train return: -1.219455730400373%, Val loss: 0.0004190406762063503, Val return: -1.1311425602458927%, Test loss: 0.0004262231814209372, Test return: -0.3039676379266922%\n",
      "Epoch: 1950, Train loss: 0.000414364185417071, Train return: -1.1142686976339344%, Val loss: 0.00041149716707877815, Val return: -1.1685049220444048%, Test loss: 0.0004187471931800246, Test return: -0.20269196164003156%\n",
      "Epoch: 2000, Train loss: 0.000407000130508095, Train return: -1.249850413651114%, Val loss: 0.0004042917862534523, Val return: -1.4150059870375657%, Test loss: 0.0004116070340387523, Test return: -0.3133659248586421%\n",
      "Epoch: 2050, Train loss: 0.0003999522887170315, Train return: -1.3230636657305472%, Val loss: 0.0003974034043494612, Val return: -1.5277176140233852%, Test loss: 0.0004047774418722838, Test return: -0.500173073593079%\n",
      "Epoch: 2100, Train loss: 0.0003931945830117911, Train return: -1.2072977772044169%, Val loss: 0.00039080684655345976, Val return: -1.8206232568270226%, Test loss: 0.00039823309634812176, Test return: -0.49172573660087376%\n",
      "Epoch: 2150, Train loss: 0.0003867087361868471, Train return: -1.261855011923015%, Val loss: 0.00038448121631518006, Val return: -1.848450442855017%, Test loss: 0.00039195685531012714, Test return: -0.5342633047367416%\n",
      "Epoch: 2200, Train loss: 0.00038047859561629593, Train return: -1.2488986313641142%, Val loss: 0.00037841018638573587, Val return: -1.9948177670305385%, Test loss: 0.0003859332064166665, Test return: -0.6217474505748545%\n",
      "Epoch: 2250, Train loss: 0.0003744895802810788, Train return: -1.2399058309938527%, Val loss: 0.0003725747228600085, Val return: -2.025459322557133%, Test loss: 0.00038014675374142826, Test return: -0.5479805958567315%\n",
      "Epoch: 2300, Train loss: 0.0003687253047246486, Train return: -1.2100575669395428%, Val loss: 0.00036695884773507714, Val return: -2.083896870417616%, Test loss: 0.0003745819558389485, Test return: -0.45561074418855585%\n",
      "Epoch: 2350, Train loss: 0.0003631719446275383, Train return: -1.2484123863823497%, Val loss: 0.0003615536552388221, Val return: -2.1260221703550704%, Test loss: 0.0003692264435812831, Test return: -0.364277180341363%\n",
      "Epoch: 2400, Train loss: 0.0003578192845452577, Train return: -1.2806080079132927%, Val loss: 0.0003563447098713368, Val return: -2.14273903667942%, Test loss: 0.0003640700888354331, Test return: -0.27639215892607466%\n",
      "Epoch: 2450, Train loss: 0.00035265515907667577, Train return: -1.3888162426200386%, Val loss: 0.0003513248229864985, Val return: -2.0804620012567434%, Test loss: 0.0003591021813917905, Test return: -0.10129501432193816%\n",
      "Epoch: 2500, Train loss: 0.0003476670535746962, Train return: -1.328130495675189%, Val loss: 0.000346479588188231, Val return: -2.134458629802834%, Test loss: 0.00035431087599135935, Test return: -0.19554189910755163%\n",
      "Epoch: 2550, Train loss: 0.0003428499330766499, Train return: -1.30037676407778%, Val loss: 0.00034179858630523086, Val return: -2.130286121522507%, Test loss: 0.00034968715044669807, Test return: -0.32119951599751784%\n",
      "Epoch: 2600, Train loss: 0.0003381950664333999, Train return: -1.265055377641953%, Val loss: 0.00033727390109561384, Val return: -2.1452124417716067%, Test loss: 0.00034522099304012954, Test return: -0.3192693309803078%\n",
      "Epoch: 2650, Train loss: 0.0003336947411298752, Train return: -1.175617136886766%, Val loss: 0.0003329015744384378, Val return: -2.091420746111479%, Test loss: 0.00034090509871020913, Test return: -0.21583290354338486%\n",
      "Epoch: 2700, Train loss: 0.0003293374611530453, Train return: -1.3125201072339154%, Val loss: 0.0003286696446593851, Val return: -2.086228400268716%, Test loss: 0.00033672901918180287, Test return: -0.28103076605032656%\n",
      "Epoch: 2750, Train loss: 0.00032511577592231333, Train return: -1.4343469776184983%, Val loss: 0.0003245693224016577, Val return: -2.2281050583235578%, Test loss: 0.00033268588595092297, Test return: -0.4578387764055606%\n",
      "Epoch: 2800, Train loss: 0.0003210243594367057, Train return: -1.589880070184595%, Val loss: 0.000320594321237877, Val return: -2.393638305504383%, Test loss: 0.0003287707222625613, Test return: -0.30831240591002224%\n",
      "Epoch: 2850, Train loss: 0.00031705887522548437, Train return: -1.580038294704904%, Val loss: 0.0003167418180964887, Val return: -2.371635996470792%, Test loss: 0.00032497875508852303, Test return: -0.260149463709686%\n",
      "Epoch: 2900, Train loss: 0.0003132118145003915, Train return: -1.5523848564480456%, Val loss: 0.00031300337286666036, Val return: -2.2905858517716577%, Test loss: 0.0003213003510609269, Test return: -0.45030723529950284%\n",
      "Epoch: 2950, Train loss: 0.00030947558116167784, Train return: -1.5561818497131392%, Val loss: 0.0003093722043558955, Val return: -2.1521909986836%, Test loss: 0.0003177312028128654, Test return: -0.5727020152940867%\n",
      "Epoch: 3000, Train loss: 0.00030584677006118, Train return: -1.6084423335047928%, Val loss: 0.0003058442671317607, Val return: -2.2740180653103907%, Test loss: 0.0003142674104310572, Test return: -0.6150851650547403%\n",
      "Epoch: 3050, Train loss: 0.0003023205208592117, Train return: -1.5836060430355323%, Val loss: 0.0003024155448656529, Val return: -2.267788857895665%, Test loss: 0.0003109037352260202, Test return: -0.5776173768285928%\n",
      "Epoch: 3100, Train loss: 0.0002988926135003567, Train return: -1.6139954180186784%, Val loss: 0.00029908207943663, Val return: -2.3363046092030606%, Test loss: 0.000307636393699795, Test return: -0.5534141459746995%\n",
      "Epoch: 3150, Train loss: 0.00029555903165601194, Train return: -1.5812145556709254%, Val loss: 0.00029584174626506865, Val return: -2.213003992223357%, Test loss: 0.00030445956508629024, Test return: -0.7269711470501037%\n",
      "Epoch: 3200, Train loss: 0.0002923175925388932, Train return: -1.5100473636715512%, Val loss: 0.00029269250808283687, Val return: -2.202646829482582%, Test loss: 0.00030137080466374755, Test return: -0.5756185437517566%\n",
      "Epoch: 3250, Train loss: 0.0002891615149565041, Train return: -1.4753291121235288%, Val loss: 0.00028962819487787783, Val return: -2.0072135982369383%, Test loss: 0.0002983646700158715, Test return: -0.48978570030030705%\n",
      "Epoch: 3300, Train loss: 0.00028608899447135627, Train return: -1.5177747203451897%, Val loss: 0.0002866443828679621, Val return: -2.0095456938560203%, Test loss: 0.00029544017161242664, Test return: -0.517226520673826%\n",
      "Epoch: 3350, Train loss: 0.00028309752815403044, Train return: -1.516530443245761%, Val loss: 0.0002837361244019121, Val return: -2.0446174802694674%, Test loss: 0.00029259436996653676, Test return: -0.5514115907459983%\n",
      "Epoch: 3400, Train loss: 0.00028018257580697536, Train return: -1.5074896383676384%, Val loss: 0.0002809019642882049, Val return: -2.064482776600353%, Test loss: 0.0002898238890338689, Test return: -0.7400409141906438%\n",
      "Epoch: 3450, Train loss: 0.00027733968454413116, Train return: -1.5333558465219146%, Val loss: 0.00027813768247142434, Val return: -1.9766415990414918%, Test loss: 0.00028712578932754695, Test return: -0.7662817524724628%\n",
      "Epoch: 3500, Train loss: 0.00027456594398245215, Train return: -1.5220484024320446%, Val loss: 0.00027544121257960796, Val return: -2.073390263377626%, Test loss: 0.0002844975679181516, Test return: -0.825699713537674%\n",
      "Epoch: 3550, Train loss: 0.00027185704675503075, Train return: -1.555845995206892%, Val loss: 0.0002728105755522847, Val return: -2.0822499427604755%, Test loss: 0.0002819326473399997, Test return: -0.891258585463313%\n",
      "Epoch: 3600, Train loss: 0.00026921386597678065, Train return: -1.4092948097173013%, Val loss: 0.000270243501290679, Val return: -2.0139287774110954%, Test loss: 0.0002794300962705165, Test return: -0.876604880988035%\n",
      "Epoch: 3650, Train loss: 0.0002666350919753313, Train return: -1.3689776026812153%, Val loss: 0.00026773681747727096, Val return: -2.0220857933491563%, Test loss: 0.0002769912243820727, Test return: -0.730296913864943%\n",
      "Epoch: 3700, Train loss: 0.0002641158935148269, Train return: -1.3357720584725046%, Val loss: 0.00026528886519372463, Val return: -2.0771487416298067%, Test loss: 0.0002746106474660337, Test return: -0.8565628318666489%\n",
      "Epoch: 3750, Train loss: 0.0002616553392726928, Train return: -1.2875692316885907%, Val loss: 0.0002628982765600085, Val return: -2.004033605003155%, Test loss: 0.0002722862991504371, Test return: -0.7959325893476218%\n",
      "Epoch: 3800, Train loss: 0.0002592518867459148, Train return: -1.275261547635681%, Val loss: 0.000260563800111413, Val return: -2.0486443670072103%, Test loss: 0.0002700163167901337, Test return: -0.8219353403551488%\n",
      "Epoch: 3850, Train loss: 0.00025690312031656504, Train return: -1.2546241728954155%, Val loss: 0.0002582811866886914, Val return: -2.065636094630314%, Test loss: 0.00026779979816637933, Test return: -0.7573770125136914%\n",
      "Epoch: 3900, Train loss: 0.0002546079340390861, Train return: -1.2267363661552597%, Val loss: 0.00025604834081605077, Val return: -2.0900787597507673%, Test loss: 0.0002656340366229415, Test return: -0.7744199811025818%\n",
      "Epoch: 3950, Train loss: 0.0002523644652683288, Train return: -1.2616592183423%, Val loss: 0.00025386473862454295, Val return: -1.8568577270241753%, Test loss: 0.0002635189739521593, Test return: -0.8362163954226515%\n",
      "Epoch: 4000, Train loss: 0.0002501702983863652, Train return: -1.242593301095785%, Val loss: 0.000251727644354105, Val return: -1.9219121429522923%, Test loss: 0.00026145324227400124, Test return: -1.0243663653901651%\n",
      "Epoch: 4050, Train loss: 0.0002480247931089252, Train return: -1.2452671703998643%, Val loss: 0.00024963769828900695, Val return: -1.8915742621154301%, Test loss: 0.00025943221407942474, Test return: -1.0819197785396106%\n",
      "Epoch: 4100, Train loss: 0.000245925533818081, Train return: -1.2783794648693376%, Val loss: 0.00024759257212281227, Val return: -1.8626832895662688%, Test loss: 0.0002574539976194501, Test return: -1.097929986685259%\n",
      "Epoch: 4150, Train loss: 0.00024387240409851074, Train return: -1.2410632289143242%, Val loss: 0.00024559148005209863, Val return: -1.9561832580864542%, Test loss: 0.00025551801081746817, Test return: -1.3138122645820682%\n",
      "Epoch: 4200, Train loss: 0.00024186322116293013, Train return: -1.248629604143865%, Val loss: 0.00024363359261769801, Val return: -1.9098015199725944%, Test loss: 0.0002536246902309358, Test return: -1.2886586448809338%\n",
      "Epoch: 4250, Train loss: 0.00023989523469936103, Train return: -1.2257085430148447%, Val loss: 0.0002417155628791079, Val return: -1.8325882888461889%, Test loss: 0.00025177147472277284, Test return: -1.2448300225496909%\n",
      "Epoch: 4300, Train loss: 0.00023796831374056637, Train return: -1.2536096335268785%, Val loss: 0.00023983743449207395, Val return: -1.7771597270243724%, Test loss: 0.00024995917920023203, Test return: -1.2747071423135337%\n",
      "Epoch: 4350, Train loss: 0.00023608282208442688, Train return: -1.2182819781580498%, Val loss: 0.0002379999787081033, Val return: -1.6240020657308303%, Test loss: 0.00024818527163006365, Test return: -1.288259273889389%\n",
      "Epoch: 4400, Train loss: 0.0002342356601729989, Train return: -1.229547036641977%, Val loss: 0.00023619996500201523, Val return: -1.5337621083862218%, Test loss: 0.00024644797667860985, Test return: -1.3283825306707948%\n",
      "Epoch: 4450, Train loss: 0.00023242595489136875, Train return: -1.2577127978237563%, Val loss: 0.00023443620011676103, Val return: -1.4578783198817695%, Test loss: 0.00024474761448800564, Test return: -1.3123271170533126%\n",
      "Epoch: 4500, Train loss: 0.00023065310961101204, Train return: -1.2486156319909876%, Val loss: 0.0002327063848497346, Val return: -1.3716092611812896%, Test loss: 0.0002430824242765084, Test return: -1.3958226993021325%\n",
      "Epoch: 4550, Train loss: 0.00022891495609655976, Train return: -1.3292117229917046%, Val loss: 0.00023101009719539434, Val return: -1.372142436339596%, Test loss: 0.0002414512709947303, Test return: -1.4102159800180913%\n",
      "Epoch: 4600, Train loss: 0.0002272117999382317, Train return: -1.298496663260977%, Val loss: 0.00022934818116482347, Val return: -1.236686105691483%, Test loss: 0.00023985240841284394, Test return: -1.364679367088104%\n",
      "Epoch: 4650, Train loss: 0.00022554230235982686, Train return: -1.2987339731828627%, Val loss: 0.00022771846852265298, Val return: -1.2086249924427348%, Test loss: 0.00023828481789678335, Test return: -1.3596515100804036%\n",
      "Epoch: 4700, Train loss: 0.00022390678350348026, Train return: -1.293628159909029%, Val loss: 0.00022612116299569607, Val return: -1.2654998305231564%, Test loss: 0.0002367479173699394, Test return: -1.332832982392846%\n",
      "Epoch: 4750, Train loss: 0.00022230301692616194, Train return: -1.2158534728974755%, Val loss: 0.00022455432917922735, Val return: -1.2985858477772343%, Test loss: 0.00023524164862465113, Test return: -1.3704599547066532%\n",
      "Epoch: 4800, Train loss: 0.0002207295474363491, Train return: -1.207659791466985%, Val loss: 0.00022301668650470674, Val return: -1.3972037152290515%, Test loss: 0.00023376484750770032, Test return: -1.3872074294678118%\n",
      "Epoch: 4850, Train loss: 0.00021918500715401024, Train return: -1.1797263361821655%, Val loss: 0.00022150800214149058, Val return: -1.3664059746090502%, Test loss: 0.00023231684463098645, Test return: -1.5017124018580021%\n",
      "Epoch: 4900, Train loss: 0.00021766917780041695, Train return: -1.2017439761862698%, Val loss: 0.00022002724290359765, Val return: -1.2873274166421855%, Test loss: 0.0002308963448740542, Test return: -1.4632250388624284%\n",
      "Epoch: 4950, Train loss: 0.0002161811280529946, Train return: -1.1365896330652663%, Val loss: 0.00021857388492207974, Val return: -1.3580629892064204%, Test loss: 0.00022950167476665229, Test return: -1.422512768970835%\n",
      "Epoch: 5000, Train loss: 0.0002147205377696082, Train return: -1.1660717627976651%, Val loss: 0.00021714577451348305, Val return: -1.3424499132771244%, Test loss: 0.0002281326160300523, Test return: -1.501112174045411%\n",
      "Epoch: 5050, Train loss: 0.00021328707225620747, Train return: -1.2141871977920722%, Val loss: 0.0002157428243663162, Val return: -1.1138605044507075%, Test loss: 0.0002267891977680847, Test return: -1.5989025688495924%\n",
      "Epoch: 5100, Train loss: 0.00021187891252338886, Train return: -1.176860112006542%, Val loss: 0.00021436532551888376, Val return: -1.1622288552788995%, Test loss: 0.00022547011030837893, Test return: -1.6788313646755264%\n",
      "Epoch: 5150, Train loss: 0.00021049528731964529, Train return: -1.1772654252422192%, Val loss: 0.00021301205561030656, Val return: -1.1274347411645818%, Test loss: 0.00022417429136112332, Test return: -1.6114270863083306%\n",
      "Epoch: 5200, Train loss: 0.00020913626940455288, Train return: -1.1507918796884655%, Val loss: 0.00021168340754229575, Val return: -1.001852007285986%, Test loss: 0.00022290261404123157, Test return: -1.687042772991354%\n",
      "Epoch: 5250, Train loss: 0.0002078013203572482, Train return: -1.1636146421077895%, Val loss: 0.0002103784354403615, Val return: -1.0418546400521183%, Test loss: 0.0002216529392171651, Test return: -1.6441393653952652%\n",
      "Epoch: 5300, Train loss: 0.00020648962527047843, Train return: -1.1460015443319624%, Val loss: 0.00020909526210743934, Val return: -1.1713038042816406%, Test loss: 0.00022042420459911227, Test return: -1.6902962106938362%\n",
      "Epoch: 5350, Train loss: 0.00020520093676168472, Train return: -1.1067355992660337%, Val loss: 0.00020783515356015414, Val return: -1.1754823346779122%, Test loss: 0.00021921825828030705, Test return: -1.7759694394101948%\n",
      "Epoch: 5400, Train loss: 0.00020393487648107111, Train return: -1.07671664932128%, Val loss: 0.00020659647998400033, Val return: -1.07755727560429%, Test loss: 0.0002180330193368718, Test return: -1.8185558351023337%\n",
      "Epoch: 5450, Train loss: 0.00020269012020435184, Train return: -1.0643018124061052%, Val loss: 0.0002053787757176906, Val return: -1.0806560709387327%, Test loss: 0.00021686821128241718, Test return: -1.8213866575734914%\n",
      "Epoch: 5500, Train loss: 0.00020146624592598528, Train return: -1.0469776576012821%, Val loss: 0.00020418193889781833, Val return: -0.9740868824376376%, Test loss: 0.00021572352852672338, Test return: -1.695549882417494%\n",
      "Epoch: 5550, Train loss: 0.00020026210404466838, Train return: -1.0407658410474478%, Val loss: 0.0002030048635788262, Val return: -1.1396371383369646%, Test loss: 0.0002145981852663681, Test return: -1.6656223572480768%\n",
      "Epoch: 5600, Train loss: 0.0001990773598663509, Train return: -1.0368307900007885%, Val loss: 0.00020184680761303753, Val return: -1.2264238925799014%, Test loss: 0.00021349183225538582, Test return: -1.7208859518890938%\n",
      "Epoch: 5650, Train loss: 0.00019791227532550693, Train return: -1.035485109969895%, Val loss: 0.0002007075963774696, Val return: -1.124394260708288%, Test loss: 0.00021240333444438875, Test return: -1.827677431953107%\n",
      "Epoch: 5700, Train loss: 0.0001967662974493578, Train return: -1.0360850157947383%, Val loss: 0.00019958734628744423, Val return: -1.1479966049176822%, Test loss: 0.0002113329101121053, Test return: -1.8374307081743098%\n",
      "Epoch: 5750, Train loss: 0.00019563942623790354, Train return: -1.0236790766993349%, Val loss: 0.00019848489318974316, Val return: -1.1140805193858292%, Test loss: 0.00021028032642789185, Test return: -1.786671916177634%\n",
      "Epoch: 5800, Train loss: 0.0001945307303685695, Train return: -1.0752549659065804%, Val loss: 0.00019739966955967247, Val return: -1.0365601144731227%, Test loss: 0.00020924568525515497, Test return: -1.8476716041590246%\n",
      "Epoch: 5850, Train loss: 0.000193439147551544, Train return: -1.0857481089639398%, Val loss: 0.00019633116608019918, Val return: -1.0162267445293507%, Test loss: 0.00020822770602535456, Test return: -1.7766815265528864%\n",
      "Epoch: 5900, Train loss: 0.00019236508524045348, Train return: -1.0714789717773805%, Val loss: 0.00019528003758750856, Val return: -0.843383285716769%, Test loss: 0.0002072266797767952, Test return: -1.7610537315393753%\n",
      "Epoch: 5950, Train loss: 0.00019130860164295882, Train return: -1.0148477460954397%, Val loss: 0.00019424647325649858, Val return: -0.759777360617065%, Test loss: 0.00020624235912691802, Test return: -1.7550624572594469%\n",
      "Epoch: 6000, Train loss: 0.00019026889640372247, Train return: -1.0280364191288291%, Val loss: 0.0001932287705130875, Val return: -0.9505027243557876%, Test loss: 0.0002052740746876225, Test return: -1.7788590372145963%\n",
      "Epoch: 6050, Train loss: 0.0001892449799925089, Train return: -0.971052939238055%, Val loss: 0.0001922270021168515, Val return: -1.1641579304089327%, Test loss: 0.00020432124438229948, Test return: -1.8466194985328046%\n",
      "Epoch: 6100, Train loss: 0.0001882373180706054, Train return: -0.9731033584252791%, Val loss: 0.0001912407169584185, Val return: -1.288031721323821%, Test loss: 0.00020338420290499926, Test return: -1.8879583997492104%\n",
      "Epoch: 6150, Train loss: 0.00018724579422269017, Train return: -1.0473094374289114%, Val loss: 0.00019026924564968795, Val return: -1.3718284325614485%, Test loss: 0.0002024633577093482, Test return: -1.9058951565322804%\n",
      "Epoch: 6200, Train loss: 0.00018626987002789974, Train return: -1.0434236730556512%, Val loss: 0.00018931271915789694, Val return: -1.3371234982623155%, Test loss: 0.00020155753009021282, Test return: -1.8689901137099285%\n",
      "Epoch: 6250, Train loss: 0.00018530889065004885, Train return: -1.095045038872102%, Val loss: 0.00018837030802387744, Val return: -1.3905439405409952%, Test loss: 0.00020066623983439058, Test return: -1.7905293824689998%\n",
      "Epoch: 6300, Train loss: 0.00018436230311635882, Train return: -1.1510878227975438%, Val loss: 0.00018744204135145992, Val return: -1.373552090589679%, Test loss: 0.00019978906493633986, Test return: -1.7510635331149393%\n",
      "Epoch: 6350, Train loss: 0.00018343006377108395, Train return: -1.042813469639088%, Val loss: 0.00018652778817340732, Val return: -1.3853089658647921%, Test loss: 0.0001989254669751972, Test return: -1.7744455103826735%\n",
      "Epoch: 6400, Train loss: 0.00018251134315505624, Train return: -1.0357134095789204%, Val loss: 0.00018562711193226278, Val return: -1.3779363693933273%, Test loss: 0.00019807489297818393, Test return: -1.8065593950357082%\n",
      "Epoch: 6450, Train loss: 0.0001816058938857168, Train return: -1.0672732387115782%, Val loss: 0.00018473956151865423, Val return: -1.2587481210866365%, Test loss: 0.00019723728473763913, Test return: -1.814498873622047%\n",
      "Epoch: 6500, Train loss: 0.0001807133376132697, Train return: -1.0750537490085204%, Val loss: 0.0001838646421674639, Val return: -1.204512684933829%, Test loss: 0.00019641210383269936, Test return: -1.9154543930276675%\n",
      "Epoch: 6550, Train loss: 0.00017983389261644334, Train return: -1.0027729278482687%, Val loss: 0.0001830023102229461, Val return: -1.2321787239303101%, Test loss: 0.00019559943757485598, Test return: -1.8665257739897585%\n",
      "Epoch: 6600, Train loss: 0.00017896801000460982, Train return: -0.9603669325335941%, Val loss: 0.0001821530022425577, Val return: -1.2425805568565662%, Test loss: 0.00019479854381643236, Test return: -1.9409042802923746%\n",
      "Epoch: 6650, Train loss: 0.0001781153114279732, Train return: -0.9699614149689938%, Val loss: 0.00018131648539565504, Val return: -1.2028603442533%, Test loss: 0.00019400955352466553, Test return: -1.9618834075028446%\n",
      "Epoch: 6700, Train loss: 0.00017727501108311117, Train return: -0.9926675746714497%, Val loss: 0.00018049187201540917, Val return: -1.2343936240251139%, Test loss: 0.0001932320446940139, Test return: -1.921055064726614%\n",
      "Epoch: 6750, Train loss: 0.00017644657054916024, Train return: -1.11599422655602%, Val loss: 0.00017967914754990488, Val return: -1.2676320953621263%, Test loss: 0.00019246562442276627, Test return: -1.9406090312725184%\n",
      "Epoch: 6800, Train loss: 0.0001756297133397311, Train return: -1.129186833012503%, Val loss: 0.00017887780268210918, Val return: -1.3583885351610885%, Test loss: 0.00019171045278199017, Test return: -1.9124616417615683%\n",
      "Epoch: 6850, Train loss: 0.00017482449766248465, Train return: -1.1786679449625483%, Val loss: 0.00017808809934649616, Val return: -1.3232810340066923%, Test loss: 0.0001909657585201785, Test return: -1.821279671170963%\n",
      "Epoch: 6900, Train loss: 0.00017403032688889652, Train return: -1.1375370517709444%, Val loss: 0.00017730866966303438, Val return: -1.3023428198633022%, Test loss: 0.00019023133791051805, Test return: -1.8448005492110342%\n",
      "Epoch: 6950, Train loss: 0.0001732471864670515, Train return: -1.1531747204252536%, Val loss: 0.0001765392516972497, Val return: -1.233950342263229%, Test loss: 0.0001895069726742804, Test return: -1.8614174466676372%\n",
      "Epoch: 7000, Train loss: 0.0001724752364680171, Train return: -1.1452817164010218%, Val loss: 0.0001757806894602254, Val return: -1.300386598064184%, Test loss: 0.00018879312847275287, Test return: -1.8049545007172578%\n",
      "Epoch: 7050, Train loss: 0.00017171383660752326, Train return: -1.149888453450227%, Val loss: 0.00017503237177152187, Val return: -1.2597911143602365%, Test loss: 0.00018808862660080194, Test return: -1.8184324018991493%\n",
      "Epoch: 7100, Train loss: 0.00017096246301662177, Train return: -1.1665866979910577%, Val loss: 0.00017429336730856448, Val return: -1.3023478506110286%, Test loss: 0.00018739374354481697, Test return: -1.7483124822729506%\n",
      "Epoch: 7150, Train loss: 0.0001702207955531776, Train return: -1.1468623676480656%, Val loss: 0.0001735636469675228, Val return: -1.2320212875212324%, Test loss: 0.00018670788267627358, Test return: -1.7890549546661387%\n",
      "Epoch: 7200, Train loss: 0.00016948884876910597, Train return: -1.1680537503940538%, Val loss: 0.00017284321074839681, Val return: -1.123542731474788%, Test loss: 0.00018603139324113727, Test return: -1.836119095478114%\n",
      "Epoch: 7250, Train loss: 0.0001687665790086612, Train return: -1.167221880625672%, Val loss: 0.00017213121464010328, Val return: -1.1758086961539955%, Test loss: 0.00018536388233769685, Test return: -1.9104808923030119%\n",
      "Epoch: 7300, Train loss: 0.00016805391351226717, Train return: -1.1287095312312443%, Val loss: 0.00017142825527116656, Val return: -1.1163574449812175%, Test loss: 0.00018470532086212188, Test return: -1.8631296293778916%\n",
      "Epoch: 7350, Train loss: 0.00016734997916501015, Train return: -1.141078387874979%, Val loss: 0.00017073401249945164, Val return: -0.9166623421477439%, Test loss: 0.00018405522860120982, Test return: -1.9032409173771592%\n",
      "Epoch: 7400, Train loss: 0.00016665486327838153, Train return: -1.1154479224093379%, Val loss: 0.00017004860274028033, Val return: -0.8939977940464814%, Test loss: 0.00018341360555496067, Test return: -1.8447885321820883%\n",
      "Epoch: 7450, Train loss: 0.00016596868226770312, Train return: -1.104817182408575%, Val loss: 0.00016937208420131356, Val return: -0.8578231984121814%, Test loss: 0.00018278048082720488, Test return: -1.748068439198291%\n",
      "Epoch: 7500, Train loss: 0.00016529139247722924, Train return: -1.0486957287429481%, Val loss: 0.0001687040930846706, Val return: -1.106759690684657%, Test loss: 0.0001821561745600775, Test return: -1.6265520977653751%\n",
      "Epoch: 7550, Train loss: 0.0001646225427975878, Train return: -1.0792993359466165%, Val loss: 0.0001680443820077926, Val return: -1.1176431272337675%, Test loss: 0.0001815402792999521, Test return: -1.5570789998595351%\n",
      "Epoch: 7600, Train loss: 0.00016396213322877884, Train return: -1.1030227751191373%, Val loss: 0.00016739282000344247, Val return: -1.0855187677347102%, Test loss: 0.00018093225662596524, Test return: -1.532614533985186%\n",
      "Epoch: 7650, Train loss: 0.00016330981452483684, Train return: -1.0424066888799433%, Val loss: 0.00016674863582011312, Val return: -1.062247120007417%, Test loss: 0.00018033196101896465, Test return: -1.501950314014251%\n",
      "Epoch: 7700, Train loss: 0.0001626656303415075, Train return: -1.0762058603930156%, Val loss: 0.00016611204773653299, Val return: -1.0743596747534658%, Test loss: 0.00017973924695979804, Test return: -1.4768869569425742%\n",
      "Epoch: 7750, Train loss: 0.00016202924598474056, Train return: -1.0232829738352478%, Val loss: 0.0001654830266488716, Val return: -1.196683467828343%, Test loss: 0.00017915383796207607, Test return: -1.5474914546391003%\n",
      "Epoch: 7800, Train loss: 0.00016140077786985785, Train return: -1.0564693827140021%, Val loss: 0.00016486163076478988, Val return: -1.1021081680619664%, Test loss: 0.00017857541388366371, Test return: -1.6259215795347832%\n",
      "Epoch: 7850, Train loss: 0.0001607798767508939, Train return: -0.9786128544241771%, Val loss: 0.00016424746718257666, Val return: -1.0248369176839254%, Test loss: 0.00017800451314542443, Test return: -1.6773787262031568%\n",
      "Epoch: 7900, Train loss: 0.00016016667359508574, Train return: -0.9747963608657564%, Val loss: 0.00016364059410989285, Val return: -0.965729729898934%, Test loss: 0.00017744078650139272, Test return: -1.6376469368714337%\n",
      "Epoch: 7950, Train loss: 0.0001595608628122136, Train return: -0.969676347257043%, Val loss: 0.00016304111341014504, Val return: -0.878376830496004%, Test loss: 0.00017688439402263612, Test return: -1.6088768365355226%\n",
      "Epoch: 8000, Train loss: 0.00015896193508524448, Train return: -0.9940779682687828%, Val loss: 0.0001624483847990632, Val return: -0.9824776319101154%, Test loss: 0.00017633457900956273, Test return: -1.6071303092458886%\n",
      "Epoch: 8050, Train loss: 0.00015837034152355045, Train return: -1.0539095641438183%, Val loss: 0.00016186291759368032, Val return: -0.9904374700611667%, Test loss: 0.0001757916179485619, Test return: -1.6906532688776783%\n",
      "Epoch: 8100, Train loss: 0.00015778570377733558, Train return: -0.9679859730641701%, Val loss: 0.00016128444985952228, Val return: -1.0352464343510088%, Test loss: 0.00017525495786685497, Test return: -1.8160658152146747%\n",
      "Epoch: 8150, Train loss: 0.0001572075707372278, Train return: -0.9942429285167068%, Val loss: 0.0001607122685527429, Val return: -1.0715581675566772%, Test loss: 0.00017472461331635714, Test return: -1.8089143839023911%\n",
      "Epoch: 8200, Train loss: 0.00015663586964365095, Train return: -0.9697939830689568%, Val loss: 0.0001601464464329183, Val return: -1.0978627113911086%, Test loss: 0.00017420014773961157, Test return: -1.829381708977197%\n",
      "Epoch: 8250, Train loss: 0.00015607092063874006, Train return: -0.9449120229625793%, Val loss: 0.00015958693984430283, Val return: -1.0189530428121871%, Test loss: 0.00017368167755194008, Test return: -1.9085231580012443%\n",
      "Epoch: 8300, Train loss: 0.00015551256365142763, Train return: -0.9243974248405229%, Val loss: 0.00015903369057923555, Val return: -0.94892042677075%, Test loss: 0.00017316943558398634, Test return: -1.8615052894516524%\n",
      "Epoch: 8350, Train loss: 0.00015496063861064613, Train return: -0.9149928038929356%, Val loss: 0.00015848674229346216, Val return: -1.071869176842131%, Test loss: 0.00017266310169361532, Test return: -1.9465365622275501%\n",
      "Epoch: 8400, Train loss: 0.00015441502910107374, Train return: -0.8926081328180412%, Val loss: 0.00015794584760442376, Val return: -1.0585818838864676%, Test loss: 0.00017216239939443767, Test return: -1.8823530638348063%\n",
      "Epoch: 8450, Train loss: 0.00015387573512271047, Train return: -0.926427846463837%, Val loss: 0.00015741115203127265, Val return: -0.9768023929012626%, Test loss: 0.00017166780889965594, Test return: -1.8627255048249505%\n",
      "Epoch: 8500, Train loss: 0.0001533426548121497, Train return: -0.9334260210227575%, Val loss: 0.00015688261191826314, Val return: -0.9450325991767088%, Test loss: 0.00017117905372288078, Test return: -1.958924396480186%\n",
      "Epoch: 8550, Train loss: 0.00015281558444257826, Train return: -0.9922771105234056%, Val loss: 0.00015635981981176883, Val return: -0.8404220510054281%, Test loss: 0.00017069558089133352, Test return: -2.0138856676898067%\n",
      "Epoch: 8600, Train loss: 0.00015229448035825044, Train return: -0.9736740716545887%, Val loss: 0.00015584279026370496, Val return: -0.6166764214266748%, Test loss: 0.00017021737585309893, Test return: -1.9698385856003968%\n",
      "Epoch: 8650, Train loss: 0.00015177896420937032, Train return: -0.956705155960426%, Val loss: 0.00015533142141066492, Val return: -0.5313752745110641%, Test loss: 0.00016974435129668564, Test return: -2.0352889665378764%\n",
      "Epoch: 8700, Train loss: 0.00015126880316529423, Train return: -1.0110813439194979%, Val loss: 0.00015482566959690303, Val return: -0.5229770910511913%, Test loss: 0.0001692764344625175, Test return: -2.039976749667567%\n",
      "Epoch: 8750, Train loss: 0.000150764113641344, Train return: -1.0067092137622364%, Val loss: 0.00015432524378411472, Val return: -0.479177877343289%, Test loss: 0.00016881347983144224, Test return: -2.0579031155178016%\n",
      "Epoch: 8800, Train loss: 0.00015026453183963895, Train return: -1.030296595740497%, Val loss: 0.00015382992569357157, Val return: -0.4331527769380262%, Test loss: 0.00016835513815749437, Test return: -2.016313343409992%\n",
      "Epoch: 8850, Train loss: 0.00014977023238316178, Train return: -1.0495693034062563%, Val loss: 0.00015333986084442586, Val return: -0.44322460657538293%, Test loss: 0.00016790151130408049, Test return: -1.9882910650189811%\n",
      "Epoch: 8900, Train loss: 0.00014928128803148866, Train return: -1.0535837809647841%, Val loss: 0.00015285491826944053, Val return: -0.4681778467169423%, Test loss: 0.00016745271568652242, Test return: -1.9748059750627478%\n",
      "Epoch: 8950, Train loss: 0.00014879756781738251, Train return: -1.0186907803526744%, Val loss: 0.00015237511252053082, Val return: -0.4876706434477172%, Test loss: 0.00016700834385119379, Test return: -2.0176355453870087%\n",
      "Epoch: 9000, Train loss: 0.00014831880980636925, Train return: -1.0549736075019482%, Val loss: 0.00015190013800747693, Val return: -0.4044517444233211%, Test loss: 0.00016656900697853416, Test return: -2.086713015896195%\n",
      "Epoch: 9050, Train loss: 0.00014784517406951636, Train return: -1.0254027916703652%, Val loss: 0.0001514300238341093, Val return: -0.4200700040106085%, Test loss: 0.00016613439947832376, Test return: -2.1253901711115923%\n",
      "Epoch: 9100, Train loss: 0.00014737638412043452, Train return: -0.9976583158252811%, Val loss: 0.00015096460992936045, Val return: -0.3674443357719858%, Test loss: 0.00016570425941608846, Test return: -2.067909171059688%\n",
      "Epoch: 9150, Train loss: 0.00014691255637444556, Train return: -1.0002394345782017%, Val loss: 0.0001505041727796197, Val return: -0.34269707595170434%, Test loss: 0.00016527889238204807, Test return: -2.0486236122263115%\n",
      "Epoch: 9200, Train loss: 0.00014645325427409261, Train return: -0.9802521625075287%, Val loss: 0.0001500480284448713, Val return: -0.4125058237193358%, Test loss: 0.00016485775995533913, Test return: -2.077766435233669%\n",
      "Epoch: 9250, Train loss: 0.00014599844871554524, Train return: -0.9531687005224193%, Val loss: 0.0001495958713348955, Val return: -0.4211243330318702%, Test loss: 0.00016444102220702916, Test return: -2.1052439267548473%\n",
      "Epoch: 9300, Train loss: 0.00014554815425071865, Train return: -0.9306696936459179%, Val loss: 0.000149147876072675, Val return: -0.44052320972923836%, Test loss: 0.00016402811161242425, Test return: -2.100277666625179%\n",
      "Epoch: 9350, Train loss: 0.00014510205073747784, Train return: -0.9728427566816643%, Val loss: 0.00014870386803522706, Val return: -0.44576520043811085%, Test loss: 0.00016361898451577872, Test return: -2.062744350299633%\n",
      "Epoch: 9400, Train loss: 0.00014466025459114462, Train return: -0.9531963674304452%, Val loss: 0.00014826410915702581, Val return: -0.38464076946415776%, Test loss: 0.0001632137573324144, Test return: -2.1455000641791435%\n",
      "Epoch: 9450, Train loss: 0.0001442225620849058, Train return: -0.9417746757437749%, Val loss: 0.00014782839571125805, Val return: -0.38518757657260877%, Test loss: 0.00016281237185467035, Test return: -2.0528121838502194%\n",
      "Epoch: 9500, Train loss: 0.00014378917694557458, Train return: -0.9801958718610627%, Val loss: 0.00014739677135366946, Val return: -0.4487972555387909%, Test loss: 0.00016241485718637705, Test return: -2.1567265334423142%\n",
      "Epoch: 9550, Train loss: 0.00014336005551740527, Train return: -0.9419460813343171%, Val loss: 0.00014696933794766665, Val return: -0.5125706529774474%, Test loss: 0.00016202135884668678, Test return: -2.1052838679639145%\n",
      "Epoch: 9600, Train loss: 0.00014293527055997401, Train return: -0.9462563281489694%, Val loss: 0.00014654593542218208, Val return: -0.5858107121937803%, Test loss: 0.00016163205145858228, Test return: -2.0946081753224006%\n",
      "Epoch: 9650, Train loss: 0.0001425148657290265, Train return: -0.9495768460735037%, Val loss: 0.00014612653467338532, Val return: -0.538593003465907%, Test loss: 0.00016124658577609807, Test return: -1.9990075976897195%\n",
      "Epoch: 9700, Train loss: 0.00014209834625944495, Train return: -0.9185297240040955%, Val loss: 0.00014571087376680225, Val return: -0.5663037814574899%, Test loss: 0.00016086487448774278, Test return: -1.9305983686965762%\n",
      "Epoch: 9750, Train loss: 0.00014168558118399233, Train return: -0.911658975351582%, Val loss: 0.00014529879263136536, Val return: -0.7699180100677913%, Test loss: 0.0001604866556590423, Test return: -1.7976197454738319%\n",
      "Epoch: 9800, Train loss: 0.00014127677422948182, Train return: -0.8417150824081967%, Val loss: 0.00014489027671515942, Val return: -0.8186790701361487%, Test loss: 0.00016011194384191185, Test return: -1.742505374635072%\n",
      "Epoch: 9850, Train loss: 0.00014087195449974388, Train return: -0.8573106690600681%, Val loss: 0.00014448573347181082, Val return: -0.8652290222842807%, Test loss: 0.00015974081179592758, Test return: -1.785087360211608%\n",
      "Epoch: 9900, Train loss: 0.00014047103468328714, Train return: -0.8448050821810407%, Val loss: 0.00014408488641493022, Val return: -0.8999797710473797%, Test loss: 0.00015937334683258086, Test return: -1.7075301111993613%\n",
      "Epoch: 9950, Train loss: 0.00014007388381287456, Train return: -0.828166884660988%, Val loss: 0.00014368748816195875, Val return: -0.8655040364932998%, Test loss: 0.0001590093015693128, Test return: -1.675760060101498%\n",
      "Epoch: 10000, Train loss: 0.00013968048733659089, Train return: -0.8399701618508895%, Val loss: 0.00014329361147247255, Val return: -0.9102458527886716%, Test loss: 0.0001586485595908016, Test return: -1.668610379969641%\n",
      "Epoch: 10050, Train loss: 0.0001392908307025209, Train return: -0.8561887269156443%, Val loss: 0.00014290313993114978, Val return: -0.8928385125161763%, Test loss: 0.00015829163021408021, Test return: -1.6612230146202234%\n",
      "Epoch: 10100, Train loss: 0.00013890484115108848, Train return: -0.8495511623731766%, Val loss: 0.00014251605898607522, Val return: -0.9457929250825154%, Test loss: 0.00015793846978340298, Test return: -1.636026260486122%\n",
      "Epoch: 10150, Train loss: 0.00013852246047463268, Train return: -0.849400449119197%, Val loss: 0.00014213254326023161, Val return: -0.9732940554549581%, Test loss: 0.00015758881636429578, Test return: -1.5968526742105988%\n",
      "Epoch: 10200, Train loss: 0.00013814345584250987, Train return: -0.8338959972086729%, Val loss: 0.0001417522580595687, Val return: -1.0308516225785915%, Test loss: 0.00015724258264526725, Test return: -1.7178215948553672%\n",
      "Epoch: 10250, Train loss: 0.0001377682201564312, Train return: -0.8626612449228759%, Val loss: 0.00014137531979940832, Val return: -0.9949685987865301%, Test loss: 0.00015689981228206307, Test return: -1.723352492735843%\n",
      "Epoch: 10300, Train loss: 0.00013739644782617688, Train return: -0.8025062211928384%, Val loss: 0.00014100217958912253, Val return: -1.0293753784565063%, Test loss: 0.00015656046161893755, Test return: -1.723092385261151%\n",
      "Epoch: 10350, Train loss: 0.00013702800788450986, Train return: -0.85634857000639%, Val loss: 0.00014063241542316973, Val return: -0.8937956441273462%, Test loss: 0.0001562244287924841, Test return: -1.7992085182108486%\n",
      "Epoch: 10400, Train loss: 0.00013666266750078648, Train return: -0.8452988386330984%, Val loss: 0.0001402656052960083, Val return: -0.8799850718560406%, Test loss: 0.00015589127724524587, Test return: -1.745063459689445%\n",
      "Epoch: 10450, Train loss: 0.0001363004994345829, Train return: -0.8365267426709186%, Val loss: 0.00013990198203828186, Val return: -0.851060871906246%, Test loss: 0.00015556093421764672, Test return: -1.7861549066031979%\n",
      "Epoch: 10500, Train loss: 0.00013594150368589908, Train return: -0.8844599919132832%, Val loss: 0.00013954134192317724, Val return: -0.7849816666716047%, Test loss: 0.00015523355978075415, Test return: -1.6922693975546372%\n",
      "Epoch: 10550, Train loss: 0.0001355857093585655, Train return: -0.8415014024264705%, Val loss: 0.00013918383046984673, Val return: -0.8311649400037224%, Test loss: 0.00015490919759031385, Test return: -1.675057205968256%\n",
      "Epoch: 10600, Train loss: 0.00013523305824492127, Train return: -0.7931870074265829%, Val loss: 0.0001388293458148837, Val return: -0.7941923592137967%, Test loss: 0.0001545878330944106, Test return: -1.6349863720237519%\n",
      "Epoch: 10650, Train loss: 0.0001348834775853902, Train return: -0.762345751543434%, Val loss: 0.00013847820810042322, Val return: -0.8343438042500475%, Test loss: 0.0001542692189104855, Test return: -1.5135025382176874%\n",
      "Epoch: 10700, Train loss: 0.00013453708379529417, Train return: -0.717332496894486%, Val loss: 0.00013813041732646525, Val return: -0.9351073824501568%, Test loss: 0.00015395364607684314, Test return: -1.4896481014055587%\n",
      "Epoch: 10750, Train loss: 0.00013419393508229405, Train return: -0.7041952018832704%, Val loss: 0.00013778559514321387, Val return: -0.9795793834855268%, Test loss: 0.00015364096907433122, Test return: -1.4061816959669415%\n",
      "Epoch: 10800, Train loss: 0.0001338535366812721, Train return: -0.6879749212989216%, Val loss: 0.00013744340685661882, Val return: -0.9824409028644081%, Test loss: 0.00015333092596847564, Test return: -1.309876287868823%\n",
      "Epoch: 10850, Train loss: 0.0001335159904556349, Train return: -0.7018819515439579%, Val loss: 0.00013710421626456082, Val return: -1.0040378901094582%, Test loss: 0.0001530235749669373, Test return: -1.2099990560237144%\n",
      "Epoch: 10900, Train loss: 0.00013318139826878905, Train return: -0.7253149578512218%, Val loss: 0.00013676781964022666, Val return: -0.9532453789399943%, Test loss: 0.00015271887241397053, Test return: -1.3370895262636882%\n",
      "Epoch: 10950, Train loss: 0.00013284970191307366, Train return: -0.7594308939213203%, Val loss: 0.0001364341878797859, Val return: -1.0037772622078915%, Test loss: 0.00015241668734233826, Test return: -1.3054239831119738%\n",
      "Epoch: 11000, Train loss: 0.00013252085773274302, Train return: -0.8017023679558871%, Val loss: 0.00013610345195047557, Val return: -0.9569576196764745%, Test loss: 0.00015211688878480345, Test return: -1.312380683160863%\n",
      "Epoch: 11050, Train loss: 0.0001321948366239667, Train return: -0.8176972168657913%, Val loss: 0.00013577537902165204, Val return: -0.9693298084073657%, Test loss: 0.00015181978233158588, Test return: -1.3313265594514472%\n",
      "Epoch: 11100, Train loss: 0.00013187136210035533, Train return: -0.8453112921292862%, Val loss: 0.00013544982357416302, Val return: -0.9982723983650585%, Test loss: 0.00015152512060012668, Test return: -1.3787617003451442%\n",
      "Epoch: 11150, Train loss: 0.00013155039050616324, Train return: -0.8718438971934707%, Val loss: 0.00013512694567907602, Val return: -1.0951028645960805%, Test loss: 0.00015123302000574768, Test return: -1.4802192313583085%\n",
      "Epoch: 11200, Train loss: 0.00013123208191245794, Train return: -0.9115775680665316%, Val loss: 0.0001348067744402215, Val return: -1.055755505835039%, Test loss: 0.0001509436551714316, Test return: -1.4897065676018497%\n",
      "Epoch: 11250, Train loss: 0.00013091658183839172, Train return: -0.8878466506603119%, Val loss: 0.00013448928075376898, Val return: -0.9459100365061476%, Test loss: 0.0001506566331954673, Test return: -1.4123512153973934%\n",
      "Epoch: 11300, Train loss: 0.00013060386118013412, Train return: -0.9205896482181005%, Val loss: 0.00013417458103504032, Val return: -0.8774478980862452%, Test loss: 0.00015037214325275272, Test return: -1.368861884273726%\n",
      "Epoch: 11350, Train loss: 0.00013029362889938056, Train return: -0.9123310774507202%, Val loss: 0.00013386239879764616, Val return: -0.812998295775694%, Test loss: 0.00015009012713562697, Test return: -1.3698724458247131%\n",
      "Epoch: 11400, Train loss: 0.00012998587044421583, Train return: -0.9266925727409605%, Val loss: 0.0001335526758339256, Val return: -0.7925700348183311%, Test loss: 0.00014981054118834436, Test return: -1.2477170253906078%\n",
      "Epoch: 11450, Train loss: 0.00012968074588570744, Train return: -0.8639778474079848%, Val loss: 0.00013324552855920047, Val return: -0.807440626921269%, Test loss: 0.00014953337085898966, Test return: -1.3062393327184105%\n",
      "Epoch: 11500, Train loss: 0.00012937802239321172, Train return: -0.887258449869121%, Val loss: 0.00013294086966197938, Val return: -0.7615688776724986%, Test loss: 0.00014925861614756286, Test return: -1.2618994068018512%\n",
      "Epoch: 11550, Train loss: 0.00012907764175906777, Train return: -0.8968075899200231%, Val loss: 0.00013263865548651665, Val return: -0.7507720214640643%, Test loss: 0.00014898614608682692, Test return: -1.2498946057421503%\n",
      "Epoch: 11600, Train loss: 0.00012877944391220808, Train return: -0.7991381909177037%, Val loss: 0.00013233850768301636, Val return: -0.8349387908345662%, Test loss: 0.00014871575694996864, Test return: -1.2043407704887459%\n",
      "Epoch: 11650, Train loss: 0.0001284835598198697, Train return: -0.7623295634045536%, Val loss: 0.00013204067363403738, Val return: -0.8317197647070798%, Test loss: 0.00014844757970422506, Test return: -1.235706631275056%\n",
      "Epoch: 11700, Train loss: 0.0001281899312743917, Train return: -0.7314641374865897%, Val loss: 0.00013174492050893605, Val return: -0.8489762437119022%, Test loss: 0.0001481812068959698, Test return: -1.3230109559002614%\n",
      "Epoch: 11750, Train loss: 0.00012789848551619798, Train return: -0.7747998932879333%, Val loss: 0.00013145121920388192, Val return: -0.8531348677401466%, Test loss: 0.0001479169586673379, Test return: -1.4053724772789153%\n",
      "Epoch: 11800, Train loss: 0.00012760941172018647, Train return: -0.7318350636305562%, Val loss: 0.00013115965703036636, Val return: -0.8045272531229448%, Test loss: 0.00014765479136258364, Test return: -1.436847572411606%\n",
      "Epoch: 11850, Train loss: 0.00012732269533444196, Train return: -0.6793864485714852%, Val loss: 0.00013087027764413506, Val return: -0.7822513264607565%, Test loss: 0.00014739466132596135, Test return: -1.5190723283858933%\n",
      "Epoch: 11900, Train loss: 0.00012703845277428627, Train return: -0.7195233822380257%, Val loss: 0.00013058350305072963, Val return: -0.7584232663118514%, Test loss: 0.000147136757732369, Test return: -1.5714961498794706%\n",
      "Epoch: 11950, Train loss: 0.00012675624748226255, Train return: -0.6903236817433299%, Val loss: 0.00013029879482928663, Val return: -0.676752323775427%, Test loss: 0.00014688076043967158, Test return: -1.5727380495124337%\n",
      "Epoch: 12000, Train loss: 0.00012647609401028603, Train return: -0.7253355019014407%, Val loss: 0.00013001621118746698, Val return: -0.72679152252325%, Test loss: 0.00014662672765552998, Test return: -1.55514878768116%\n",
      "Epoch: 12050, Train loss: 0.00012619802146218717, Train return: -0.73585834377323%, Val loss: 0.00012973573757335544, Val return: -0.7234585706026564%, Test loss: 0.00014637451386079192, Test return: -1.6037652516604242%\n",
      "Epoch: 12100, Train loss: 0.00012592223356477916, Train return: -0.7197511034024797%, Val loss: 0.000129457373986952, Val return: -0.7100748957583162%, Test loss: 0.00014612429367844015, Test return: -1.565486520369606%\n",
      "Epoch: 12150, Train loss: 0.0001256484683835879, Train return: -0.6908728078046952%, Val loss: 0.00012918084394186735, Val return: -0.7207026790864892%, Test loss: 0.00014587605255655944, Test return: -1.647808952899461%\n",
      "Epoch: 12200, Train loss: 0.00012537698785308748, Train return: -0.6604718179102007%, Val loss: 0.0001289060601266101, Val return: -0.7605724685112001%, Test loss: 0.00014562981959898025, Test return: -1.7121258251246843%\n",
      "Epoch: 12250, Train loss: 0.00012510760279837996, Train return: -0.6658090243529837%, Val loss: 0.00012863331357948482, Val return: -0.8091823462003273%, Test loss: 0.0001453855074942112, Test return: -1.7387726174069147%\n",
      "Epoch: 12300, Train loss: 0.00012484021135605872, Train return: -0.6952716032979982%, Val loss: 0.0001283623423660174, Val return: -0.7619211090169535%, Test loss: 0.0001451430725865066, Test return: -1.7861999566131215%\n",
      "Epoch: 12350, Train loss: 0.00012457479897420853, Train return: -0.7377280894734473%, Val loss: 0.00012809324834961444, Val return: -0.7881520596892357%, Test loss: 0.00014490241301245987, Test return: -1.6822125685019038%\n",
      "Epoch: 12400, Train loss: 0.00012431132199708372, Train return: -0.7907642284476846%, Val loss: 0.00012782608973793685, Val return: -0.7696273302020991%, Test loss: 0.00014466358697973192, Test return: -1.7098967100348856%\n",
      "Epoch: 12450, Train loss: 0.00012404975132085383, Train return: -0.7956473559010885%, Val loss: 0.0001275607501156628, Val return: -0.7256491528682795%, Test loss: 0.0001444266235921532, Test return: -1.6000977419854117%\n",
      "Epoch: 12500, Train loss: 0.00012378997053019702, Train return: -0.8041204171117622%, Val loss: 0.0001272973167942837, Val return: -0.7010673776340355%, Test loss: 0.00014419140643440187, Test return: -1.5582736726580897%\n",
      "Epoch: 12550, Train loss: 0.00012353205238468945, Train return: -0.765103956787847%, Val loss: 0.00012703561515081674, Val return: -0.719886719329185%, Test loss: 0.00014395783364307135, Test return: -1.5317678361048181%\n",
      "Epoch: 12600, Train loss: 0.00012327617150731385, Train return: -0.7950155014000532%, Val loss: 0.00012677577615249902, Val return: -0.7277334242720036%, Test loss: 0.00014372625446412712, Test return: -1.44628158521117%\n",
      "Epoch: 12650, Train loss: 0.0001230219641001895, Train return: -0.8117086140829032%, Val loss: 0.00012651761062443256, Val return: -0.702271413450468%, Test loss: 0.00014349636330734938, Test return: -1.4290377187021002%\n",
      "Epoch: 12700, Train loss: 0.00012276943016331643, Train return: -0.8188980812206835%, Val loss: 0.0001262612349819392, Val return: -0.7278912429567442%, Test loss: 0.00014326813106890768, Test return: -1.4558821424169395%\n",
      "Epoch: 12750, Train loss: 0.00012251884618308395, Train return: -0.7958501521765391%, Val loss: 0.0001260068966075778, Val return: -0.7455698180062038%, Test loss: 0.00014304157230071723, Test return: -1.5776138075655255%\n",
      "Epoch: 12800, Train loss: 0.00012226989201735705, Train return: -0.7472054825832413%, Val loss: 0.00012575433356687427, Val return: -0.7556659195996613%, Test loss: 0.00014281642506830394, Test return: -1.5235684573263677%\n",
      "Epoch: 12850, Train loss: 0.0001220226549776271, Train return: -0.7047882347639283%, Val loss: 0.00012550347310025245, Val return: -0.6919991923620272%, Test loss: 0.00014259267481975257, Test return: -1.4845217905326118%\n",
      "Epoch: 12900, Train loss: 0.00012177711323602125, Train return: -0.7134587954874314%, Val loss: 0.00012525421334430575, Val return: -0.6702578297930548%, Test loss: 0.00014237043797038496, Test return: -1.4926772522076595%\n",
      "Epoch: 12950, Train loss: 0.00012153312854934484, Train return: -0.7209931625984093%, Val loss: 0.00012500648153945804, Val return: -0.6389575954594648%, Test loss: 0.00014214974362403154, Test return: -1.5019642551973622%\n",
      "Epoch: 13000, Train loss: 0.00012129081733291969, Train return: -0.7240466129834225%, Val loss: 0.00012476046686060727, Val return: -0.6484339848785144%, Test loss: 0.00014193069364409894, Test return: -1.6539413419441211%\n",
      "Epoch: 13050, Train loss: 0.00012105020141461864, Train return: -0.7713288167874159%, Val loss: 0.00012451606744434685, Val return: -0.6825757175402589%, Test loss: 0.00014171325892675668, Test return: -1.5655281287739162%\n",
      "Epoch: 13100, Train loss: 0.00012081128807039931, Train return: -0.7913457800025355%, Val loss: 0.00012427335605025291, Val return: -0.6474685094638719%, Test loss: 0.00014149713388178498, Test return: -1.6162747357712792%\n",
      "Epoch: 13150, Train loss: 0.00012057420099154115, Train return: -0.7354956392121758%, Val loss: 0.00012403243454173207, Val return: -0.6029410580693214%, Test loss: 0.00014128242037259042, Test return: -1.6331457884760436%\n",
      "Epoch: 13200, Train loss: 0.00012033880921080709, Train return: -0.7044980077098899%, Val loss: 0.00012379315739963204, Val return: -0.6261658388003222%, Test loss: 0.00014106917660683393, Test return: -1.6206829089944697%\n",
      "Epoch: 13250, Train loss: 0.00012010508362436667, Train return: -0.6838774999020255%, Val loss: 0.00012355555372778326, Val return: -0.6263794213102865%, Test loss: 0.00014085748989600688, Test return: -1.6244184931281727%\n",
      "Epoch: 13300, Train loss: 0.00011987294419668615, Train return: -0.6520885543071224%, Val loss: 0.00012331950711086392, Val return: -0.5721723168811204%, Test loss: 0.00014064724382478744, Test return: -1.682076289139451%\n",
      "Epoch: 13350, Train loss: 0.00011964231089223176, Train return: -0.6274632324804988%, Val loss: 0.00012308503210078925, Val return: -0.562192551965952%, Test loss: 0.00014043835108168423, Test return: -1.623360973137981%\n",
      "Epoch: 13400, Train loss: 0.00011941316188313067, Train return: -0.6306696510945764%, Val loss: 0.00012285214324947447, Val return: -0.5197382528503396%, Test loss: 0.00014023094263393432, Test return: -1.626241848991709%\n",
      "Epoch: 13450, Train loss: 0.00011918558448087424, Train return: -0.6714917418109653%, Val loss: 0.00012262073869351298, Val return: -0.58081355630458%, Test loss: 0.0001400249166181311, Test return: -1.5118555923390646%\n",
      "Epoch: 13500, Train loss: 0.00011895950592588633, Train return: -0.5627130022427892%, Val loss: 0.00012239084753673524, Val return: -0.5217304595318749%, Test loss: 0.0001398202293785289, Test return: -1.4759210215443703%\n",
      "Epoch: 13550, Train loss: 0.00011873483890667558, Train return: -0.5288665727668953%, Val loss: 0.00012216246977914125, Val return: -0.4942900880533202%, Test loss: 0.0001396167790517211, Test return: -1.460681988585474%\n",
      "Epoch: 13600, Train loss: 0.0001185116489068605, Train return: -0.5191346269736342%, Val loss: 0.00012193548900540918, Val return: -0.43277157419532003%, Test loss: 0.00013941471115686, Test return: -1.4511449545562054%\n",
      "Epoch: 13650, Train loss: 0.00011828987044282258, Train return: -0.5373559799902713%, Val loss: 0.00012170989066362381, Val return: -0.4745302208018048%, Test loss: 0.00013921399659011513, Test return: -1.4213797191393265%\n",
      "Epoch: 13700, Train loss: 0.00011806956899818033, Train return: -0.5273100033498589%, Val loss: 0.00012148590030847117, Val return: -0.478771717096793%, Test loss: 0.00013901466445531696, Test return: -1.3147155677614857%\n",
      "Epoch: 13750, Train loss: 0.00011785090464400128, Train return: -0.5337892287574691%, Val loss: 0.00012126361252740026, Val return: -0.4678985790540452%, Test loss: 0.00013881686027161777, Test return: -1.2345922711846846%\n",
      "Epoch: 13800, Train loss: 0.00011763367365347221, Train return: -0.5278299355373592%, Val loss: 0.00012104269990231842, Val return: -0.4632785865280452%, Test loss: 0.000138620293000713, Test return: -1.3018626986120796%\n",
      "Epoch: 13850, Train loss: 0.00011741770867956802, Train return: -0.5304209881969599%, Val loss: 0.00012082326429663226, Val return: -0.4815168029542176%, Test loss: 0.00013842512271367013, Test return: -1.396030907149749%\n",
      "Epoch: 13900, Train loss: 0.00011720316979335621, Train return: -0.498408383972081%, Val loss: 0.0001206053639180027, Val return: -0.5257567754046273%, Test loss: 0.00013823136396240443, Test return: -1.3544372967403522%\n",
      "Epoch: 13950, Train loss: 0.00011699004971887916, Train return: -0.47431546252994694%, Val loss: 0.00012038889690302312, Val return: -0.6714428257329608%, Test loss: 0.0001380388712277636, Test return: -1.4363414616123296%\n",
      "Epoch: 14000, Train loss: 0.00011677826842060313, Train return: -0.46722976770930014%, Val loss: 0.00012017382687190548, Val return: -0.7034987884703822%, Test loss: 0.00013784771726932377, Test return: -1.4264586248433702%\n",
      "Epoch: 14050, Train loss: 0.0001165677749668248, Train return: -0.48263825151067946%, Val loss: 0.00011996001558145508, Val return: -0.7849905762430585%, Test loss: 0.00013765788753516972, Test return: -1.422663688201651%\n",
      "Epoch: 14100, Train loss: 0.00011635851842584088, Train return: -0.501029896052787%, Val loss: 0.00011974758672295138, Val return: -0.8059460539553926%, Test loss: 0.00013746938202530146, Test return: -1.4339046690324155%\n",
      "Epoch: 14150, Train loss: 0.00011615065886871889, Train return: -0.5582987306407204%, Val loss: 0.00011953645298490301, Val return: -0.8329143532176264%, Test loss: 0.00013728228805121034, Test return: -1.4260946902385927%\n",
      "Epoch: 14200, Train loss: 0.0001159442836069502, Train return: -0.5579135217095539%, Val loss: 0.00011932662164326757, Val return: -0.7841909625931074%, Test loss: 0.00013709640188608319, Test return: -1.6024918236321146%\n",
      "Epoch: 14250, Train loss: 0.00011573923984542489, Train return: -0.5607255101550652%, Val loss: 0.00011911816545762122, Val return: -0.8119958109798576%, Test loss: 0.0001369117817375809, Test return: -1.6118559411027287%\n",
      "Epoch: 14300, Train loss: 0.00011553541844477877, Train return: -0.5389960537936206%, Val loss: 0.00011891100439243019, Val return: -0.8308319855467042%, Test loss: 0.00013672832574229687, Test return: -1.6261883754465511%\n",
      "Epoch: 14350, Train loss: 0.00011533289944054559, Train return: -0.5352226572320911%, Val loss: 0.00011870506568811834, Val return: -0.9110640526813606%, Test loss: 0.00013654596114065498, Test return: -1.6630629816809457%\n",
      "Epoch: 14400, Train loss: 0.00011513163190102205, Train return: -0.5520726102072321%, Val loss: 0.00011850023292936385, Val return: -1.0124492187551313%, Test loss: 0.00013636478979606181, Test return: -1.5875599173423536%\n",
      "Epoch: 14450, Train loss: 0.00011493170313769951, Train return: -0.543489569745455%, Val loss: 0.00011829658615170047, Val return: -0.997875002425265%, Test loss: 0.00013618481170851737, Test return: -1.7332555538518064%\n",
      "Epoch: 14500, Train loss: 0.00011473293125163764, Train return: -0.5174311735417708%, Val loss: 0.0001180941253551282, Val return: -1.127450021997087%, Test loss: 0.00013600602687802166, Test return: -1.7715238638321706%\n",
      "Epoch: 14550, Train loss: 0.00011453533079475164, Train return: -0.5587061039060737%, Val loss: 0.00011789279960794374, Val return: -1.1227712099890632%, Test loss: 0.00013582828978542238, Test return: -1.7861619935231683%\n",
      "Epoch: 14600, Train loss: 0.00011433903273427859, Train return: -0.560317004928013%, Val loss: 0.00011769268166972324, Val return: -1.2587240377198632%, Test loss: 0.00013565171684604138, Test return: -1.781303370234471%\n",
      "Epoch: 14650, Train loss: 0.00011414408800192177, Train return: -0.5261042257315102%, Val loss: 0.00011749377881642431, Val return: -1.253014449968431%, Test loss: 0.0001354763371637091, Test return: -1.778732061980963%\n",
      "Epoch: 14700, Train loss: 0.00011395043839002028, Train return: -0.5286061320929293%, Val loss: 0.00011729587276931852, Val return: -1.2996677898438918%, Test loss: 0.0001353020779788494, Test return: -1.844054199329844%\n",
      "Epoch: 14750, Train loss: 0.00011375803296687081, Train return: -0.5401018186649743%, Val loss: 0.0001170991818071343, Val return: -1.3700437735314095%, Test loss: 0.00013512889563571662, Test return: -1.9384162492453423%\n",
      "Epoch: 14800, Train loss: 0.00011356684990460053, Train return: -0.5168667477812561%, Val loss: 0.00011690372775774449, Val return: -1.3725975610015222%, Test loss: 0.0001349567755823955, Test return: -1.9994554013284802%\n",
      "Epoch: 14850, Train loss: 0.00011337675096001476, Train return: -0.538051937015745%, Val loss: 0.00011670941603370011, Val return: -1.3491303588845347%, Test loss: 0.00013478560140356421, Test return: -1.997925861375921%\n",
      "Epoch: 14900, Train loss: 0.00011318786710035056, Train return: -0.5449950155280955%, Val loss: 0.00011651629756670445, Val return: -1.3710245756516366%, Test loss: 0.0001346155331702903, Test return: -1.9450723609609946%\n",
      "Epoch: 14950, Train loss: 0.0001130001328419894, Train return: -0.5227883078794491%, Val loss: 0.00011632434325292706, Val return: -1.5088442312919998%, Test loss: 0.00013444649812299758, Test return: -2.0527844669526782%\n",
      "Epoch: 15000, Train loss: 0.00011281354090897366, Train return: -0.49842070068690064%, Val loss: 0.00011613361130002886, Val return: -1.4872879767931022%, Test loss: 0.00013427853991743177, Test return: -2.0261365203209016%\n",
      "Epoch: 15050, Train loss: 0.00011262822226854041, Train return: -0.46493483265089824%, Val loss: 0.00011594409443205222, Val return: -1.5031017831245708%, Test loss: 0.00013411176041699946, Test return: -2.1083510890710975%\n",
      "Epoch: 15100, Train loss: 0.00011244408233324066, Train return: -0.4990602961185627%, Val loss: 0.0001157556107500568, Val return: -1.4636744140672489%, Test loss: 0.0001339461305178702, Test return: -2.116642806878654%\n",
      "Epoch: 15150, Train loss: 0.00011226097558392212, Train return: -0.46382711321213854%, Val loss: 0.0001155681093223393, Val return: -1.3452632225978838%, Test loss: 0.00013378144649323076, Test return: -2.1367262410395%\n",
      "Epoch: 15200, Train loss: 0.00011207905481569469, Train return: -0.45760133345374127%, Val loss: 0.00011538173566805199, Val return: -1.3941959422784895%, Test loss: 0.00013361775199882686, Test return: -2.1271860799023266%\n",
      "Epoch: 15250, Train loss: 0.00011189830547664315, Train return: -0.45149153662586133%, Val loss: 0.00011519657709868625, Val return: -1.3325749046361586%, Test loss: 0.00013345503248274326, Test return: -2.1054955106268527%\n",
      "Epoch: 15300, Train loss: 0.00011171876394655555, Train return: -0.4657567557292948%, Val loss: 0.00011501261906232685, Val return: -1.3942413133026874%, Test loss: 0.00013329324428923428, Test return: -2.0869090426757197%\n",
      "Epoch: 15350, Train loss: 0.000111540372017771, Train return: -0.4572343746086276%, Val loss: 0.00011482986155897379, Val return: -1.3883413267860034%, Test loss: 0.00013313237286638469, Test return: -2.1693419032137156%\n",
      "Epoch: 15400, Train loss: 0.00011136296234326437, Train return: -0.38889466953431456%, Val loss: 0.00011464815906947479, Val return: -1.3707501221135903%, Test loss: 0.00013297246186994016, Test return: -2.0622329645160526%\n",
      "Epoch: 15450, Train loss: 0.00011118657130282372, Train return: -0.39094019974636235%, Val loss: 0.00011446738062659279, Val return: -1.35855628791606%, Test loss: 0.00013281348219607025, Test return: -2.086029821443616%\n",
      "Epoch: 15500, Train loss: 0.00011101122800027952, Train return: -0.437201444405812%, Val loss: 0.00011428772268118337, Val return: -1.348636453630843%, Test loss: 0.00013265527377370745, Test return: -2.1025184270648127%\n",
      "Epoch: 15550, Train loss: 0.000110836852400098, Train return: -0.46105170547355684%, Val loss: 0.00011410910519771278, Val return: -1.2672082204614037%, Test loss: 0.00013249792391434312, Test return: -2.1993479402345466%\n",
      "Epoch: 15600, Train loss: 0.00011066338629461825, Train return: -0.4777567941961351%, Val loss: 0.0001139314699685201, Val return: -1.2387357454686376%, Test loss: 0.0001323415053775534, Test return: -2.0268593164679762%\n",
      "Epoch: 15650, Train loss: 0.00011049090971937403, Train return: -0.5056030264310806%, Val loss: 0.00011375480244169012, Val return: -1.2462287485799515%, Test loss: 0.00013218609092291445, Test return: -2.054522556411729%\n",
      "Epoch: 15700, Train loss: 0.00011031954636564478, Train return: -0.5269840475962008%, Val loss: 0.00011357921903254464, Val return: -1.1655201493462366%, Test loss: 0.0001320316514465958, Test return: -1.9377032275441162%\n",
      "Epoch: 15750, Train loss: 0.00011014926712960005, Train return: -0.47749311167590247%, Val loss: 0.0001134047270170413, Val return: -1.1877923577404752%, Test loss: 0.00013187808508519083, Test return: -1.941301819334764%\n",
      "Epoch: 15800, Train loss: 0.00010997999197570607, Train return: -0.471863967133644%, Val loss: 0.000113231290015392, Val return: -1.3130060579849727%, Test loss: 0.0001317254063906148, Test return: -1.8675209230086933%\n",
      "Epoch: 15850, Train loss: 0.00010981163359247148, Train return: -0.47582181337061535%, Val loss: 0.00011305885709589347, Val return: -1.3747232812811405%, Test loss: 0.00013157358625903726, Test return: -1.7881013239908077%\n",
      "Epoch: 15900, Train loss: 0.00010964419925585389, Train return: -0.4597065578673361%, Val loss: 0.00011288730456726626, Val return: -1.447780426689704%, Test loss: 0.00013142261013854295, Test return: -1.7889711853422252%\n",
      "Epoch: 15950, Train loss: 0.00010947776172542945, Train return: -0.44680804861062257%, Val loss: 0.0001127167561207898, Val return: -1.386109739084254%, Test loss: 0.00013127246347721666, Test return: -1.720312967083177%\n",
      "Epoch: 16000, Train loss: 0.00010931216820608824, Train return: -0.43570344873359856%, Val loss: 0.00011254718992859125, Val return: -1.3729276764246159%, Test loss: 0.00013112317537888885, Test return: -1.6400426857133787%\n",
      "Epoch: 16050, Train loss: 0.00010914745507761836, Train return: -0.5102664817283197%, Val loss: 0.00011237856961088255, Val return: -1.3649339762953303%, Test loss: 0.00013097480405122042, Test return: -1.6681899858783882%\n",
      "Epoch: 16100, Train loss: 0.00010898366599576548, Train return: -0.5087755168741139%, Val loss: 0.00011221081513212994, Val return: -1.3885319243345995%, Test loss: 0.00013082745135761797, Test return: -1.6879373399247473%\n",
      "Epoch: 16150, Train loss: 0.0001088208009605296, Train return: -0.5069792029546655%, Val loss: 0.00011204396287212148, Val return: -1.3900087571612052%, Test loss: 0.00013068091357126832, Test return: -1.7273652851571744%\n",
      "Epoch: 16200, Train loss: 0.00010865892545552924, Train return: -0.5013243754733898%, Val loss: 0.00011187810741830617, Val return: -1.3963226666688497%, Test loss: 0.00013053520524408668, Test return: -1.6288356688482881%\n",
      "Epoch: 16250, Train loss: 0.00010849820682778955, Train return: -0.4649414703822625%, Val loss: 0.000111713248770684, Val return: -1.3976826403737606%, Test loss: 0.0001303904427913949, Test return: -1.6614954947207015%\n",
      "Epoch: 16300, Train loss: 0.00010833841224666685, Train return: -0.46041238680045754%, Val loss: 0.00011154924868606031, Val return: -1.3968398779954663%, Test loss: 0.00013024646614212543, Test return: -1.637840919415848%\n",
      "Epoch: 16350, Train loss: 0.00010817950533237308, Train return: -0.45968452136620364%, Val loss: 0.00011138608533656225, Val return: -1.4271515041397889%, Test loss: 0.00013010339171160012, Test return: -1.6615488592638112%\n",
      "Epoch: 16400, Train loss: 0.00010802142787724733, Train return: -0.4536538768482895%, Val loss: 0.00011122383148176596, Val return: -1.4388104888116084%, Test loss: 0.00012996100122109056, Test return: -1.7691731346225272%\n",
      "Epoch: 16450, Train loss: 0.00010786417260533199, Train return: -0.4539473879335025%, Val loss: 0.00011106245074188337, Val return: -1.4565097220378993%, Test loss: 0.0001298192364629358, Test return: -1.7523876270815832%\n",
      "Epoch: 16500, Train loss: 0.00010770779044833034, Train return: -0.44681855776604806%, Val loss: 0.0001109020522562787, Val return: -1.4537889845917078%, Test loss: 0.00012967828661203384, Test return: -1.7959710108894873%\n",
      "Epoch: 16550, Train loss: 0.00010755235416581854, Train return: -0.4339628061094658%, Val loss: 0.00011074247595388442, Val return: -1.4378483315497208%, Test loss: 0.00012953802070114762, Test return: -1.7541780786222954%\n",
      "Epoch: 16600, Train loss: 0.00010739791468949988, Train return: -0.40864798090913457%, Val loss: 0.00011058370728278533, Val return: -1.42100231721536%, Test loss: 0.00012939856969751418, Test return: -1.7133061046496705%\n",
      "Epoch: 16650, Train loss: 0.0001072444865712896, Train return: -0.4262986034389998%, Val loss: 0.00011042598634958267, Val return: -1.4484293804038317%, Test loss: 0.00012926006456837058, Test return: -1.6734891132425382%\n",
      "Epoch: 16700, Train loss: 0.00010709195339586586, Train return: -0.3827661336589478%, Val loss: 0.00011026912397937849, Val return: -1.5350294439554484%, Test loss: 0.00012912230158690363, Test return: -1.6690320427732428%\n",
      "Epoch: 16750, Train loss: 0.0001069402860593982, Train return: -0.40871572539730017%, Val loss: 0.0001101131783798337, Val return: -1.5774452937848695%, Test loss: 0.00012898545537609607, Test return: -1.67057435740832%\n",
      "Epoch: 16800, Train loss: 0.00010678936814656481, Train return: -0.3903125822719235%, Val loss: 0.00010995813499903306, Val return: -1.6260790968334955%, Test loss: 0.00012884924944955856, Test return: -1.672525304635093%\n",
      "Epoch: 16850, Train loss: 0.00010663924331311136, Train return: -0.39068464932590313%, Val loss: 0.00010980384104186669, Val return: -1.5762397721903596%, Test loss: 0.00012871372746303678, Test return: -1.7058992833283344%\n",
      "Epoch: 16900, Train loss: 0.00010648984607541934, Train return: -0.40886892372586325%, Val loss: 0.00010965025285258889, Val return: -1.5565505290933435%, Test loss: 0.00012857896217610687, Test return: -1.7151855411728967%\n",
      "Epoch: 16950, Train loss: 0.00010634122736519203, Train return: -0.4072397681437153%, Val loss: 0.00010949751595035195, Val return: -1.5050743128034756%, Test loss: 0.00012844495358876884, Test return: -1.681045796296143%\n",
      "Epoch: 17000, Train loss: 0.00010619346721796319, Train return: -0.41390652080755946%, Val loss: 0.00010934567399090156, Val return: -1.5398582430941288%, Test loss: 0.00012831168714910746, Test return: -1.7471860208110659%\n",
      "Epoch: 17050, Train loss: 0.00010604649287415668, Train return: -0.4176453084870282%, Val loss: 0.00010919467604253441, Val return: -1.4922240832589102%, Test loss: 0.0001281790464418009, Test return: -1.7354279628603346%\n",
      "Epoch: 17100, Train loss: 0.00010590028978185728, Train return: -0.40442643361613795%, Val loss: 0.00010904447117354721, Val return: -1.5420841807615766%, Test loss: 0.0001280471042264253, Test return: -1.7516264532793935%\n",
      "Epoch: 17150, Train loss: 0.00010575494525255635, Train return: -0.4196159310856902%, Val loss: 0.00010889507393585518, Val return: -1.5583292985296664%, Test loss: 0.00012791597691830248, Test return: -1.785563663418345%\n",
      "Epoch: 17200, Train loss: 0.00010561032104305923, Train return: -0.46729879947632735%, Val loss: 0.00010874640429392457, Val return: -1.6254243325292894%, Test loss: 0.00012778554810211062, Test return: -1.8177292543538406%\n",
      "Epoch: 17250, Train loss: 0.00010546648263698444, Train return: -0.43814594084785996%, Val loss: 0.00010859849862754345, Val return: -1.5473240026931876%, Test loss: 0.00012765597784891725, Test return: -1.8043429988765594%\n",
      "Epoch: 17300, Train loss: 0.00010532343731028959, Train return: -0.47566262286761396%, Val loss: 0.00010845139331649989, Val return: -1.5391154891263021%, Test loss: 0.00012752706243190914, Test return: -1.8440611986907909%\n",
      "Epoch: 17350, Train loss: 0.00010518104681978002, Train return: -0.49040367814633373%, Val loss: 0.00010830497922142968, Val return: -1.526579023800469%, Test loss: 0.0001273987872991711, Test return: -1.849220606873794%\n",
      "Epoch: 17400, Train loss: 0.00010503947851248085, Train return: -0.46214270882291975%, Val loss: 0.0001081594018614851, Val return: -1.50626888347449%, Test loss: 0.0001272711670026183, Test return: -1.8344520285363948%\n",
      "Epoch: 17450, Train loss: 0.00010489870328456163, Train return: -0.4960984636838432%, Val loss: 0.00010801462485687807, Val return: -1.5376709280111724%, Test loss: 0.00012714423064608127, Test return: -1.8554465000186184%\n",
      "Epoch: 17500, Train loss: 0.00010475869930814952, Train return: -0.421541766095312%, Val loss: 0.00010787067003548145, Val return: -1.527973183143676%, Test loss: 0.00012701787636615336, Test return: -1.8617438734893708%\n",
      "Epoch: 17550, Train loss: 0.00010461948841111735, Train return: -0.4067217841319675%, Val loss: 0.00010772761015687138, Val return: -1.5473545286791361%, Test loss: 0.000126892322441563, Test return: -1.8653044414348254%\n",
      "Epoch: 17600, Train loss: 0.00010448105604154989, Train return: -0.4290978929488584%, Val loss: 0.00010758527059806511, Val return: -1.5988421062017102%, Test loss: 0.00012676749611273408, Test return: -1.913028621805541%\n",
      "Epoch: 17650, Train loss: 0.00010434325668029487, Train return: -0.44099274782164144%, Val loss: 0.00010744365135906264, Val return: -1.5921979207151933%, Test loss: 0.00012664325186051428, Test return: -1.9919102850002817%\n",
      "Epoch: 17700, Train loss: 0.00010420612670714036, Train return: -0.4500058282668638%, Val loss: 0.000107302657852415, Val return: -1.6613922036790199%, Test loss: 0.0001265195751329884, Test return: -2.0051719511379145%\n",
      "Epoch: 17750, Train loss: 0.00010406965157017112, Train return: -0.43925115475874726%, Val loss: 0.0001071622536983341, Val return: -1.7438435212055257%, Test loss: 0.00012639642227441072, Test return: -2.014814070288105%\n",
      "Epoch: 17800, Train loss: 0.00010393378033768386, Train return: -0.4295836182892556%, Val loss: 0.00010702243162086233, Val return: -1.712622122307678%, Test loss: 0.00012627386604435742, Test return: -2.004091322713772%\n",
      "Epoch: 17850, Train loss: 0.0001037985784932971, Train return: -0.3841640878217286%, Val loss: 0.00010688327165553346, Val return: -1.7510527305100916%, Test loss: 0.0001261520228581503, Test return: -1.9974175991264054%\n",
      "Epoch: 17900, Train loss: 0.0001036639732774347, Train return: -0.4204907131321995%, Val loss: 0.00010674484656192362, Val return: -1.7146241662162764%, Test loss: 0.00012603076174855232, Test return: -1.9297483475046202%\n",
      "Epoch: 17950, Train loss: 0.00010353003744967282, Train return: -0.45666845895386%, Val loss: 0.00010660705447662622, Val return: -1.7074787128467974%, Test loss: 0.0001259100390598178, Test return: -1.9492476003837185%\n",
      "Epoch: 18000, Train loss: 0.00010339679283788428, Train return: -0.43777983592755093%, Val loss: 0.00010646985174389556, Val return: -1.8160988459593104%, Test loss: 0.0001257898984476924, Test return: -2.0314719543012747%\n",
      "Epoch: 18050, Train loss: 0.00010326413757866248, Train return: -0.4187566481578961%, Val loss: 0.00010633333295118064, Val return: -1.8027211651973671%, Test loss: 0.00012567023804876953, Test return: -2.06150264544082%\n",
      "Epoch: 18100, Train loss: 0.00010313208622392267, Train return: -0.3691257759492932%, Val loss: 0.00010619751992635429, Val return: -1.758349802053056%, Test loss: 0.00012555114517454058, Test return: -2.0833360949568025%\n",
      "Epoch: 18150, Train loss: 0.00010300069698132575, Train return: -0.3758510582555566%, Val loss: 0.00010606247815303504, Val return: -1.772728214239795%, Test loss: 0.00012543285265564919, Test return: -2.1459722985717673%\n",
      "Epoch: 18200, Train loss: 0.00010287007899023592, Train return: -0.3648916607495246%, Val loss: 0.00010592829494271427, Val return: -1.7931722861185821%, Test loss: 0.00012531525862868875, Test return: -2.2096336700120087%\n",
      "Epoch: 18250, Train loss: 0.00010274007945554331, Train return: -0.36705273414638145%, Val loss: 0.00010579483932815492, Val return: -1.81075142107406%, Test loss: 0.00012519836309365928, Test return: -2.224520161760412%\n",
      "Epoch: 18300, Train loss: 0.0001026106983772479, Train return: -0.359557120265097%, Val loss: 0.0001056619657902047, Val return: -1.8211774112737178%, Test loss: 0.00012508210784289986, Test return: -2.1829375945092133%\n",
      "Epoch: 18350, Train loss: 0.000102481986687053, Train return: -0.319306014423259%, Val loss: 0.0001055297179846093, Val return: -1.7430011207148595%, Test loss: 0.00012496637646108866, Test return: -2.293303187363202%\n",
      "Epoch: 18400, Train loss: 0.00010235397348878905, Train return: -0.3186666152153355%, Val loss: 0.0001053982341545634, Val return: -1.716093557409975%, Test loss: 0.00012485119805205613, Test return: -2.3228846971495907%\n",
      "Epoch: 18450, Train loss: 0.00010222657874692231, Train return: -0.3087038483060449%, Val loss: 0.00010526736150495708, Val return: -1.7208113538901184%, Test loss: 0.0001247365289600566, Test return: -2.310246883678034%\n",
      "Epoch: 18500, Train loss: 0.0001020998606691137, Train return: -0.2698341166797947%, Val loss: 0.00010513726738281548, Val return: -1.6757937817709405%, Test loss: 0.00012462241284083575, Test return: -2.3538116102667246%\n",
      "Epoch: 18550, Train loss: 0.00010197384108323604, Train return: -0.29201548781700304%, Val loss: 0.00010500786447664723, Val return: -1.7038877681782076%, Test loss: 0.00012450884969439358, Test return: -2.336875245679738%\n",
      "Epoch: 18600, Train loss: 0.0001018483962980099, Train return: -0.2742835538768776%, Val loss: 0.0001048790363711305, Val return: -1.7303972627381325%, Test loss: 0.0001243957958649844, Test return: -2.4433698524147034%\n",
      "Epoch: 18650, Train loss: 0.00010172340989811346, Train return: -0.2562950091411052%, Val loss: 0.0001047506884788163, Val return: -1.7355045140182952%, Test loss: 0.0001242831931449473, Test return: -2.4572406916804517%\n",
      "Epoch: 18700, Train loss: 0.00010159904923057184, Train return: -0.2292516829001755%, Val loss: 0.00010462298814672977, Val return: -1.7456133377458196%, Test loss: 0.0001241711142938584, Test return: -2.4471841610431575%\n",
      "Epoch: 18750, Train loss: 0.00010147531429538503, Train return: -0.23050836939338787%, Val loss: 0.00010449590627104044, Val return: -1.7469813239486278%, Test loss: 0.0001240596320712939, Test return: -2.366470816730181%\n",
      "Epoch: 18800, Train loss: 0.00010135210322914645, Train return: -0.21624842787761697%, Val loss: 0.00010436939192004502, Val return: -1.740556694666438%, Test loss: 0.0001239486737176776, Test return: -2.326372144377657%\n",
      "Epoch: 18850, Train loss: 0.000101229474239517, Train return: -0.20860163862586031%, Val loss: 0.00010424348147353157, Val return: -1.7358836932484925%, Test loss: 0.0001238383847521618, Test return: -2.29036491031679%\n",
      "Epoch: 18900, Train loss: 0.00010110754374181852, Train return: -0.22587673941344352%, Val loss: 0.0001041182258632034, Val return: -1.7401242669848798%, Test loss: 0.00012372867786325514, Test return: -2.2265105480720684%\n",
      "Epoch: 18950, Train loss: 0.00010098618076881394, Train return: -0.2057835947285484%, Val loss: 0.00010399360326118767, Val return: -1.7542477955990166%, Test loss: 0.00012361942208372056, Test return: -2.2588645856775296%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11291/3342308045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_conf_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_conf_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_conf_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_path_cnn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# fig.savefig(\"auto_encoder.png\", bbox_inches='tight', dpi=600)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11291/2465750898.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1, train_conf_1, val_conf_1, test_conf_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_cnn')\n",
    "# fig.savefig(\"auto_encoder.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de8fa4-22a4-42a9-83f7-c7a3bb94972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Autoencoder_model()\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1cf12-4f50-4572-abce-8a412e32f5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Single_cell_competition",
   "language": "python",
   "name": "single_cell_competition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
