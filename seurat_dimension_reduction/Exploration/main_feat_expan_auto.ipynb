{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c70773-809f-470a-a9a4-b5907273e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython import display\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8b0a34-3d3c-4a77-8a53-a01326ec1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a475f5-7b98-40da-be68-1770720796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22db1e1e-0226-489b-8e26-805b8c8d23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = pd.read_csv('./integrate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff9203-6526-4815-a36c-c9df9dcca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6eac3f-02b5-4c75-bca7-7e266b51af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat_std = train_test_seurat.std()\n",
    "column_names = list(train_test_seurat.columns)\n",
    "columns_remove = []\n",
    "for i in range(train_test_seurat.shape[1]):\n",
    "    if train_test_seurat_std[i] == 0:\n",
    "        columns_remove.append(column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6a640d-17ca-4197-9120-b68b3a8d8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.drop(columns_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0866b5b-8359-4a8d-aa6d-e3ae91eb6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat[columns_remove[0]] = train_test_seurat.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc71a28-7056-4114-b6cf-fd9f9454edb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109707, 54)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_seurat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1f1207-e8b6-4503-8c77-d2edcd5e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seurat = train_test_seurat.iloc[:90000, :]\n",
    "test_seurat = train_test_seurat.iloc[90000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9614e-73b8-4b4d-8ab0-7a7ca7ab07d1",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e7a64-bad8-4cf8-8ad0-00cd3ccf1f8b",
   "metadata": {},
   "source": [
    "# 1. Load data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75a9ad-7160-472c-9860-cd8b46ba1ac2",
   "metadata": {},
   "source": [
    "## 1.1 Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5f81e6-1e1c-4e7a-b3b3-9adfaa705711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./MLR_Project_train.csv')\n",
    "test = pd.read_csv('./MLR_Project_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0ca5-9892-4287-8e9d-41a273c0e932",
   "metadata": {},
   "source": [
    "Show the data format and dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d204a102-98ba-4502-bc62-b2219ccbba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5874.524387</td>\n",
       "      <td>1072.671848</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41440.23732</td>\n",
       "      <td>70.405148</td>\n",
       "      <td>7.392780</td>\n",
       "      <td>70.377281</td>\n",
       "      <td>23229.69262</td>\n",
       "      <td>23229.72655</td>\n",
       "      <td>70.378864</td>\n",
       "      <td>7.389173</td>\n",
       "      <td>70.380160</td>\n",
       "      <td>23229.76782</td>\n",
       "      <td>23379.81637</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.425618</td>\n",
       "      <td>70.494241</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>4898.757333</td>\n",
       "      <td>15165.92759</td>\n",
       "      <td>297487.1654</td>\n",
       "      <td>297487.16540</td>\n",
       "      <td>15165.92759</td>\n",
       "      <td>4898.757333</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>64.192982</td>\n",
       "      <td>20.940618</td>\n",
       "      <td>76.070270</td>\n",
       "      <td>23376.73707</td>\n",
       "      <td>23343.13291</td>\n",
       "      <td>77.290965</td>\n",
       "      <td>347.308164</td>\n",
       "      <td>102.380501</td>\n",
       "      <td>24823.08137</td>\n",
       "      <td>24120.94894</td>\n",
       "      <td>332.757607</td>\n",
       "      <td>17.386711</td>\n",
       "      <td>129.622187</td>\n",
       "      <td>23936.99077</td>\n",
       "      <td>21670.19233</td>\n",
       "      <td>71.518948</td>\n",
       "      <td>11.399004</td>\n",
       "      <td>78.006816</td>\n",
       "      <td>26437.161240</td>\n",
       "      <td>23811.09670</td>\n",
       "      <td>141.997532</td>\n",
       "      <td>22.474794</td>\n",
       "      <td>0.013314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6124.154099</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41442.22458</td>\n",
       "      <td>70.456758</td>\n",
       "      <td>7.356050</td>\n",
       "      <td>70.379576</td>\n",
       "      <td>23229.76020</td>\n",
       "      <td>23230.10472</td>\n",
       "      <td>70.273618</td>\n",
       "      <td>7.389813</td>\n",
       "      <td>70.329906</td>\n",
       "      <td>23229.46908</td>\n",
       "      <td>23384.98219</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.441223</td>\n",
       "      <td>70.702624</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5100.350569</td>\n",
       "      <td>15788.07683</td>\n",
       "      <td>308790.4312</td>\n",
       "      <td>308790.43120</td>\n",
       "      <td>15788.07683</td>\n",
       "      <td>5100.350569</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>2.937218</td>\n",
       "      <td>-14.428210</td>\n",
       "      <td>75.204973</td>\n",
       "      <td>23317.46049</td>\n",
       "      <td>23309.42032</td>\n",
       "      <td>74.880368</td>\n",
       "      <td>689.670872</td>\n",
       "      <td>127.070357</td>\n",
       "      <td>36746.26762</td>\n",
       "      <td>31012.79786</td>\n",
       "      <td>-310.576721</td>\n",
       "      <td>4.532673</td>\n",
       "      <td>-47.045260</td>\n",
       "      <td>22053.23653</td>\n",
       "      <td>14626.73339</td>\n",
       "      <td>48.124991</td>\n",
       "      <td>-99.618253</td>\n",
       "      <td>-115.120518</td>\n",
       "      <td>7705.543821</td>\n",
       "      <td>22665.35143</td>\n",
       "      <td>-377.287072</td>\n",
       "      <td>-73.700375</td>\n",
       "      <td>-0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5905.732593</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41443.14358</td>\n",
       "      <td>41442.30403</td>\n",
       "      <td>70.422472</td>\n",
       "      <td>7.417794</td>\n",
       "      <td>70.376448</td>\n",
       "      <td>23229.48142</td>\n",
       "      <td>23229.61008</td>\n",
       "      <td>70.474265</td>\n",
       "      <td>7.388979</td>\n",
       "      <td>70.397301</td>\n",
       "      <td>23229.83396</td>\n",
       "      <td>23387.39168</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.440201</td>\n",
       "      <td>70.686178</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5308.548086</td>\n",
       "      <td>16430.60795</td>\n",
       "      <td>320463.9968</td>\n",
       "      <td>320463.99680</td>\n",
       "      <td>16430.60795</td>\n",
       "      <td>5308.548086</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>141.231442</td>\n",
       "      <td>25.855392</td>\n",
       "      <td>75.184756</td>\n",
       "      <td>23248.26780</td>\n",
       "      <td>23307.91084</td>\n",
       "      <td>74.635569</td>\n",
       "      <td>634.523047</td>\n",
       "      <td>71.705965</td>\n",
       "      <td>28917.00549</td>\n",
       "      <td>24632.17456</td>\n",
       "      <td>419.071308</td>\n",
       "      <td>7.403187</td>\n",
       "      <td>118.846496</td>\n",
       "      <td>23430.24573</td>\n",
       "      <td>31251.55292</td>\n",
       "      <td>71.535567</td>\n",
       "      <td>53.482719</td>\n",
       "      <td>106.179152</td>\n",
       "      <td>37586.677270</td>\n",
       "      <td>23251.62576</td>\n",
       "      <td>261.098973</td>\n",
       "      <td>22.565621</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6029.325221</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41440.76212</td>\n",
       "      <td>41442.25682</td>\n",
       "      <td>70.458533</td>\n",
       "      <td>7.366031</td>\n",
       "      <td>70.379221</td>\n",
       "      <td>23230.08433</td>\n",
       "      <td>23229.87971</td>\n",
       "      <td>70.288944</td>\n",
       "      <td>7.390120</td>\n",
       "      <td>70.370247</td>\n",
       "      <td>23229.81662</td>\n",
       "      <td>23390.02296</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.439191</td>\n",
       "      <td>70.692085</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>5510.265781</td>\n",
       "      <td>17053.14128</td>\n",
       "      <td>331774.2409</td>\n",
       "      <td>75710.89648</td>\n",
       "      <td>17053.14128</td>\n",
       "      <td>5510.265781</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>-1.580478</td>\n",
       "      <td>-7.499746</td>\n",
       "      <td>75.184756</td>\n",
       "      <td>23248.26836</td>\n",
       "      <td>23307.91084</td>\n",
       "      <td>74.635569</td>\n",
       "      <td>583.001082</td>\n",
       "      <td>70.384022</td>\n",
       "      <td>25642.94219</td>\n",
       "      <td>23482.26269</td>\n",
       "      <td>-249.671869</td>\n",
       "      <td>7.388974</td>\n",
       "      <td>49.809681</td>\n",
       "      <td>23193.67720</td>\n",
       "      <td>15867.01579</td>\n",
       "      <td>70.369496</td>\n",
       "      <td>-12.169114</td>\n",
       "      <td>63.930236</td>\n",
       "      <td>10052.351290</td>\n",
       "      <td>23229.64352</td>\n",
       "      <td>-10.549985</td>\n",
       "      <td>4.656636</td>\n",
       "      <td>-0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6142.360146</td>\n",
       "      <td>1072.802927</td>\n",
       "      <td>41443.14358</td>\n",
       "      <td>41442.46480</td>\n",
       "      <td>70.413623</td>\n",
       "      <td>7.411287</td>\n",
       "      <td>70.376788</td>\n",
       "      <td>23229.70975</td>\n",
       "      <td>23229.82255</td>\n",
       "      <td>70.467206</td>\n",
       "      <td>7.389910</td>\n",
       "      <td>70.386986</td>\n",
       "      <td>23229.87603</td>\n",
       "      <td>23392.66710</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>11.615135</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>23466.72590</td>\n",
       "      <td>23466.7259</td>\n",
       "      <td>83.418623</td>\n",
       "      <td>7.438244</td>\n",
       "      <td>70.680386</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>1152.667997</td>\n",
       "      <td>17699.01445</td>\n",
       "      <td>343508.5253</td>\n",
       "      <td>343508.52530</td>\n",
       "      <td>17699.01445</td>\n",
       "      <td>5719.546213</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>149.144800</td>\n",
       "      <td>26.789196</td>\n",
       "      <td>74.904988</td>\n",
       "      <td>23229.77108</td>\n",
       "      <td>23260.32245</td>\n",
       "      <td>70.506980</td>\n",
       "      <td>566.549008</td>\n",
       "      <td>120.694073</td>\n",
       "      <td>25765.82131</td>\n",
       "      <td>24618.79392</td>\n",
       "      <td>363.188022</td>\n",
       "      <td>7.389057</td>\n",
       "      <td>78.826922</td>\n",
       "      <td>23235.86490</td>\n",
       "      <td>31790.06425</td>\n",
       "      <td>120.694068</td>\n",
       "      <td>42.971377</td>\n",
       "      <td>145.572170</td>\n",
       "      <td>37109.895810</td>\n",
       "      <td>24143.94971</td>\n",
       "      <td>188.639704</td>\n",
       "      <td>31.863254</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            0            1            2            3          4  \\\n",
       "0           0  5874.524387  1072.671848  41440.76212  41440.23732  70.405148   \n",
       "1           1  6124.154099  1072.802927  41440.76212  41442.22458  70.456758   \n",
       "2           2  5905.732593  1072.802927  41443.14358  41442.30403  70.422472   \n",
       "3           3  6029.325221  1072.802927  41440.76212  41442.25682  70.458533   \n",
       "4           4  6142.360146  1072.802927  41443.14358  41442.46480  70.413623   \n",
       "\n",
       "          5          6            7            8          9        10  \\\n",
       "0  7.392780  70.377281  23229.69262  23229.72655  70.378864  7.389173   \n",
       "1  7.356050  70.379576  23229.76020  23230.10472  70.273618  7.389813   \n",
       "2  7.417794  70.376448  23229.48142  23229.61008  70.474265  7.388979   \n",
       "3  7.366031  70.379221  23230.08433  23229.87971  70.288944  7.390120   \n",
       "4  7.411287  70.376788  23229.70975  23229.82255  70.467206  7.389910   \n",
       "\n",
       "          11           12           13         14         15         16  \\\n",
       "0  70.380160  23229.76782  23379.81637  83.418623  11.615135  83.418623   \n",
       "1  70.329906  23229.46908  23384.98219  83.418623  11.615135  83.418623   \n",
       "2  70.397301  23229.83396  23387.39168  83.418623  11.615135  83.418623   \n",
       "3  70.370247  23229.81662  23390.02296  83.418623  11.615135  83.418623   \n",
       "4  70.386986  23229.87603  23392.66710  70.572881  11.615135  83.418623   \n",
       "\n",
       "            17          18         19        20         21           22  \\\n",
       "0  23466.72590  23466.7259  83.418623  7.425618  70.494241  23229.77107   \n",
       "1  23466.72590  23466.7259  83.418623  7.441223  70.702624  23229.77107   \n",
       "2  23466.72590  23466.7259  83.418623  7.440201  70.686178  23229.77107   \n",
       "3  23233.34325  23466.7259  83.418623  7.439191  70.692085  23229.77107   \n",
       "4  23466.72590  23466.7259  83.418623  7.438244  70.680386  23229.77107   \n",
       "\n",
       "            23         24        25         26           27           28  \\\n",
       "0  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "1  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "2  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "3  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "4  23229.77107  70.376262  7.389056  70.376262  23229.77107  23229.77107   \n",
       "\n",
       "          29           30           31           32            33  \\\n",
       "0  70.376262  4898.757333  15165.92759  297487.1654  297487.16540   \n",
       "1  70.376262  5100.350569  15788.07683  308790.4312  308790.43120   \n",
       "2  70.376262  5308.548086  16430.60795  320463.9968  320463.99680   \n",
       "3  70.376262  5510.265781  17053.14128  331774.2409   75710.89648   \n",
       "4  70.376262  1152.667997  17699.01445  343508.5253  343508.52530   \n",
       "\n",
       "            34           35         36           37           38         39  \\\n",
       "0  15165.92759  4898.757333  70.376262  23229.77107  23229.77107  70.376262   \n",
       "1  15788.07683  5100.350569  70.376262  23229.77107  23229.77107  70.376262   \n",
       "2  16430.60795  5308.548086  70.376262  23229.77107  23229.77107  70.376262   \n",
       "3  17053.14128  5510.265781  70.376262  23229.77107  23229.77107  70.376262   \n",
       "4  17699.01445  5719.546213  70.376262  23229.77107  23229.77107  70.376262   \n",
       "\n",
       "         40         41           42           43          44         45  \\\n",
       "0  7.389056  70.376262  23229.77107  23229.77107   64.192982  20.940618   \n",
       "1  7.389056  70.376262  23229.77107  23229.77107    2.937218 -14.428210   \n",
       "2  7.389056  70.376262  23229.77107  23229.77107  141.231442  25.855392   \n",
       "3  7.389056  70.376262  23229.77107  23229.77107   -1.580478  -7.499746   \n",
       "4  7.389056  70.376262  23229.77107  23229.77107  149.144800  26.789196   \n",
       "\n",
       "          46           47           48         49          50          51  \\\n",
       "0  76.070270  23376.73707  23343.13291  77.290965  347.308164  102.380501   \n",
       "1  75.204973  23317.46049  23309.42032  74.880368  689.670872  127.070357   \n",
       "2  75.184756  23248.26780  23307.91084  74.635569  634.523047   71.705965   \n",
       "3  75.184756  23248.26836  23307.91084  74.635569  583.001082   70.384022   \n",
       "4  74.904988  23229.77108  23260.32245  70.506980  566.549008  120.694073   \n",
       "\n",
       "            52           53          54         55          56           57  \\\n",
       "0  24823.08137  24120.94894  332.757607  17.386711  129.622187  23936.99077   \n",
       "1  36746.26762  31012.79786 -310.576721   4.532673  -47.045260  22053.23653   \n",
       "2  28917.00549  24632.17456  419.071308   7.403187  118.846496  23430.24573   \n",
       "3  25642.94219  23482.26269 -249.671869   7.388974   49.809681  23193.67720   \n",
       "4  25765.82131  24618.79392  363.188022   7.389057   78.826922  23235.86490   \n",
       "\n",
       "            58          59         60          61            62           63  \\\n",
       "0  21670.19233   71.518948  11.399004   78.006816  26437.161240  23811.09670   \n",
       "1  14626.73339   48.124991 -99.618253 -115.120518   7705.543821  22665.35143   \n",
       "2  31251.55292   71.535567  53.482719  106.179152  37586.677270  23251.62576   \n",
       "3  15867.01579   70.369496 -12.169114   63.930236  10052.351290  23229.64352   \n",
       "4  31790.06425  120.694068  42.971377  145.572170  37109.895810  24143.94971   \n",
       "\n",
       "           64         65    TARGET  \n",
       "0  141.997532  22.474794  0.013314  \n",
       "1 -377.287072 -73.700375 -0.000448  \n",
       "2  261.098973  22.565621  0.000244  \n",
       "3  -10.549985   4.656636 -0.000628  \n",
       "4  188.639704  31.863254  0.003811  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b677e07a-0a62-4e5f-921b-89861aa36a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16335.49448</td>\n",
       "      <td>1061.530132</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41237.30921</td>\n",
       "      <td>70.432910</td>\n",
       "      <td>7.379175</td>\n",
       "      <td>70.376460</td>\n",
       "      <td>23229.86828</td>\n",
       "      <td>23229.78910</td>\n",
       "      <td>70.397639</td>\n",
       "      <td>7.389850</td>\n",
       "      <td>70.389234</td>\n",
       "      <td>23229.90881</td>\n",
       "      <td>25602.51370</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>7.537712</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.703960</td>\n",
       "      <td>7.467846</td>\n",
       "      <td>70.433077</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23278.59091</td>\n",
       "      <td>70.835039</td>\n",
       "      <td>7.537712</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23238.10616</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.703960</td>\n",
       "      <td>7133.700113</td>\n",
       "      <td>22063.32144</td>\n",
       "      <td>422799.6647</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>11589.95489</td>\n",
       "      <td>3740.045136</td>\n",
       "      <td>1860.710635</td>\n",
       "      <td>4138543.879</td>\n",
       "      <td>422799.6647</td>\n",
       "      <td>22063.32144</td>\n",
       "      <td>137.249904</td>\n",
       "      <td>66395.12033</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>232518.5574</td>\n",
       "      <td>258.079242</td>\n",
       "      <td>30.226219</td>\n",
       "      <td>75.913405</td>\n",
       "      <td>23229.77107</td>\n",
       "      <td>23229.83301</td>\n",
       "      <td>70.376262</td>\n",
       "      <td>1473.581679</td>\n",
       "      <td>71.758838</td>\n",
       "      <td>23874.69807</td>\n",
       "      <td>23499.73762</td>\n",
       "      <td>-155.498907</td>\n",
       "      <td>7.389056</td>\n",
       "      <td>70.371005</td>\n",
       "      <td>23229.77106</td>\n",
       "      <td>12139.388420</td>\n",
       "      <td>71.758829</td>\n",
       "      <td>14.703740</td>\n",
       "      <td>84.826620</td>\n",
       "      <td>8035.667142</td>\n",
       "      <td>23254.88967</td>\n",
       "      <td>92.945298</td>\n",
       "      <td>12.071364</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13166.20458</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41238.66844</td>\n",
       "      <td>70.394428</td>\n",
       "      <td>7.378583</td>\n",
       "      <td>70.391576</td>\n",
       "      <td>23229.87463</td>\n",
       "      <td>23230.21191</td>\n",
       "      <td>70.288403</td>\n",
       "      <td>7.392921</td>\n",
       "      <td>70.343310</td>\n",
       "      <td>23229.64193</td>\n",
       "      <td>25607.58558</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.469469</td>\n",
       "      <td>70.576552</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>3945.011279</td>\n",
       "      <td>12222.51346</td>\n",
       "      <td>244010.9410</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>22695.88002</td>\n",
       "      <td>7338.666257</td>\n",
       "      <td>139.167102</td>\n",
       "      <td>1239716.334</td>\n",
       "      <td>244010.9410</td>\n",
       "      <td>12222.51346</td>\n",
       "      <td>792.472120</td>\n",
       "      <td>227216.19080</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>434292.0482</td>\n",
       "      <td>-147.808334</td>\n",
       "      <td>-20.702534</td>\n",
       "      <td>76.035317</td>\n",
       "      <td>23313.93717</td>\n",
       "      <td>23332.97615</td>\n",
       "      <td>75.852147</td>\n",
       "      <td>1828.336140</td>\n",
       "      <td>140.129854</td>\n",
       "      <td>37438.79508</td>\n",
       "      <td>30329.40335</td>\n",
       "      <td>167.006374</td>\n",
       "      <td>16.564437</td>\n",
       "      <td>34.929416</td>\n",
       "      <td>23264.53239</td>\n",
       "      <td>39398.972520</td>\n",
       "      <td>84.750309</td>\n",
       "      <td>60.805885</td>\n",
       "      <td>155.880284</td>\n",
       "      <td>41154.557460</td>\n",
       "      <td>24005.38063</td>\n",
       "      <td>199.782364</td>\n",
       "      <td>35.714646</td>\n",
       "      <td>-0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12897.58082</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41240.71985</td>\n",
       "      <td>41238.89069</td>\n",
       "      <td>70.476942</td>\n",
       "      <td>7.400935</td>\n",
       "      <td>70.375313</td>\n",
       "      <td>23229.84429</td>\n",
       "      <td>23229.89405</td>\n",
       "      <td>70.469979</td>\n",
       "      <td>7.391477</td>\n",
       "      <td>70.407134</td>\n",
       "      <td>23230.09254</td>\n",
       "      <td>25610.09095</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.470161</td>\n",
       "      <td>70.605117</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>995.797054</td>\n",
       "      <td>23323.37355</td>\n",
       "      <td>445692.4097</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>12850.00699</td>\n",
       "      <td>4148.336212</td>\n",
       "      <td>3120.762743</td>\n",
       "      <td>4161436.624</td>\n",
       "      <td>445692.4097</td>\n",
       "      <td>23323.37355</td>\n",
       "      <td>233.004088</td>\n",
       "      <td>67655.17244</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>255411.3025</td>\n",
       "      <td>289.867668</td>\n",
       "      <td>32.185249</td>\n",
       "      <td>75.966899</td>\n",
       "      <td>23326.54724</td>\n",
       "      <td>23327.60253</td>\n",
       "      <td>75.649862</td>\n",
       "      <td>1924.171252</td>\n",
       "      <td>111.147939</td>\n",
       "      <td>37691.96604</td>\n",
       "      <td>29511.34868</td>\n",
       "      <td>53.482595</td>\n",
       "      <td>13.727445</td>\n",
       "      <td>165.790143</td>\n",
       "      <td>24499.54560</td>\n",
       "      <td>7180.863302</td>\n",
       "      <td>74.919427</td>\n",
       "      <td>-22.153780</td>\n",
       "      <td>54.137349</td>\n",
       "      <td>6873.937564</td>\n",
       "      <td>23667.70306</td>\n",
       "      <td>74.616186</td>\n",
       "      <td>24.773579</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12887.42863</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41238.33840</td>\n",
       "      <td>41239.00250</td>\n",
       "      <td>70.412815</td>\n",
       "      <td>7.378490</td>\n",
       "      <td>70.377339</td>\n",
       "      <td>23229.91979</td>\n",
       "      <td>23229.86689</td>\n",
       "      <td>70.283757</td>\n",
       "      <td>7.389189</td>\n",
       "      <td>70.359007</td>\n",
       "      <td>23229.67957</td>\n",
       "      <td>25612.79171</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.469667</td>\n",
       "      <td>70.602493</td>\n",
       "      <td>23230.96180</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.452766</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>23240.48762</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>4329.804019</td>\n",
       "      <td>13410.04591</td>\n",
       "      <td>265586.1421</td>\n",
       "      <td>88824.3188</td>\n",
       "      <td>23883.41247</td>\n",
       "      <td>7723.458997</td>\n",
       "      <td>1326.699554</td>\n",
       "      <td>1261291.535</td>\n",
       "      <td>265586.1421</td>\n",
       "      <td>13410.04591</td>\n",
       "      <td>1177.264861</td>\n",
       "      <td>228403.72320</td>\n",
       "      <td>455867.2494</td>\n",
       "      <td>455867.2494</td>\n",
       "      <td>-141.955445</td>\n",
       "      <td>-15.581110</td>\n",
       "      <td>75.883198</td>\n",
       "      <td>23230.50078</td>\n",
       "      <td>23310.57289</td>\n",
       "      <td>73.052689</td>\n",
       "      <td>1808.877007</td>\n",
       "      <td>123.263599</td>\n",
       "      <td>31327.33150</td>\n",
       "      <td>25876.72169</td>\n",
       "      <td>86.016065</td>\n",
       "      <td>7.327061</td>\n",
       "      <td>26.252334</td>\n",
       "      <td>22958.21980</td>\n",
       "      <td>39599.779840</td>\n",
       "      <td>122.820383</td>\n",
       "      <td>46.190366</td>\n",
       "      <td>145.600464</td>\n",
       "      <td>39883.925250</td>\n",
       "      <td>24179.10475</td>\n",
       "      <td>145.999434</td>\n",
       "      <td>26.920634</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11994.73779</td>\n",
       "      <td>1061.661211</td>\n",
       "      <td>41240.71985</td>\n",
       "      <td>41238.81077</td>\n",
       "      <td>70.481341</td>\n",
       "      <td>7.392913</td>\n",
       "      <td>70.385266</td>\n",
       "      <td>23229.57329</td>\n",
       "      <td>23229.79297</td>\n",
       "      <td>70.450114</td>\n",
       "      <td>7.390389</td>\n",
       "      <td>70.377065</td>\n",
       "      <td>23229.74335</td>\n",
       "      <td>25615.17798</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>7.469671</td>\n",
       "      <td>70.607659</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>23280.97236</td>\n",
       "      <td>70.966118</td>\n",
       "      <td>7.580185</td>\n",
       "      <td>70.441802</td>\n",
       "      <td>23235.72471</td>\n",
       "      <td>23233.34325</td>\n",
       "      <td>70.572881</td>\n",
       "      <td>1408.908041</td>\n",
       "      <td>24598.30068</td>\n",
       "      <td>468855.4055</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>14124.93413</td>\n",
       "      <td>4561.447200</td>\n",
       "      <td>4395.689873</td>\n",
       "      <td>4184599.619</td>\n",
       "      <td>468855.4055</td>\n",
       "      <td>24598.30068</td>\n",
       "      <td>175.464314</td>\n",
       "      <td>68930.09957</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>278574.2983</td>\n",
       "      <td>276.189378</td>\n",
       "      <td>21.701184</td>\n",
       "      <td>76.055710</td>\n",
       "      <td>23354.81165</td>\n",
       "      <td>23345.67063</td>\n",
       "      <td>77.102636</td>\n",
       "      <td>1906.984098</td>\n",
       "      <td>598.329736</td>\n",
       "      <td>40066.97972</td>\n",
       "      <td>36345.65544</td>\n",
       "      <td>-605.228200</td>\n",
       "      <td>-132.958332</td>\n",
       "      <td>-529.575904</td>\n",
       "      <td>12869.21582</td>\n",
       "      <td>6244.418503</td>\n",
       "      <td>-23.787704</td>\n",
       "      <td>-46.168214</td>\n",
       "      <td>-57.984418</td>\n",
       "      <td>-6030.026819</td>\n",
       "      <td>13649.75983</td>\n",
       "      <td>-694.862277</td>\n",
       "      <td>-218.983324</td>\n",
       "      <td>-0.001462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            0            1            2            3          4  \\\n",
       "0           0  16335.49448  1061.530132  41238.33840  41237.30921  70.432910   \n",
       "1           1  13166.20458  1061.661211  41238.33840  41238.66844  70.394428   \n",
       "2           2  12897.58082  1061.661211  41240.71985  41238.89069  70.476942   \n",
       "3           3  12887.42863  1061.661211  41238.33840  41239.00250  70.412815   \n",
       "4           4  11994.73779  1061.661211  41240.71985  41238.81077  70.481341   \n",
       "\n",
       "          5          6            7            8          9        10  \\\n",
       "0  7.379175  70.376460  23229.86828  23229.78910  70.397639  7.389850   \n",
       "1  7.378583  70.391576  23229.87463  23230.21191  70.288403  7.392921   \n",
       "2  7.400935  70.375313  23229.84429  23229.89405  70.469979  7.391477   \n",
       "3  7.378490  70.377339  23229.91979  23229.86689  70.283757  7.389189   \n",
       "4  7.392913  70.385266  23229.57329  23229.79297  70.450114  7.390389   \n",
       "\n",
       "          11           12           13         14        15         16  \\\n",
       "0  70.389234  23229.90881  25602.51370  70.835039  7.537712  70.835039   \n",
       "1  70.343310  23229.64193  25607.58558  70.572881  7.452766  70.572881   \n",
       "2  70.407134  23230.09254  25610.09095  70.572881  7.580185  70.966118   \n",
       "3  70.359007  23229.67957  25612.79171  70.572881  7.452766  70.572881   \n",
       "4  70.377065  23229.74335  25615.17798  70.572881  7.580185  70.966118   \n",
       "\n",
       "            17           18         19        20         21           22  \\\n",
       "0  23235.72471  23235.72471  70.703960  7.467846  70.433077  23230.96180   \n",
       "1  23240.48762  23240.48762  70.966118  7.469469  70.576552  23230.96180   \n",
       "2  23233.34325  23233.34325  70.572881  7.470161  70.605117  23233.34325   \n",
       "3  23233.34325  23240.48762  70.966118  7.469667  70.602493  23230.96180   \n",
       "4  23233.34325  23233.34325  70.572881  7.469671  70.607659  23233.34325   \n",
       "\n",
       "            23         24        25         26           27           28  \\\n",
       "0  23278.59091  70.835039  7.537712  70.441802  23238.10616  23235.72471   \n",
       "1  23235.72471  70.572881  7.452766  70.572881  23280.97236  23240.48762   \n",
       "2  23280.97236  70.966118  7.580185  70.441802  23235.72471  23233.34325   \n",
       "3  23235.72471  70.572881  7.452766  70.572881  23280.97236  23240.48762   \n",
       "4  23280.97236  70.966118  7.580185  70.441802  23235.72471  23233.34325   \n",
       "\n",
       "          29           30           31           32           33           34  \\\n",
       "0  70.703960  7133.700113  22063.32144  422799.6647  232518.5574  11589.95489   \n",
       "1  70.966118  3945.011279  12222.51346  244010.9410  434292.0482  22695.88002   \n",
       "2  70.572881   995.797054  23323.37355  445692.4097  255411.3025  12850.00699   \n",
       "3  70.966118  4329.804019  13410.04591  265586.1421   88824.3188  23883.41247   \n",
       "4  70.572881  1408.908041  24598.30068  468855.4055  278574.2983  14124.93413   \n",
       "\n",
       "            35           36           37           38           39  \\\n",
       "0  3740.045136  1860.710635  4138543.879  422799.6647  22063.32144   \n",
       "1  7338.666257   139.167102  1239716.334  244010.9410  12222.51346   \n",
       "2  4148.336212  3120.762743  4161436.624  445692.4097  23323.37355   \n",
       "3  7723.458997  1326.699554  1261291.535  265586.1421  13410.04591   \n",
       "4  4561.447200  4395.689873  4184599.619  468855.4055  24598.30068   \n",
       "\n",
       "            40            41           42           43          44         45  \\\n",
       "0   137.249904   66395.12033  232518.5574  232518.5574  258.079242  30.226219   \n",
       "1   792.472120  227216.19080  434292.0482  434292.0482 -147.808334 -20.702534   \n",
       "2   233.004088   67655.17244  255411.3025  255411.3025  289.867668  32.185249   \n",
       "3  1177.264861  228403.72320  455867.2494  455867.2494 -141.955445 -15.581110   \n",
       "4   175.464314   68930.09957  278574.2983  278574.2983  276.189378  21.701184   \n",
       "\n",
       "          46           47           48         49           50          51  \\\n",
       "0  75.913405  23229.77107  23229.83301  70.376262  1473.581679   71.758838   \n",
       "1  76.035317  23313.93717  23332.97615  75.852147  1828.336140  140.129854   \n",
       "2  75.966899  23326.54724  23327.60253  75.649862  1924.171252  111.147939   \n",
       "3  75.883198  23230.50078  23310.57289  73.052689  1808.877007  123.263599   \n",
       "4  76.055710  23354.81165  23345.67063  77.102636  1906.984098  598.329736   \n",
       "\n",
       "            52           53          54          55          56           57  \\\n",
       "0  23874.69807  23499.73762 -155.498907    7.389056   70.371005  23229.77106   \n",
       "1  37438.79508  30329.40335  167.006374   16.564437   34.929416  23264.53239   \n",
       "2  37691.96604  29511.34868   53.482595   13.727445  165.790143  24499.54560   \n",
       "3  31327.33150  25876.72169   86.016065    7.327061   26.252334  22958.21980   \n",
       "4  40066.97972  36345.65544 -605.228200 -132.958332 -529.575904  12869.21582   \n",
       "\n",
       "             58          59         60          61            62           63  \\\n",
       "0  12139.388420   71.758829  14.703740   84.826620   8035.667142  23254.88967   \n",
       "1  39398.972520   84.750309  60.805885  155.880284  41154.557460  24005.38063   \n",
       "2   7180.863302   74.919427 -22.153780   54.137349   6873.937564  23667.70306   \n",
       "3  39599.779840  122.820383  46.190366  145.600464  39883.925250  24179.10475   \n",
       "4   6244.418503  -23.787704 -46.168214  -57.984418  -6030.026819  13649.75983   \n",
       "\n",
       "           64          65    TARGET  \n",
       "0   92.945298   12.071364  0.010438  \n",
       "1  199.782364   35.714646 -0.000532  \n",
       "2   74.616186   24.773579  0.000726  \n",
       "3  145.999434   26.920634  0.001459  \n",
       "4 -694.862277 -218.983324 -0.001462  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b0f25-fdff-4ce6-a2a4-26bc5f7336a0",
   "metadata": {},
   "source": [
    "## 1.3 Show the maximum return of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0404bc17-40e7-423f-87c3-a050e7fbd448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum return of training set: 195.6927566509\n",
      "Maximum return of testing set: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "train_max = np.sum(train['TARGET'][train['TARGET']>0])\n",
    "test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Maximum return of training set:', train_max)\n",
    "print('Maximum return of testing set:', test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9149c4-b51c-435c-9e81-744f395e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=0.5).fit(pd.DataFrame(train_seurat.iloc[:, :]), train['TARGET'])\n",
    "pred = reg.predict(pd.DataFrame(train_seurat.iloc[:, :]))\n",
    "\n",
    "pred_test = reg.predict(pd.DataFrame(test_seurat.iloc[:, :]))\n",
    "\n",
    "train_res = np.sum(train['TARGET'][pred>0])\n",
    "test_res = np.sum(test['TARGET'][pred_test>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4db722a-05ac-4744-bd74-10c3aa0102b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train naive random selection percentage return: 6.326525261528151%\n",
      "Test naive random selection percentage return: 4.010962118285209%\n"
     ]
    }
   ],
   "source": [
    "print(f'Train naive random selection percentage return: {train_res/train_max*100}%')\n",
    "print(f'Test naive random selection percentage return: {test_res/test_max*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbf2b1-9cdc-476e-9975-8e12dcbdee2b",
   "metadata": {},
   "source": [
    "### 1.3.1 Remove the Unnamed columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb5b0f0-c09f-44ee-975c-8613c205ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5692e8-9d50-4da7-8910-71dd37a12bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 67)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80365c-a512-42fd-a21c-1858ea6fd192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c18b19d7-31bb-4904-8c99-fd0c206e925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame()\n",
    "\n",
    "for i in range(train.shape[1]-1):\n",
    "    for j in range(train.shape[1]-1):\n",
    "        train_[str(i)+'_'+str(j)+'_feat'] = train.iloc[:, i] * train.iloc[:, j]\n",
    "        \n",
    "train_target = pd.DataFrame(train['TARGET'])\n",
    "\n",
    "train = train.drop(['TARGET'], axis = 1)\n",
    "\n",
    "train = pd.concat([train, train_], axis = 1)\n",
    "\n",
    "train = (train-train.mean())/train.std()\n",
    "\n",
    "train['TARGET'] = train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d0774cd-7482-41db-977f-13a02a109d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.DataFrame()\n",
    "\n",
    "for i in range(test.shape[1]-1):\n",
    "    for j in range(test.shape[1]-1):\n",
    "        test_[str(i)+'_'+str(j)+'_feat'] = test.iloc[:, i] * test.iloc[:, j]\n",
    "        \n",
    "test_target = pd.DataFrame(test['TARGET'])\n",
    "\n",
    "test = test.drop(['TARGET'], axis = 1)\n",
    "\n",
    "test = pd.concat([test, test_], axis = 1)\n",
    "\n",
    "test = (test-test.mean())/test.std()\n",
    "\n",
    "test['TARGET'] = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91132f5e-e1fc-4108-b2c0-ed422db1125f",
   "metadata": {},
   "source": [
    "## 5.5 Autoencoder Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e90eb95-282e-415e-a6bb-81d93114b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (109707, 4422)\n",
      "Validation X shape: (18000, 4422)\n",
      "Test X shape: (19707, 4422)\n",
      "Train Y shape: (109707,)\n",
      "Val Y shape: (18000,)\n",
      "Test Y shape: (19707,)\n",
      "train_max: 156.45405680890002\n",
      "val_max: 39.238699842\n",
      "test_max: 55.96225182400002\n"
     ]
    }
   ],
   "source": [
    "input_features = train.drop(['TARGET'], axis=1).to_numpy()\n",
    "# output_features = pd.DataFrame((np.sign(train['TARGET'])+1)//2).to_numpy()\n",
    "output_features = train['TARGET'].to_numpy()\n",
    "\n",
    "X_test = test.drop(['TARGET'], axis=1).to_numpy()\n",
    "# Y_test = pd.DataFrame((np.sign(test['TARGET'])+1)//2).to_numpy()\n",
    "Y_test = test['TARGET'].to_numpy()\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(input_features, output_features, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "Y_train = np.concatenate((Y_train, Y_val, Y_test), axis=0)\n",
    "\n",
    "####\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "test_data = test\n",
    "####\n",
    "\n",
    "auto_train_max = np.sum(train_data['TARGET'][train_data['TARGET']>0])\n",
    "auto_val_max = np.sum(val_data['TARGET'][val_data['TARGET']>0])\n",
    "auto_test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Train X shape:', X_train.shape)\n",
    "print('Validation X shape:', X_val.shape)\n",
    "print('Test X shape:', X_test.shape)\n",
    "\n",
    "print('Train Y shape:', Y_train.shape)\n",
    "print('Val Y shape:', Y_val.shape)\n",
    "print('Test Y shape:', Y_test.shape)\n",
    "\n",
    "print('train_max:', auto_train_max)\n",
    "print('val_max:', auto_val_max)\n",
    "print('test_max:', auto_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f7a19ea-d496-4ee3-b66d-7f33a40361be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_feature: 4422\n",
      "output_feature: 1\n"
     ]
    }
   ],
   "source": [
    "train_input = torch.from_numpy(X_train)\n",
    "train_output = torch.from_numpy(Y_train)\n",
    "val_input = torch.from_numpy(X_val)\n",
    "val_output = torch.from_numpy(Y_val)\n",
    "test_input = torch.from_numpy(X_test)\n",
    "test_output = torch.from_numpy(Y_test)\n",
    "\n",
    "# train_input = torch.unsqueeze(train_input, 1)\n",
    "# val_input = torch.unsqueeze(val_input, 1)\n",
    "# test_input = torch.unsqueeze(test_input, 1)\n",
    "\n",
    "train_input = train_input.float()\n",
    "train_output = train_output.float()\n",
    "val_input = val_input.float()\n",
    "val_output = val_output.float()\n",
    "test_input = test_input.float()\n",
    "test_output = test_output.float()\n",
    "\n",
    "input_feature = train_input.shape[1]\n",
    "output_feature = 1\n",
    "\n",
    "print('input_feature:', input_feature)\n",
    "print('output_feature:', output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b5591cb-674f-4288-9119-9b6c5ee1b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.to(device)\n",
    "train_output = train_output.to(device)\n",
    "val_input = val_input.to(device)\n",
    "val_output = val_output.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_output = test_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "852276a1-d972-4d87-8724-e9c1c973513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9a0344a-7b96-48ff-852a-999ea40a6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_feature, input_feature//2)\n",
    "        self.linear2 = nn.Linear(input_feature*2, input_feature//16)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        self.linear = nn.Linear(input_feature//2, input_feature)\n",
    "        \n",
    "        # nn.init.constant_(self.linear1.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear2.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear3.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear4.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear.weight, 0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.linear(x)\n",
    "                \n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22bb31c5-cbce-4611-ab4a-3f33902bdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "train_ds = TensorDataset(train_input, train_output)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed2d7429-de64-4f06-8610-93c27155281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path):\n",
    "    best_loss = float('inf')\n",
    "    train_pred_output = []\n",
    "    val_pred_output = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    test_error = []\n",
    "    epochs = []\n",
    "    \n",
    "    train_returns = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "    \n",
    "    train_sum = []\n",
    "    val_sum = []\n",
    "    test_sum = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_dl:\n",
    "            model = model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            # y = torch.reshape(y, (y.shape[0], 1))\n",
    "            loss = loss_fn(pred, x)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            \n",
    "            train_pred = model(train_input)\n",
    "            # train_output_index = (torch.sign(train_output)+1)//2\n",
    "            # train_pred_index = (torch.sign(train_pred)+1)//2\n",
    "            # train_output = torch.reshape(train_output, (train_output.shape[0], 1))\n",
    "            # print(train_input.shape, train_pred.shape)\n",
    "            train_loss = loss_fn(train_input, train_pred)\n",
    "            # train_loss = loss_fn(train_pred, train_output.long().squeeze())\n",
    "            train_loss = train_loss.cpu().detach().numpy()\n",
    "            \n",
    "            val_pred = model(val_input)\n",
    "            # val_pred_index = (torch.sign(val_pred)+1)//2\n",
    "            # val_output = torch.reshape(val_output, (val_output.shape[0], 1))\n",
    "            val_loss = loss_fn(val_input, val_pred)\n",
    "            # val_loss = loss_fn(val_pred, val_output.long().squeeze())\n",
    "            val_loss = val_loss.cpu().detach().numpy()\n",
    "        \n",
    "            test_pred = model(test_input)\n",
    "            # test_pred_index = (torch.sign(test_pred)+1)//2\n",
    "            # test_output = torch.reshape(test_output, (test_output.shape[0], 1))\n",
    "            test_loss = loss_fn(test_input, test_pred)\n",
    "            # test_loss = loss_fn(test_pred, test_output.long().squeeze())\n",
    "            test_loss = test_loss.cpu().detach().numpy()\n",
    "    \n",
    "            epochs.append(epoch)\n",
    "            train_error.append(math.log(train_loss+1))\n",
    "            val_error.append(math.log(val_loss+1))\n",
    "            test_error.append(math.log(test_loss+1))\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 2, figsize = (20, 7))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 4, figsize = (22, 5))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # train_conf = confusion_matrix(train_output.cpu().detach().numpy(), train_pred_index.cpu().detach().numpy())\n",
    "#             g1 = sns.heatmap(train_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[0], annot = True)\n",
    "#             g1.set_ylabel('True Target')\n",
    "#             g1.set_xlabel('Predict Target')\n",
    "#             g1.set_title('Train dataset')\n",
    "\n",
    "#             plt.grid(False)\n",
    "            # val_conf = confusion_matrix(val_output.cpu().detach().numpy(), val_pred_index.cpu().detach().numpy())\n",
    "#             g2 = sns.heatmap(val_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[1], annot = True)\n",
    "#             g2.set_ylabel('True Target')\n",
    "#             g2.set_xlabel('Predict Target')\n",
    "#             g2.set_title('Val dataset')\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # test_conf = confusion_matrix(test_output.cpu().detach().numpy(), test_pred_index.cpu().detach().numpy())\n",
    "#             g3 = sns.heatmap(test_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[2], annot = True)\n",
    "#             g3.set_ylabel('True Target')\n",
    "#             g3.set_xlabel('Predict Target')\n",
    "#             g3.set_title('Test dataset')\n",
    "            \n",
    "            # train_pred_np = train_pred_index.cpu().detach().numpy()\n",
    "            # train_output_np = train_output.cpu().detach().numpy()\n",
    "            # val_pred_np = val_pred_index.cpu().detach().numpy()\n",
    "            # val_output_np = val_output.cpu().detach().numpy()\n",
    "            # test_pred_np = test_pred_index.cpu().detach().numpy()\n",
    "            # test_output_np = test_output.cpu().detach().numpy()\n",
    "            \n",
    "#             train_max_value = max(max(train_output_np), max(train_pred_np))\n",
    "#             train_min_value = min(min(train_output_np), min(train_pred_np))\n",
    "#             val_max_value = max(max(val_output_np), max(val_pred_np))\n",
    "#             val_min_value = min(min(val_output_np), min(val_pred_np))\n",
    "#             test_max_value = max(max(test_output_np), max(test_pred_np))\n",
    "#             test_min_value = min(min(test_output_np), min(test_pred_np))\n",
    "            \n",
    "#             ax[0].scatter(train_output_np, train_pred_np, s = 20, alpha=0.3, c='blue')\n",
    "#             ax[1].scatter(val_output_np, val_pred_np, s = 20, alpha=0.3, c='red')\n",
    "#             ax[2].scatter(test_output_np, test_pred_np, s = 20, alpha=0.3, c='green')\n",
    "            \n",
    "#             ax[0].plot(epochs, train_error, c='blue')\n",
    "#             ax[0].plot(epochs, val_error, c='red')\n",
    "#             ax[0].plot(epochs, test_error, c='green')\n",
    "#             ax[0].set_title('Errors vs Epochs', fontsize=15)\n",
    "#             ax[0].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[0].set_ylabel('Errors', fontsize=10)\n",
    "\n",
    "#             ax[0].legend(['train', 'valid', 'test'])\n",
    "            \n",
    "#             ax[0].set_xlim([train_min_value, train_max_value])\n",
    "#             ax[0].set_ylim([train_min_value, train_max_value])\n",
    "#             ax[0].set_title('Trainig data', fontsize=15)\n",
    "#             ax[0].set_xlabel('Target', fontsize=10)\n",
    "#             ax[0].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[0].plot([train_min_value, train_max_value], [train_min_value, train_max_value], 'k-')\n",
    "            \n",
    "#             ax[1].set_xlim([val_min_value, val_max_value])\n",
    "#             ax[1].set_ylim([val_min_value, val_max_value])\n",
    "#             ax[1].set_title('Validation data', fontsize=15)\n",
    "#             ax[1].set_xlabel('Target', fontsize=10)\n",
    "#             ax[1].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[1].plot([val_min_value, val_max_value], [val_min_value, val_max_value], 'k-')\n",
    "            \n",
    "#             ax[2].set_xlim([test_min_value, test_max_value])\n",
    "#             ax[2].set_ylim([test_min_value, test_max_value])\n",
    "#             ax[2].set_title('Testing data', fontsize=15)\n",
    "#             ax[2].set_xlabel('Target', fontsize=10)\n",
    "#             ax[2].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[2].plot([test_min_value, test_max_value], [test_min_value, test_max_value], 'k-')\n",
    "            \n",
    "#             ax[3].plot(epochs, train_error, c='blue')\n",
    "#             ax[3].plot(epochs, val_error, c='red')\n",
    "#             ax[3].plot(epochs, test_error, c='green')\n",
    "#             ax[3].set_title('Training and Validation error', fontsize=15)\n",
    "#             ax[3].set_xlabel('Epochs', fontsize=10)\n",
    "#             ax[3].set_ylabel('MSE error', fontsize=10)\n",
    "            \n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "#             print('Epoch ', epoch, 'Train_loss: ', train_loss*1000, ' Validation_loss: ', val_loss*100, ' Test_loss: ', test_loss*100)\n",
    "            # print(train_pred_np.shape, train_pred_np)\n",
    "            # print(train_pred, train_pred_np)\n",
    "#             train_pred_np = np.squeeze(train_pred_np)\n",
    "#             val_pred_np = np.squeeze(val_pred_np)\n",
    "#             test_pred_np = np.squeeze(test_pred_np)\n",
    "            \n",
    "#             train_res = np.sum(train_data['TARGET'][train_pred_np>0])\n",
    "#             train_output_check = np.squeeze(train_output_np)\n",
    "#             train_check = np.sum(train_data['TARGET'][train_output_check>0])\n",
    "            \n",
    "#             val_res = np.sum(val_data['TARGET'][val_pred_np>0])\n",
    "#             val_output_check = np.squeeze(val_output_np)\n",
    "#             val_check = np.sum(val_data['TARGET'][val_output_check>0])\n",
    "            \n",
    "#             test_res = np.sum(test_data['TARGET'][test_pred_np>0])\n",
    "#             test_output_check = np.squeeze(test_output_np)\n",
    "#             test_check = np.sum(test_data['TARGET'][test_output_check>0])\n",
    "            \n",
    "#             train_returns.append(train_res)\n",
    "#             val_returns.append(val_res)\n",
    "#             test_returns.append(test_res)\n",
    "            \n",
    "#             ax[1].plot(epochs, train_returns, c='blu`e')\n",
    "#             ax[1].plot(epochs, val_returns, c='red')\n",
    "#             ax[1].plot(epochs, test_returns, c='green')\n",
    "#             ax[1].legend(['train', 'valid', 'test'])\n",
    "#             ax[1].set_title('Return vs Epochs', fontsize=15)\n",
    "#             ax[1].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[1].set_ylabel('Returns', fontsize=10)\n",
    "\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "            # train_sum.append(train_res)\n",
    "            # val_sum.append(val_res)\n",
    "            # test_sum.append(test_res)\n",
    "            # print(f'Checks: {train_check/auto_train_max*100}%, {val_check/auto_val_max*100}%, {test_check/auto_test_max*100}%')\n",
    "#             print(f'Maximum sum train return {train_res}, Total train return: {auto_train_max}, Maximum train percentage return: {train_res/auto_train_max*100}%')\n",
    "#             print(f'Maximum sum train return {val_res}, Total train return: {auto_val_max}, Maximum train percentage return: {val_res/auto_val_max*100}%')\n",
    "#             print(f'Maximum sum test return {test_res}, Total test return: {auto_test_max}, Maximum test percentage return: {test_res/auto_test_max*100}%')\n",
    "#             print('Epoch:', epoch, 'Train loss:', train_loss, 'Val loss:', val_loss, 'Test loss:', test_loss)\n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss}, Val loss: {val_loss}, Test loss: {test_loss}')\n",
    "            print(train_input)\n",
    "            print(train_pred)\n",
    "            # print(train_output.cpu().detach().numpy(), train_pred.cpu().detach().numpy())\n",
    "            # print(confusion_matrix(train_output_index.cpu().detach().numpy(), train_pred_index.cpu().detach().numpy()))\n",
    "            # print(train_conf)\n",
    "            # print(val_conf)\n",
    "            # print(test_conf)\n",
    "            # print(train_output, train_pred)\n",
    "            if val_loss < best_loss:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                best_loss = val_loss\n",
    "                \n",
    "#             train_pred_output.append([train_pred.cpu().detach().numpy(), train_output.cpu().detach().numpy()])\n",
    "#             val_pred_output.append([val_pred.cpu().detach().numpy(), val_output.cpu().detach().numpy()])\n",
    "    return train_error, val_error, test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "966e1596-f5a6-4719-8056-f538126e72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.041418194770813, Val loss: 1.058411717414856, Test loss: 1.0412636995315552\n",
      "tensor([[-0.6044,  1.4338,  1.4437,  ..., -0.0587, -0.1851, -0.1745],\n",
      "        [-1.1134, -0.4356, -0.4455,  ..., -0.1709, -0.1942, -0.1776],\n",
      "        [ 0.0831, -0.7339, -0.7438,  ..., -0.2114, -0.1862, -0.1755],\n",
      "        ...,\n",
      "        [-2.3945,  1.7459,  1.7536,  ..., -0.1103, -0.2308, -0.2134],\n",
      "        [-2.4066,  1.7459,  1.7380,  ..., -0.1613, -0.2336, -0.2146],\n",
      "        [-2.4512,  1.7381,  1.7536,  ..., -0.3300, -0.2251, -0.1980]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0079,  0.1394,  0.1843,  ...,  0.0245,  0.1683, -0.0361],\n",
      "        [-0.1176,  0.4751, -0.1328,  ...,  0.3532, -0.2457, -0.0935],\n",
      "        [-0.0352,  0.0674, -0.1193,  ..., -0.1643, -0.2765,  0.1314],\n",
      "        ...,\n",
      "        [ 0.0974, -0.1827, -0.3755,  ..., -0.1232, -0.2148, -0.3996],\n",
      "        [ 0.2549, -0.0033,  0.2436,  ..., -0.3274,  0.0864,  0.0648],\n",
      "        [ 0.0671, -0.2209, -0.2328,  ..., -0.2179, -0.1887, -0.2444]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Epoch: 200, Train loss: 0.09450381994247437, Val loss: 0.09781617671251297, Test loss: 0.09140017628669739\n",
      "tensor([[-0.6044,  1.4338,  1.4437,  ..., -0.0587, -0.1851, -0.1745],\n",
      "        [-1.1134, -0.4356, -0.4455,  ..., -0.1709, -0.1942, -0.1776],\n",
      "        [ 0.0831, -0.7339, -0.7438,  ..., -0.2114, -0.1862, -0.1755],\n",
      "        ...,\n",
      "        [-2.3945,  1.7459,  1.7536,  ..., -0.1103, -0.2308, -0.2134],\n",
      "        [-2.4066,  1.7459,  1.7380,  ..., -0.1613, -0.2336, -0.2146],\n",
      "        [-2.4512,  1.7381,  1.7536,  ..., -0.3300, -0.2251, -0.1980]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.7036,  1.3332,  1.3433,  ...,  0.0991, -0.0454, -0.1716],\n",
      "        [-0.9233, -0.2276, -0.2028,  ..., -0.2038, -0.1977, -0.0538],\n",
      "        [-0.0488, -0.5129, -0.5577,  ..., -0.0109, -0.3196,  0.1611],\n",
      "        ...,\n",
      "        [-2.0908,  1.7367,  1.6265,  ..., -0.2128, -0.2915, -0.6759],\n",
      "        [-2.5770,  1.6781,  1.9315,  ..., -0.0740, -0.2758,  0.0489],\n",
      "        [-2.1966,  1.4992,  1.7429,  ..., -0.6394, -0.4606, -0.6300]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Epoch: 400, Train loss: 0.061800021678209305, Val loss: 0.0644218698143959, Test loss: 0.05925102159380913\n",
      "tensor([[-0.6044,  1.4338,  1.4437,  ..., -0.0587, -0.1851, -0.1745],\n",
      "        [-1.1134, -0.4356, -0.4455,  ..., -0.1709, -0.1942, -0.1776],\n",
      "        [ 0.0831, -0.7339, -0.7438,  ..., -0.2114, -0.1862, -0.1755],\n",
      "        ...,\n",
      "        [-2.3945,  1.7459,  1.7536,  ..., -0.1103, -0.2308, -0.2134],\n",
      "        [-2.4066,  1.7459,  1.7380,  ..., -0.1613, -0.2336, -0.2146],\n",
      "        [-2.4512,  1.7381,  1.7536,  ..., -0.3300, -0.2251, -0.1980]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.6981,  1.3095,  1.3757,  ...,  0.0816, -0.1021, -0.1974],\n",
      "        [-1.0066, -0.2891, -0.3034,  ..., -0.2182, -0.2496, -0.1238],\n",
      "        [-0.0236, -0.5472, -0.5987,  ..., -0.0831, -0.2313,  0.1345],\n",
      "        ...,\n",
      "        [-2.2492,  1.6597,  1.6161,  ..., -0.2977, -0.2383, -0.6443],\n",
      "        [-2.5062,  1.7299,  1.8155,  ..., -0.1517, -0.3725, -0.0563],\n",
      "        [-2.3509,  1.4607,  1.7302,  ..., -0.5846, -0.3309, -0.5075]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Epoch: 600, Train loss: 0.04786746948957443, Val loss: 0.050073061138391495, Test loss: 0.045629750937223434\n",
      "tensor([[-0.6044,  1.4338,  1.4437,  ..., -0.0587, -0.1851, -0.1745],\n",
      "        [-1.1134, -0.4356, -0.4455,  ..., -0.1709, -0.1942, -0.1776],\n",
      "        [ 0.0831, -0.7339, -0.7438,  ..., -0.2114, -0.1862, -0.1755],\n",
      "        ...,\n",
      "        [-2.3945,  1.7459,  1.7536,  ..., -0.1103, -0.2308, -0.2134],\n",
      "        [-2.4066,  1.7459,  1.7380,  ..., -0.1613, -0.2336, -0.2146],\n",
      "        [-2.4512,  1.7381,  1.7536,  ..., -0.3300, -0.2251, -0.1980]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.6770,  1.3388,  1.3954,  ...,  0.0645, -0.1270, -0.1973],\n",
      "        [-1.0366, -0.3303, -0.3498,  ..., -0.2322, -0.2579, -0.1809],\n",
      "        [-0.0027, -0.5804, -0.6168,  ..., -0.1118, -0.2205,  0.1031],\n",
      "        ...,\n",
      "        [-2.3115,  1.6564,  1.6169,  ..., -0.3169, -0.2370, -0.6006],\n",
      "        [-2.4205,  1.7557,  1.7782,  ..., -0.1816, -0.3899, -0.0896],\n",
      "        [-2.4261,  1.4703,  1.6966,  ..., -0.5392, -0.3130, -0.4658]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Epoch: 800, Train loss: 0.03981832414865494, Val loss: 0.041737694293260574, Test loss: 0.037796877324581146\n",
      "tensor([[-0.6044,  1.4338,  1.4437,  ..., -0.0587, -0.1851, -0.1745],\n",
      "        [-1.1134, -0.4356, -0.4455,  ..., -0.1709, -0.1942, -0.1776],\n",
      "        [ 0.0831, -0.7339, -0.7438,  ..., -0.2114, -0.1862, -0.1755],\n",
      "        ...,\n",
      "        [-2.3945,  1.7459,  1.7536,  ..., -0.1103, -0.2308, -0.2134],\n",
      "        [-2.4066,  1.7459,  1.7380,  ..., -0.1613, -0.2336, -0.2146],\n",
      "        [-2.4512,  1.7381,  1.7536,  ..., -0.3300, -0.2251, -0.1980]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.6599,  1.3655,  1.4073,  ...,  0.0419, -0.1445, -0.1967],\n",
      "        [-1.0450, -0.3521, -0.3701,  ..., -0.2359, -0.2590, -0.2001],\n",
      "        [ 0.0100, -0.6125, -0.6281,  ..., -0.1338, -0.2225,  0.0630],\n",
      "        ...,\n",
      "        [-2.3377,  1.6611,  1.6207,  ..., -0.2922, -0.2368, -0.5504],\n",
      "        [-2.3792,  1.7649,  1.7648,  ..., -0.1946, -0.3879, -0.1089],\n",
      "        [-2.4467,  1.4762,  1.6752,  ..., -0.5194, -0.2995, -0.4428]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Epoch: 1000, Train loss: 0.03439007326960564, Val loss: 0.03609664365649223, Test loss: 0.03253865987062454\n",
      "tensor([[-0.6044,  1.4338,  1.4437,  ..., -0.0587, -0.1851, -0.1745],\n",
      "        [-1.1134, -0.4356, -0.4455,  ..., -0.1709, -0.1942, -0.1776],\n",
      "        [ 0.0831, -0.7339, -0.7438,  ..., -0.2114, -0.1862, -0.1755],\n",
      "        ...,\n",
      "        [-2.3945,  1.7459,  1.7536,  ..., -0.1103, -0.2308, -0.2134],\n",
      "        [-2.4066,  1.7459,  1.7380,  ..., -0.1613, -0.2336, -0.2146],\n",
      "        [-2.4512,  1.7381,  1.7536,  ..., -0.3300, -0.2251, -0.1980]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.6435,  1.3782,  1.4078,  ...,  0.0234, -0.1542, -0.1947],\n",
      "        [-1.0570, -0.3635, -0.3822,  ..., -0.2344, -0.2634, -0.2118],\n",
      "        [ 0.0185, -0.6355, -0.6282,  ..., -0.1488, -0.2159,  0.0309],\n",
      "        ...,\n",
      "        [-2.3424,  1.6725,  1.6280,  ..., -0.2622, -0.2455, -0.5169],\n",
      "        [-2.3584,  1.7582,  1.7589,  ..., -0.1984, -0.3831, -0.1271],\n",
      "        [-2.4443,  1.4850,  1.6613,  ..., -0.4971, -0.2870, -0.4217]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6bf5bc79929e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sum_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_path_auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# fig.savefig(\"auto_encoder.png\", bbox_inches='tight', dpi=600)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-0fb626074018>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "learning_rate = 0.1\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_auto')\n",
    "# fig.savefig(\"auto_encoder.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f226a-dfd0-49c4-af62-ee5f9def3ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a74bc-ffdf-4e79-96b8-b7ff6c73a9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6a3fc-ca20-4309-9fec-bb26b87c4d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "398f1d52-4416-43f1-8291-d193a03ec67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_feature, input_feature//2)\n",
    "        self.linear2 = nn.Linear(input_feature*2, input_feature//16)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        self.linear = nn.Linear(input_feature//2, input_feature)\n",
    "        \n",
    "        # nn.init.constant_(self.linear1.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear2.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear3.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear4.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear.weight, 0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        emb = self.dropout(x)\n",
    "        \n",
    "        output = self.linear(emb)\n",
    "                \n",
    "        return emb.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20de8fa4-22a4-42a9-83f7-c7a3bb94972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# num_epochs = 20000\n",
    "# learning_rate = 0.001\n",
    "# loss_fn = F.mse_loss\n",
    "\n",
    "# seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load('model_path_auto'))\n",
    "model = model.to(device)\n",
    "\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# train_sum_1, val_sum_1, test_sum_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_cnn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552305b3-0e3c-4667-9d4d-564ad2adfeda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ad617-b9c9-40ce-a639-840f6b186660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242c667-d58a-4597-8c7b-be7f69d945bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e375671-cda2-400e-9250-12c28eb72a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.53586859,  0.22069168,  0.23062414, ..., -0.06651156,\n",
       "        -0.18962822, -0.17525288],\n",
       "       [-1.48468705,  0.24057933,  0.23062414, ..., -0.37520296,\n",
       "        -0.15439889, -0.14735997],\n",
       "       [-1.52946998,  0.24057933,  0.25051085, ..., -0.06797582,\n",
       "        -0.18576321, -0.17522972],\n",
       "       ...,\n",
       "       [-0.30910729, -1.46975895, -1.47962955, ..., -0.10927053,\n",
       "        -0.19246352, -0.17755649],\n",
       "       [-0.32030669, -1.46975895, -1.45974284, ..., -0.12209406,\n",
       "        -0.19383566, -0.17791277],\n",
       "       [ 0.02215166, -1.46975895, -1.47962955, ..., -0.12212034,\n",
       "        -0.19355161, -0.17791272]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bfc2d5ad-23a1-413b-a46e-358e80444aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_tensor = torch.from_numpy(input_features)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "\n",
    "input_features_tensor = input_features_tensor.float()\n",
    "X_test_tensor = X_test_tensor.float()\n",
    "\n",
    "input_features_tensor = input_features_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17155cbf-8aa7-40ea-8698-bc1abe9e62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_emb = model(input_features_tensor)\n",
    "X_test_emb = model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5da8efe5-91d8-4c21-930c-8c2618b88873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90000, 2211])\n",
      "torch.Size([19707, 2211])\n"
     ]
    }
   ],
   "source": [
    "print(input_features_emb.shape)\n",
    "print(X_test_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d7ef322b-27d4-4e0b-9762-11d219cffad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "# savetxt('input_features_emb.csv', input_features_emb, delimiter=',')\n",
    "# savetxt('X_test_emb.csv', X_test_emb, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd0b28bf-04f7-49a1-8501-ed52aa45e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = input_features_emb\n",
    "X_test = X_test_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f01c6fb9-bf2b-4b10-a147-750fb79cbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(input_features, output_features, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "59f6a94b-8270-416c-ad06-6707b9576703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "test_data = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0656767c-b333-4a15-a9fc-2de1b71beab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: torch.Size([72000, 2211])\n",
      "Validation X shape: torch.Size([18000, 2211])\n",
      "Test X shape: torch.Size([19707, 2211])\n",
      "Train Y shape: (72000,)\n",
      "Val Y shape: (18000,)\n",
      "Test Y shape: (19707,)\n"
     ]
    }
   ],
   "source": [
    "print('Train X shape:', X_train.shape)\n",
    "print('Validation X shape:', X_val.shape)\n",
    "print('Test X shape:', X_test.shape)\n",
    "\n",
    "print('Train Y shape:', Y_train.shape)\n",
    "print('Val Y shape:', Y_val.shape)\n",
    "print('Test Y shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7c7e2b1-046a-4a03-af91-ab55624889ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_feature: 2211\n",
      "output_feature: 1\n"
     ]
    }
   ],
   "source": [
    "train_input = X_train\n",
    "train_output = torch.from_numpy(Y_train)\n",
    "val_input = X_val\n",
    "val_output = torch.from_numpy(Y_val)\n",
    "test_input = X_test\n",
    "test_output = torch.from_numpy(Y_test)\n",
    "\n",
    "# train_input = torch.unsqueeze(train_input, 1)\n",
    "# val_input = torch.unsqueeze(val_input, 1)\n",
    "# test_input = torch.unsqueeze(test_input, 1)\n",
    "\n",
    "# train_input = train_input.float()\n",
    "train_output = train_output.float()\n",
    "# val_input = val_input.float()\n",
    "val_output = val_output.float()\n",
    "# test_input = test_input.float()\n",
    "test_output = test_output.float()\n",
    "\n",
    "input_feature = train_input.shape[1]\n",
    "output_feature = 1\n",
    "\n",
    "print('input_feature:', input_feature)\n",
    "print('output_feature:', output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "267400a5-2b1b-4fff-9e15-109814a8ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.to(device)\n",
    "train_output = train_output.to(device)\n",
    "val_input = val_input.to(device)\n",
    "val_output = val_output.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_output = test_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3063276-fc8d-4c2e-8d60-2ed13931d1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062ae2e-e232-47b4-ab9e-acf8dc93da9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "781ab30e-760d-48b6-ba7d-6ce52d1e3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder_2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_feature, input_feature*2)\n",
    "        self.linear2 = nn.Linear(input_feature*2, input_feature//16)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        self.linear = nn.Linear(input_feature//16, 2)\n",
    "        \n",
    "        # nn.init.constant_(self.linear1.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear2.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear3.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear4.weight, 0.1)\n",
    "        # nn.init.constant_(self.linear.weight, 0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "#         x = self.batchnorm_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "#         x = self.batchnorm_2(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        output = self.linear(x)\n",
    "                \n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64c08cfc-bf56-4a0a-a7de-623c1bf27014",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "train_ds = TensorDataset(train_input, train_output)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40bc83-f267-4514-8024-5db3a69e70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def fit_2(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path):\n",
    "    best_loss = float('inf')\n",
    "    train_pred_output = []\n",
    "    val_pred_output = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    test_error = []\n",
    "    epochs = []\n",
    "    \n",
    "    train_returns = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "    \n",
    "    train_sum = []\n",
    "    val_sum = []\n",
    "    test_sum = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_dl:\n",
    "            model = model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            # y = torch.reshape(y, (y.shape[0], 1))\n",
    "            loss = loss_fn(pred, y.long().squeeze())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            \n",
    "            train_pred = model(train_input)\n",
    "            train_output_index = (torch.sign(train_output)+1)//2\n",
    "            train_pred_index = (torch.sign(train_pred)+1)//2\n",
    "            train_output = torch.reshape(train_output, (train_output_index.shape[0], 1))\n",
    "            # train_loss = loss_fn(train_output, train_pred)\n",
    "            train_loss = loss_fn(train_pred, train_output.long().squeeze())\n",
    "            train_loss = train_loss.cpu().detach().numpy()\n",
    "            \n",
    "            val_pred = model(val_input)\n",
    "            val_pred_index = (torch.sign(val_pred)+1)//2\n",
    "            val_output = torch.reshape(val_output, (val_output.shape[0], 1))\n",
    "            # val_loss = loss_fn(val_output, val_pred)\n",
    "            val_loss = loss_fn(val_pred, val_output.long().squeeze())\n",
    "            val_loss = val_loss.cpu().detach().numpy()\n",
    "        \n",
    "            test_pred = model(test_input)\n",
    "            test_pred_index = (torch.sign(test_pred)+1)//2\n",
    "            test_output = torch.reshape(test_output, (test_output.shape[0], 1))\n",
    "            # test_loss = loss_fn(test_output, test_pred)\n",
    "            test_loss = loss_fn(test_pred, test_output.long().squeeze())\n",
    "            test_loss = test_loss.cpu().detach().numpy()\n",
    "    \n",
    "            epochs.append(epoch)\n",
    "            train_error.append(math.log(train_loss+1))\n",
    "            val_error.append(math.log(val_loss+1))\n",
    "            test_error.append(math.log(test_loss+1))\n",
    "            \n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            train_pred_np = torch.argmax(softmax(train_pred), 1)\n",
    "            val_pred_np = torch.argmax(softmax(val_pred), 1)\n",
    "            test_pred_np = torch.argmax(softmax(test_pred), 1)\n",
    "            # print(train_pred_np)\n",
    "                            \n",
    "            # train_pred_np = train_pred_index.cpu().detach().numpy()\n",
    "            train_output_np = train_output.cpu().detach().numpy()\n",
    "            # val_pred_np = val_pred_index.cpu().detach().numpy()\n",
    "            val_output_np = val_output.cpu().detach().numpy()\n",
    "            # test_pred_np = test_pred_index.cpu().detach().numpy()\n",
    "            test_output_np = test_output.cpu().detach().numpy()\n",
    "            \n",
    "            train_pred_np = train_pred_np.cpu().detach().numpy()\n",
    "            val_pred_np = val_pred_np.cpu().detach().numpy()\n",
    "            test_pred_np = test_pred_np.cpu().detach().numpy()\n",
    "            \n",
    "            train_res = np.sum(train_data['TARGET'][train_pred_np>0])\n",
    "            train_output_check = np.squeeze(train_output_np)\n",
    "            train_check = np.sum(train_data['TARGET'][train_output_check>0])\n",
    "            \n",
    "            val_res = np.sum(val_data['TARGET'][val_pred_np>0])\n",
    "            val_output_check = np.squeeze(val_output_np)\n",
    "            val_check = np.sum(val_data['TARGET'][val_output_check>0])\n",
    "            \n",
    "            test_res = np.sum(test_data['TARGET'][test_pred_np>0])\n",
    "            test_output_check = np.squeeze(test_output_np)\n",
    "            test_check = np.sum(test_data['TARGET'][test_output_check>0])\n",
    "            \n",
    "            train_sum.append(train_res)\n",
    "            val_sum.append(val_res)\n",
    "            test_sum.append(test_res)\n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss}, Train return: {train_res/auto_train_max*100}%, Val loss: {val_loss}, Val return: {val_res/auto_val_max*100}%, Test loss: {test_loss}, Test return: {test_res/auto_test_max*100}%')\n",
    "            # print(np.squeeze(train_output.cpu().detach().numpy()))\n",
    "            # print(train_pred_np)\n",
    "            # print(confusion_matrix(np.squeeze(train_output.cpu().detach().numpy()), train_pred_np))\n",
    "            # print(confusion_matrix(np.squeeze(val_output.cpu().detach().numpy()), val_pred_np))\n",
    "            # print(confusion_matrix(np.squeeze(test_output.cpu().detach().numpy()), test_pred_np))\n",
    "    return train_sum, val_sum, test_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471bd86-582f-4e96-93ca-6c9c68a2ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "# loss_fn = F.mse_loss\n",
    "loss_fn_2 = nn.CrossEntropyLoss()\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder_2()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1 = fit_2(num_epochs, model, loss_fn_2, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_cnn_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a4875-e05b-480e-bcac-4a5a09277916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
