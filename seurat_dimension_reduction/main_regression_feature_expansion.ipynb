{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c70773-809f-470a-a9a4-b5907273e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython import display\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8b0a34-3d3c-4a77-8a53-a01326ec1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a475f5-7b98-40da-be68-1770720796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22db1e1e-0226-489b-8e26-805b8c8d23be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1639473/2339567505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_test_seurat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./integrate.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_parse_dates_presence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_noconvert_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m_set_noconvert_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         col_indices = [\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         ]\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Single_cell_competition/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         col_indices = [\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         ]\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test_seurat = pd.read_csv('./integrate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff9203-6526-4815-a36c-c9df9dcca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6eac3f-02b5-4c75-bca7-7e266b51af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat_std = train_test_seurat.std()\n",
    "column_names = list(train_test_seurat.columns)\n",
    "columns_remove = []\n",
    "for i in range(train_test_seurat.shape[1]):\n",
    "    if train_test_seurat_std[i] == 0:\n",
    "        columns_remove.append(column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a640d-17ca-4197-9120-b68b3a8d8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat = train_test_seurat.drop(columns_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0866b5b-8359-4a8d-aa6d-e3ae91eb6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat[columns_remove[0]] = train_test_seurat.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc71a28-7056-4114-b6cf-fd9f9454edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seurat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f1207-e8b6-4503-8c77-d2edcd5e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seurat = train_test_seurat.iloc[:90000, :]\n",
    "test_seurat = train_test_seurat.iloc[90000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9614e-73b8-4b4d-8ab0-7a7ca7ab07d1",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e7a64-bad8-4cf8-8ad0-00cd3ccf1f8b",
   "metadata": {},
   "source": [
    "# 1. Load data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75a9ad-7160-472c-9860-cd8b46ba1ac2",
   "metadata": {},
   "source": [
    "## 1.1 Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f81e6-1e1c-4e7a-b3b3-9adfaa705711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./MLR_Project_train.csv')\n",
    "test = pd.read_csv('./MLR_Project_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0ca5-9892-4287-8e9d-41a273c0e932",
   "metadata": {},
   "source": [
    "Show the data format and dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204a102-98ba-4502-bc62-b2219ccbba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677e07a-0a62-4e5f-921b-89861aa36a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b0f25-fdff-4ce6-a2a4-26bc5f7336a0",
   "metadata": {},
   "source": [
    "## 1.3 Show the maximum return of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404bc17-40e7-423f-87c3-a050e7fbd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max = np.sum(train['TARGET'][train['TARGET']>0])\n",
    "test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Maximum return of training set:', train_max)\n",
    "print('Maximum return of testing set:', test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9149c4-b51c-435c-9e81-744f395e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=0.5).fit(pd.DataFrame(train_seurat.iloc[:, :]), train['TARGET'])\n",
    "pred = reg.predict(pd.DataFrame(train_seurat.iloc[:, :]))\n",
    "\n",
    "pred_test = reg.predict(pd.DataFrame(test_seurat.iloc[:, :]))\n",
    "\n",
    "train_res = np.sum(train['TARGET'][pred>0])\n",
    "test_res = np.sum(test['TARGET'][pred_test>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db722a-05ac-4744-bd74-10c3aa0102b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train naive random selection percentage return: {train_res/train_max*100}%')\n",
    "print(f'Test naive random selection percentage return: {test_res/test_max*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbf2b1-9cdc-476e-9975-8e12dcbdee2b",
   "metadata": {},
   "source": [
    "### 1.3.1 Remove the Unnamed columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5b0f0-c09f-44ee-975c-8613c205ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5692e8-9d50-4da7-8910-71dd37a12bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80365c-a512-42fd-a21c-1858ea6fd192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b19d7-31bb-4904-8c99-fd0c206e925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame()\n",
    "\n",
    "for i in range(train.shape[1]-1):\n",
    "    for j in range(train.shape[1]-1):\n",
    "        train_[str(i)+'_'+str(j)+'_feat'] = train.iloc[:, i] * train.iloc[:, j]\n",
    "        \n",
    "train_target = pd.DataFrame(train['TARGET'])\n",
    "\n",
    "train = train.drop(['TARGET'], axis = 1)\n",
    "\n",
    "train = pd.concat([train, train_], axis = 1)\n",
    "\n",
    "train['TARGET'] = train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0774cd-7482-41db-977f-13a02a109d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.DataFrame()\n",
    "\n",
    "for i in range(test.shape[1]-1):\n",
    "    for j in range(test.shape[1]-1):\n",
    "        test_[str(i)+'_'+str(j)+'_feat'] = test.iloc[:, i] * test.iloc[:, j]\n",
    "        \n",
    "test_target = pd.DataFrame(test['TARGET'])\n",
    "\n",
    "test = test.drop(['TARGET'], axis = 1)\n",
    "\n",
    "test = pd.concat([test, test_], axis = 1)\n",
    "\n",
    "test['TARGET'] = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91132f5e-e1fc-4108-b2c0-ed422db1125f",
   "metadata": {},
   "source": [
    "## 5.5 Autoencoder Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90eb95-282e-415e-a6bb-81d93114b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = train.drop(['TARGET'], axis=1).to_numpy()\n",
    "output_features = pd.DataFrame((np.sign(train['TARGET'])+1)//2).to_numpy()\n",
    "\n",
    "X_test = test.drop(['TARGET'], axis=1).to_numpy()\n",
    "Y_test = pd.DataFrame((np.sign(test['TARGET'])+1)//2).to_numpy()\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(input_features, output_features, test_size=0.2, random_state=42)\n",
    "\n",
    "####\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "test_data = test\n",
    "####\n",
    "\n",
    "auto_train_max = np.sum(train_data['TARGET'][train_data['TARGET']>0])\n",
    "auto_val_max = np.sum(val_data['TARGET'][val_data['TARGET']>0])\n",
    "auto_test_max = np.sum(test['TARGET'][test['TARGET']>0])\n",
    "\n",
    "print('Train X shape:', X_train.shape)\n",
    "print('Validation X shape:', X_val.shape)\n",
    "print('Test X shape:', X_test.shape)\n",
    "\n",
    "print('Train Y shape:', Y_train.shape)\n",
    "print('Val Y shape:', Y_val.shape)\n",
    "print('Test Y shape:', Y_test.shape)\n",
    "\n",
    "print('train_max:', auto_train_max)\n",
    "print('val_max:', auto_val_max)\n",
    "print('test_max:', auto_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a19ea-d496-4ee3-b66d-7f33a40361be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.from_numpy(X_train)\n",
    "train_output = torch.from_numpy(Y_train)\n",
    "val_input = torch.from_numpy(X_val)\n",
    "val_output = torch.from_numpy(Y_val)\n",
    "test_input = torch.from_numpy(X_test)\n",
    "test_output = torch.from_numpy(Y_test)\n",
    "\n",
    "train_input = torch.unsqueeze(train_input, 1)\n",
    "val_input = torch.unsqueeze(val_input, 1)\n",
    "test_input = torch.unsqueeze(test_input, 1)\n",
    "\n",
    "train_input = train_input.float()\n",
    "train_output = train_output.float()\n",
    "val_input = val_input.float()\n",
    "val_output = val_output.float()\n",
    "test_input = test_input.float()\n",
    "test_output = test_output.float()\n",
    "\n",
    "input_feature = train_input.shape[1]\n",
    "output_feature = 1\n",
    "\n",
    "# print('input_feature:', input_feature)\n",
    "# print('output_feature:', output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5591cb-674f-4288-9119-9b6c5ee1b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.to(device)\n",
    "train_output = train_output.to(device)\n",
    "val_input = val_input.to(device)\n",
    "val_output = val_output.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_output = test_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852276a1-d972-4d87-8724-e9c1c973513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0344a-7b96-48ff-852a-999ea40a6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder model\n",
    "# base model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_feature, input_feature//2)\n",
    "        self.linear2 = nn.Linear(input_feature//2, input_feature//4)\n",
    "        self.linear3 = nn.Linear(input_feature//4, input_feature//16)\n",
    "        self.linear4 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.linear5 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        self.linear6 = nn.Linear(input_feature//16, input_feature//16)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(input_feature//2)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(input_feature//4)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(input_feature//16)\n",
    "        self.linear = nn.Linear(input_feature//16, 1)\n",
    "        \n",
    "        nn.init.constant_(self.linear1.weight, 0.1)\n",
    "        nn.init.constant_(self.linear2.weight, 0.1)\n",
    "        nn.init.constant_(self.linear3.weight, 0.1)\n",
    "        nn.init.constant_(self.linear4.weight, 0.1)\n",
    "        nn.init.constant_(self.linear.weight, 0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "#         x = self.batchnorm_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "#         x = self.batchnorm_2(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "#         x = self.batchnorm_3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear6(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        output = self.linear(x)\n",
    "                \n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb31c5-cbce-4611-ab4a-3f33902bdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "train_ds = TensorDataset(train_input, train_output)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d7429-de64-4f06-8610-93c27155281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, model_path):\n",
    "    best_loss = float('inf')\n",
    "    train_pred_output = []\n",
    "    val_pred_output = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    test_error = []\n",
    "    epochs = []\n",
    "    \n",
    "    train_returns = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "    \n",
    "    train_sum = []\n",
    "    val_sum = []\n",
    "    test_sum = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x,y in train_dl:\n",
    "            model = model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            y = torch.reshape(y, (y.shape[0], 1))\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            \n",
    "            train_pred = model(train_input)\n",
    "            train_pred_index = (torch.sign(train_pred)+1)//2\n",
    "            train_output = torch.reshape(train_output, (train_output.shape[0], 1))\n",
    "            train_loss = loss_fn(train_output, train_pred)\n",
    "            # train_loss = loss_fn(train_pred, train_output.long().squeeze())\n",
    "            train_loss = train_loss.cpu().detach().numpy()\n",
    "            \n",
    "            val_pred = model(val_input)\n",
    "            val_pred_index = (torch.sign(val_pred)+1)//2\n",
    "            val_output = torch.reshape(val_output, (val_output.shape[0], 1))\n",
    "            val_loss = loss_fn(val_output, val_pred)\n",
    "            # val_loss = loss_fn(val_pred, val_output.long().squeeze())\n",
    "            val_loss = val_loss.cpu().detach().numpy()\n",
    "        \n",
    "            test_pred = model(test_input)\n",
    "            test_pred_index = (torch.sign(test_pred)+1)//2\n",
    "            test_output = torch.reshape(test_output, (test_output.shape[0], 1))\n",
    "            test_loss = loss_fn(test_output, test_pred)\n",
    "            # test_loss = loss_fn(test_pred, test_output.long().squeeze())\n",
    "            test_loss = test_loss.cpu().detach().numpy()\n",
    "    \n",
    "            epochs.append(epoch)\n",
    "            train_error.append(math.log(train_loss+1))\n",
    "            val_error.append(math.log(val_loss+1))\n",
    "            test_error.append(math.log(test_loss+1))\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 2, figsize = (20, 7))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             figure, ax = plt.subplots(1, 4, figsize = (22, 5))\n",
    "#             ax = ax.flatten()\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # train_conf = confusion_matrix(train_output, train_pred_index)\n",
    "#             g1 = sns.heatmap(train_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[0], annot = True)\n",
    "#             g1.set_ylabel('True Target')\n",
    "#             g1.set_xlabel('Predict Target')\n",
    "#             g1.set_title('Train dataset')\n",
    "\n",
    "#             plt.grid(False)\n",
    "            # val_conf = confusion_matrix(val_output, val_pred_index)\n",
    "#             g2 = sns.heatmap(val_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[1], annot = True)\n",
    "#             g2.set_ylabel('True Target')\n",
    "#             g2.set_xlabel('Predict Target')\n",
    "#             g2.set_title('Val dataset')\n",
    "            \n",
    "#             plt.grid(False)\n",
    "            # test_conf = confusion_matrix(test_output, test_pred_index)\n",
    "#             g3 = sns.heatmap(test_conf, cmap=\"YlGnBu\",cbar=False, ax=ax[2], annot = True)\n",
    "#             g3.set_ylabel('True Target')\n",
    "#             g3.set_xlabel('Predict Target')\n",
    "#             g3.set_title('Test dataset')\n",
    "            \n",
    "            train_pred_np = train_pred_index.cpu().detach().numpy()\n",
    "            train_output_np = train_output.cpu().detach().numpy()\n",
    "            val_pred_np = val_pred_index.cpu().detach().numpy()\n",
    "            val_output_np = val_output.cpu().detach().numpy()\n",
    "            test_pred_np = test_pred_index.cpu().detach().numpy()\n",
    "            test_output_np = test_output.cpu().detach().numpy()\n",
    "            \n",
    "#             train_max_value = max(max(train_output_np), max(train_pred_np))\n",
    "#             train_min_value = min(min(train_output_np), min(train_pred_np))\n",
    "#             val_max_value = max(max(val_output_np), max(val_pred_np))\n",
    "#             val_min_value = min(min(val_output_np), min(val_pred_np))\n",
    "#             test_max_value = max(max(test_output_np), max(test_pred_np))\n",
    "#             test_min_value = min(min(test_output_np), min(test_pred_np))\n",
    "            \n",
    "#             ax[0].scatter(train_output_np, train_pred_np, s = 20, alpha=0.3, c='blue')\n",
    "#             ax[1].scatter(val_output_np, val_pred_np, s = 20, alpha=0.3, c='red')\n",
    "#             ax[2].scatter(test_output_np, test_pred_np, s = 20, alpha=0.3, c='green')\n",
    "            \n",
    "#             ax[0].plot(epochs, train_error, c='blue')\n",
    "#             ax[0].plot(epochs, val_error, c='red')\n",
    "#             ax[0].plot(epochs, test_error, c='green')\n",
    "#             ax[0].set_title('Errors vs Epochs', fontsize=15)\n",
    "#             ax[0].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[0].set_ylabel('Errors', fontsize=10)\n",
    "\n",
    "#             ax[0].legend(['train', 'valid', 'test'])\n",
    "            \n",
    "#             ax[0].set_xlim([train_min_value, train_max_value])\n",
    "#             ax[0].set_ylim([train_min_value, train_max_value])\n",
    "#             ax[0].set_title('Trainig data', fontsize=15)\n",
    "#             ax[0].set_xlabel('Target', fontsize=10)\n",
    "#             ax[0].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[0].plot([train_min_value, train_max_value], [train_min_value, train_max_value], 'k-')\n",
    "            \n",
    "#             ax[1].set_xlim([val_min_value, val_max_value])\n",
    "#             ax[1].set_ylim([val_min_value, val_max_value])\n",
    "#             ax[1].set_title('Validation data', fontsize=15)\n",
    "#             ax[1].set_xlabel('Target', fontsize=10)\n",
    "#             ax[1].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[1].plot([val_min_value, val_max_value], [val_min_value, val_max_value], 'k-')\n",
    "            \n",
    "#             ax[2].set_xlim([test_min_value, test_max_value])\n",
    "#             ax[2].set_ylim([test_min_value, test_max_value])\n",
    "#             ax[2].set_title('Testing data', fontsize=15)\n",
    "#             ax[2].set_xlabel('Target', fontsize=10)\n",
    "#             ax[2].set_ylabel('Prediction', fontsize=10)\n",
    "#             ax[2].plot([test_min_value, test_max_value], [test_min_value, test_max_value], 'k-')\n",
    "            \n",
    "#             ax[3].plot(epochs, train_error, c='blue')\n",
    "#             ax[3].plot(epochs, val_error, c='red')\n",
    "#             ax[3].plot(epochs, test_error, c='green')\n",
    "#             ax[3].set_title('Training and Validation error', fontsize=15)\n",
    "#             ax[3].set_xlabel('Epochs', fontsize=10)\n",
    "#             ax[3].set_ylabel('MSE error', fontsize=10)\n",
    "            \n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "#             print('Epoch ', epoch, 'Train_loss: ', train_loss*1000, ' Validation_loss: ', val_loss*100, ' Test_loss: ', test_loss*100)\n",
    "            # print(train_pred_np.shape, train_pred_np)\n",
    "            # print(train_pred, train_pred_np)\n",
    "            train_pred_np = np.squeeze(train_pred_np)\n",
    "            val_pred_np = np.squeeze(val_pred_np)\n",
    "            test_pred_np = np.squeeze(test_pred_np)\n",
    "            \n",
    "            train_res = np.sum(train_data['TARGET'][train_pred_np>0])\n",
    "            train_output_check = np.squeeze(train_output_np)\n",
    "            train_check = np.sum(train_data['TARGET'][train_output_check>0])\n",
    "            \n",
    "            val_res = np.sum(val_data['TARGET'][val_pred_np>0])\n",
    "            val_output_check = np.squeeze(val_output_np)\n",
    "            val_check = np.sum(val_data['TARGET'][val_output_check>0])\n",
    "            \n",
    "            test_res = np.sum(test_data['TARGET'][test_pred_np>0])\n",
    "            test_output_check = np.squeeze(test_output_np)\n",
    "            test_check = np.sum(test_data['TARGET'][test_output_check>0])\n",
    "            \n",
    "#             train_returns.append(train_res)\n",
    "#             val_returns.append(val_res)\n",
    "#             test_returns.append(test_res)\n",
    "            \n",
    "#             ax[1].plot(epochs, train_returns, c='blu`e')\n",
    "#             ax[1].plot(epochs, val_returns, c='red')\n",
    "#             ax[1].plot(epochs, test_returns, c='green')\n",
    "#             ax[1].legend(['train', 'valid', 'test'])\n",
    "#             ax[1].set_title('Return vs Epochs', fontsize=15)\n",
    "#             ax[1].set_xlabel('Epoch', fontsize=10)\n",
    "#             ax[1].set_ylabel('Returns', fontsize=10)\n",
    "\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(pl.gcf())\n",
    "            \n",
    "            train_sum.append(train_res)\n",
    "            val_sum.append(val_res)\n",
    "            test_sum.append(test_res)\n",
    "            # print(f'Checks: {train_check/auto_train_max*100}%, {val_check/auto_val_max*100}%, {test_check/auto_test_max*100}%')\n",
    "#             print(f'Maximum sum train return {train_res}, Total train return: {auto_train_max}, Maximum train percentage return: {train_res/auto_train_max*100}%')\n",
    "#             print(f'Maximum sum train return {val_res}, Total train return: {auto_val_max}, Maximum train percentage return: {val_res/auto_val_max*100}%')\n",
    "#             print(f'Maximum sum test return {test_res}, Total test return: {auto_test_max}, Maximum test percentage return: {test_res/auto_test_max*100}%')\n",
    "#             print('Epoch:', epoch, 'Train loss:', train_loss, 'Val loss:', val_loss, 'Test loss:', test_loss)\n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss}, Train return: {train_res/auto_train_max*100}%, Val loss: {val_loss}, Val return: {val_res/auto_val_max*100}%, Test loss: {test_loss}, Test return: {test_res/auto_test_max*100}%')\n",
    "        \n",
    "            if val_loss < best_loss:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                best_loss = val_loss\n",
    "                \n",
    "#             train_pred_output.append([train_pred.cpu().detach().numpy(), train_output.cpu().detach().numpy()])\n",
    "#             val_pred_output.append([val_pred.cpu().detach().numpy(), val_output.cpu().detach().numpy()])\n",
    "    return train_sum, val_sum, test_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e1596-f5a6-4719-8056-f538126e72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_sum_1, val_sum_1, test_sum_1 = fit(num_epochs, model, loss_fn, train_input, train_output, val_input, val_output, test_input, test_output, 'model_path_cnn')\n",
    "# fig.savefig(\"auto_encoder.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de8fa4-22a4-42a9-83f7-c7a3bb94972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Autoencoder_model()\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1cf12-4f50-4572-abce-8a412e32f5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Single_cell_competition",
   "language": "python",
   "name": "single_cell_competition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
